{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3017f6c2",
   "metadata": {},
   "source": [
    "Seed Setting and Saver Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41d298b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "import os\n",
    "# -----------------\n",
    "# Global Seed Setting\n",
    "# -----------------\n",
    "SEED = 42\n",
    "\n",
    "# Python built-in\n",
    "random.seed(SEED)\n",
    "\n",
    "# NumPy\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# PyTorch\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Ensure deterministic runs\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def save_df_to_downloads(df_name: str):\n",
    "    if df_name in globals():\n",
    "        df = globals()[df_name]\n",
    "        path = f\"/mnt/c/Users/baris/Downloads/{df_name}.pkl\"\n",
    "        df.to_pickle(path)\n",
    "        print(f\" Saved {df_name} to {path}\")\n",
    "    else:\n",
    "        print(f\" No DataFrame named '{df_name}' found in globals().\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7c599e",
   "metadata": {},
   "source": [
    "Data Loader and Holiday Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee956ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your ATM data\n",
    "atm_path = \"/mnt/c/Users/baris/Downloads/2024-12-09_ATM_Branch_Data.xlsx\"\n",
    "atm = pd.read_excel(atm_path, sheet_name=\"ATM\")\n",
    "\n",
    "# Official Holidays (special_days)\n",
    "special_days = [\n",
    "    # 2006\n",
    "    '2006-01-01', '2006-01-10', '2006-01-11', '2006-01-12', '2006-01-13',\n",
    "    '2006-04-23', '2006-05-19', '2006-08-30',\n",
    "    '2006-10-23', '2006-10-24', '2006-10-25',\n",
    "    '2006-10-29', '2006-12-31',\n",
    "\n",
    "    # 2007\n",
    "    '2007-01-01', '2007-01-02', '2007-01-03',\n",
    "    '2007-04-23', '2007-05-19', '2007-08-30',\n",
    "    '2007-10-12', '2007-10-13', '2007-10-14',\n",
    "    '2007-10-29',\n",
    "    '2007-12-20', '2007-12-21', '2007-12-22', '2007-12-23',\n",
    "    '2007-12-31',\n",
    "\n",
    "    # 2008\n",
    "    '2008-01-01', '2008-04-23', '2008-05-19', '2008-08-30',\n",
    "    '2008-09-30', '2008-10-01', '2008-10-02',\n",
    "    '2008-10-29',\n",
    "    '2008-12-08', '2008-12-09', '2008-12-10', '2008-12-11',\n",
    "    '2008-12-31'\n",
    "]\n",
    "\n",
    "# Half Working Days (half_days)\n",
    "half_days = [\n",
    "    '2006-01-09', '2006-10-22', '2006-12-30',\n",
    "    '2007-10-11', '2007-12-19',\n",
    "    '2008-09-29', '2008-12-07'\n",
    "]\n",
    "\n",
    "feature_eng_data = feature_engineering_pipeline(atm, special_days, half_days)\n",
    "\n",
    "print(feature_eng_data)\n",
    "print(feature_eng_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e537c129",
   "metadata": {},
   "source": [
    "Calendar and Holiday Features Creation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba94a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -----------------\n",
    "# Calendar Features\n",
    "# -----------------\n",
    "def add_calendar_features(df):\n",
    "    df[\"day_of_month\"] = df[\"DATE\"].dt.day\n",
    "    df[\"month\"] = df[\"DATE\"].dt.month\n",
    "    df[\"day_of_week\"] = df[\"DATE\"].dt.dayofweek \n",
    "\n",
    "    # --- Day of month dummies ---\n",
    "    dom_dummies = pd.get_dummies(df[\"day_of_month\"], prefix=\"dom\")\n",
    "    dom_dummies = dom_dummies.drop(columns=[\"dom_1\"], errors=\"ignore\")\n",
    "\n",
    "    # --- Month dummies ---\n",
    "    month_dummies = pd.get_dummies(df[\"month\"], prefix=\"month\")\n",
    "    month_dummies = month_dummies.drop(columns=[\"month_12\"], errors=\"ignore\")\n",
    "\n",
    "    # --- Day of week dummies ---\n",
    "    dow_dummies = pd.get_dummies(df[\"day_of_week\"], prefix=\"dow\")\n",
    "    dow_dummies = dow_dummies.drop(columns=[\"dow_0\"], errors=\"ignore\")  \n",
    "\n",
    "    # Drop raw columns\n",
    "    df = df.drop(columns=[\"day_of_month\", \"month\", \"day_of_week\"])\n",
    "    df = pd.concat([df, dom_dummies, month_dummies, dow_dummies], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "# -----------------\n",
    "# Holiday Features\n",
    "# -----------------\n",
    "def add_holiday_features(df, official_holidays, half_days):\n",
    "    df[\"DATE\"] = pd.to_datetime(df[\"DATE\"])\n",
    "    official_holidays = pd.to_datetime(official_holidays)\n",
    "    half_days = pd.to_datetime(half_days)\n",
    "\n",
    "    df[\"day_official_holiday\"] = df[\"DATE\"].isin(official_holidays).astype(int)\n",
    "    df[\"day_half_day\"] = df[\"DATE\"].isin(half_days).astype(int)\n",
    "\n",
    "    # Everything else = normal day\n",
    "    df[\"day_normal_day\"] = (\n",
    "        (df[\"day_official_holiday\"] == 0) &\n",
    "        (df[\"day_half_day\"] == 0)\n",
    "    ).astype(int)\n",
    "    return df\n",
    "\n",
    "# -----------------\n",
    "# Final Wrapper\n",
    "# -----------------\n",
    "def feature_engineering_pipeline(atm, official_holidays, half_days):\n",
    "    df = atm.copy()\n",
    "    df = add_calendar_features(df)  \n",
    "    df = add_holiday_features(df, official_holidays, half_days)\n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e0fbf3",
   "metadata": {},
   "source": [
    "Helper Functions (Clustering, Data Prep, Forecast Metrics, Normalization, Reverse Transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a35b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------\n",
    "# 1. Data Preparation\n",
    "# -----------------\n",
    "def prepare_data(df, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Prepare features (X) and target (y), then split train/test by time.\n",
    "    Target is kept raw (WITHDRWLS).\n",
    "    Normalization must be applied later with normalize_target().\n",
    "    \"\"\"\n",
    "    feature_cols = [\n",
    "        col for col in df.columns\n",
    "        if col not in [\n",
    "            \"CASHP_ID\", \"DATE\", \"WITHDRWLS\",\n",
    "            \"day_of_week\", \"day_of_month\",\n",
    "            \"is_active\", \"in_active\", \"cluster\"  # exclude cluster\n",
    "        ]\n",
    "    ]\n",
    "    X = df[feature_cols]\n",
    "    y = df[\"WITHDRWLS\"]\n",
    "\n",
    "    split_idx = int(len(df) * (1 - test_size))\n",
    "    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# -----------------\n",
    "# 2. ATM Clustering (range of clusters)\n",
    "# -----------------\n",
    "def cluster_atms_range(df, cluster_range=(1, 4), zero_run_threshold=14):\n",
    "    \"\"\"\n",
    "    Cluster ATMs for a range of cluster sizes.\n",
    "    Uses withdrawals with zeros allowed, but discards sequences of ≥zero_run_threshold consecutive zeros.\n",
    "    Returns dict {n_clusters: (clustered_df, kmeans_model)}.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Step 1: Drop sequences of consecutive zeros ≥ threshold\n",
    "    def drop_long_zero_runs(group):\n",
    "        mask = np.ones(len(group), dtype=bool)  # True = keep, False = drop\n",
    "        zero_count = 0\n",
    "\n",
    "        for i, val in enumerate(group[\"WITHDRWLS\"].values):\n",
    "            if val == 0:\n",
    "                zero_count += 1\n",
    "            else:\n",
    "                if zero_count >= zero_run_threshold:\n",
    "                    mask[i - zero_count:i] = False  # discard long zero run\n",
    "                zero_count = 0\n",
    "\n",
    "        # Handle case where series ends with zeros\n",
    "        if zero_count >= zero_run_threshold:\n",
    "            mask[len(group) - zero_count:] = False\n",
    "\n",
    "        return group[mask]\n",
    "\n",
    "    filtered_df = df.groupby(\"CASHP_ID\", group_keys=False).apply(drop_long_zero_runs)\n",
    "\n",
    "    # Step 2: ATM profiles based on filtered withdrawals\n",
    "    atm_profiles = filtered_df.groupby(\"CASHP_ID\")[\"WITHDRWLS\"].mean().to_frame(\"avg_withdrawals\")\n",
    "\n",
    "    # Step 3: Clustering for each cluster size\n",
    "    results = {}\n",
    "    for n in range(cluster_range[0], cluster_range[1] + 1):\n",
    "        kmeans = KMeans(n_clusters=n, random_state=42, n_init=10)\n",
    "        atm_profiles[f\"cluster_{n}\"] = kmeans.fit_predict(atm_profiles[[\"avg_withdrawals\"]])\n",
    "\n",
    "        # Merge cluster labels back to the *filtered* dataset\n",
    "        df_clustered = filtered_df.merge(\n",
    "            atm_profiles[[f\"cluster_{n}\"]], on=\"CASHP_ID\", how=\"left\"\n",
    "        )\n",
    "        df_clustered = df_clustered.rename(columns={f\"cluster_{n}\": \"cluster\"})\n",
    "\n",
    "        results[n] = (df_clustered, kmeans)\n",
    "\n",
    "    return results\n",
    "# -----------------\n",
    "# 3. Forecast Metrics\n",
    "# -----------------\n",
    "def calculate_forecast_metrics(y_true, y_pred, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Calculate MAE, RMSE, MAPE, and SMAPE for forecasting.\n",
    "    Metrics are calculated in raw units.\n",
    "    \"\"\"\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-9))) * 100\n",
    "    smape = np.mean(2 * np.abs(y_pred - y_true) /\n",
    "                    (np.abs(y_true) + np.abs(y_pred) + 1e-9)) * 100\n",
    "\n",
    "    results = {\n",
    "        \"Model\": model_name,\n",
    "        \"MAE\": mae,\n",
    "        \"RMSE\": rmse,\n",
    "        \"MAPE\": mape,\n",
    "        \"SMAPE\": smape\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n",
    "# -----------------\n",
    "# 4. Normalization (target only, applied after split)\n",
    "# -----------------\n",
    "def normalize_target(y_train, y_test, method=\"log\"):\n",
    "    \"\"\"\n",
    "    Normalize target values after train/test split.\n",
    "    Prevents data leakage by fitting only on training set.\n",
    "    \n",
    "    Returns:\n",
    "        y_train_norm, y_test_norm, scaler\n",
    "    \"\"\"\n",
    "    if method == \"log\":\n",
    "        y_train_norm = np.log1p(y_train)\n",
    "        y_test_norm  = np.log1p(y_test)\n",
    "        scaler = None\n",
    "\n",
    "    elif method == \"minmax\":\n",
    "        scaler = MinMaxScaler()\n",
    "        y_train_norm = scaler.fit_transform(y_train.to_frame()).ravel()\n",
    "        y_test_norm  = scaler.transform(y_test.to_frame()).ravel()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"method must be 'log' or 'minmax'\")\n",
    "\n",
    "    return y_train_norm, y_test_norm, scaler\n",
    "\n",
    "# -----------------\n",
    "# 5. Reverse transform predictions\n",
    "# -----------------\n",
    "def inverse_transform_predictions(y_pred_norm, method=\"log\", scaler=None):\n",
    "    \"\"\"\n",
    "    Reverse transform model predictions back to raw scale.\n",
    "    Uses the same method as in normalize_target.\n",
    "    \"\"\"\n",
    "    if method == \"log\":\n",
    "        return np.expm1(y_pred_norm)\n",
    "    elif method == \"minmax\" and scaler is not None:\n",
    "        return scaler.inverse_transform(y_pred_norm.reshape(-1,1)).ravel()\n",
    "    else:\n",
    "        return y_pred_norm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807146b7",
   "metadata": {},
   "source": [
    "LSTM with Feature Set 0 (No Feature Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36d3cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import optuna\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------\n",
    "# Dataset (target-only)\n",
    "# -----------------\n",
    "class SeqOnlyDataset(Dataset):\n",
    "    def __init__(self, series, seq_len=30):\n",
    "        self.samples = []\n",
    "        series = np.array(series, dtype=np.float32)\n",
    "\n",
    "        for i in range(len(series) - seq_len):\n",
    "            X_seq = series[i:i+seq_len].reshape(-1, 1) \n",
    "            y_seq = series[i+seq_len]\n",
    "\n",
    "            # Skip if *any* value in the sequence OR the target is 0\n",
    "            if np.any(X_seq == 0) or y_seq == 0:\n",
    "                continue\n",
    "\n",
    "            self.samples.append((X_seq, y_seq))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X_seq, y_seq = self.samples[idx]\n",
    "        return (\n",
    "            torch.tensor(X_seq, dtype=torch.float32),\n",
    "            torch.tensor(y_seq, dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "# -----------------\n",
    "# LSTM Model (Univariate)\n",
    "# -----------------\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, hidden_dim, num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])  \n",
    "        return out.view(-1)         \n",
    "        \n",
    "# -----------------\n",
    "# Runner: LSTM Univariate + Clustering + Optuna\n",
    "# -----------------\n",
    "def run_lstm_univariate_with_clustering(df, cluster_range=(1,4), seq_lens=[7,14,30], n_trials=30, norm_method=\"minmax\"):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    all_metrics = []\n",
    "\n",
    "    clustered_results = cluster_atms_range(df, cluster_range=cluster_range)\n",
    "\n",
    "    for n_clusters, (df_clustered, _) in clustered_results.items():\n",
    "        print(f\"\\n=== LSTM (Univariate) with {n_clusters} Clusters ===\")\n",
    "\n",
    "        for seq_len in seq_lens:\n",
    "            print(f\"\\n--- Seq_len={seq_len} ---\")\n",
    "\n",
    "            for cluster_id in sorted(df_clustered[\"cluster\"].dropna().unique()):\n",
    "                cluster_df = df_clustered[df_clustered[\"cluster\"] == cluster_id].copy()\n",
    "\n",
    "                # --- Use prepare_data for time split ---\n",
    "                _, _, y_train, y_val = prepare_data(cluster_df, test_size=0.2)\n",
    "\n",
    "                # --- Normalize target ---\n",
    "                y_train_norm, y_val_norm, scaler = normalize_target(y_train, y_val, method=norm_method)\n",
    "\n",
    "                trial_metrics = []\n",
    "\n",
    "                def objective(trial):\n",
    "                    hidden_dim = trial.suggest_int(\"hidden_dim\", 16, 128, step=16)\n",
    "                    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "                    dropout    = trial.suggest_float(\"dropout\", 0.0, 0.5)\n",
    "                    lr         = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "                    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "\n",
    "                    train_dataset = SeqOnlyDataset(y_train_norm, seq_len)\n",
    "                    val_dataset   = SeqOnlyDataset(y_val_norm, seq_len)\n",
    "\n",
    "                    if len(train_dataset) == 0 or len(val_dataset) == 0:\n",
    "                        print(f\"Skipping: cluster={cluster_id}, seq_len={seq_len}, trial={trial.number} (no valid samples)\")\n",
    "                        return float(\"inf\")\n",
    "\n",
    "                    train_loader  = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "                    val_loader    = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "                    model = LSTMModel(\n",
    "                        input_dim=1,\n",
    "                        hidden_dim=hidden_dim,\n",
    "                        num_layers=num_layers,\n",
    "                        dropout=dropout\n",
    "                    ).to(device)\n",
    "\n",
    "                    criterion = nn.MSELoss()\n",
    "                    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "                    # Training\n",
    "                    for _ in range(20):\n",
    "                        model.train()\n",
    "                        for X_batch, y_batch in train_loader:\n",
    "                            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                            optimizer.zero_grad()\n",
    "                            preds = model(X_batch)\n",
    "                            loss = criterion(preds, y_batch)\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # Validation\n",
    "                    model.eval()\n",
    "                    preds_norm, trues_norm = [], []\n",
    "                    with torch.no_grad():\n",
    "                        for X_batch, y_batch in val_loader:\n",
    "                            X_batch = X_batch.to(device)\n",
    "                            out = model(X_batch).cpu().numpy()\n",
    "                            preds_norm.extend(out)\n",
    "                            trues_norm.extend(y_batch.numpy())\n",
    "\n",
    "                    preds = inverse_transform_predictions(\n",
    "                        np.array(preds_norm), method=norm_method, scaler=scaler\n",
    "                    )\n",
    "                    trues = inverse_transform_predictions(\n",
    "                        np.array(trues_norm), method=norm_method, scaler=scaler\n",
    "                    )\n",
    "\n",
    "                    metrics = calculate_forecast_metrics(\n",
    "                        trues, preds, model_name=f\"LSTM trial {trial.number}\"\n",
    "                    )\n",
    "                    metrics[\"trial\"] = trial.number\n",
    "                    trial_metrics.append(metrics)\n",
    "\n",
    "                    return metrics[\"SMAPE\"]\n",
    "\n",
    "                # Run Optuna\n",
    "                study = optuna.create_study(direction=\"minimize\")\n",
    "                study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "                if len(trial_metrics) == 0:\n",
    "                    print(f\"⚠️ No valid trials for cluster={cluster_id}, seq_len={seq_len}, n_clusters={n_clusters}\")\n",
    "                    continue\n",
    "\n",
    "                # Best trial\n",
    "                best_trial = study.best_trial.number\n",
    "                best_metrics = [m for m in trial_metrics if m[\"trial\"] == best_trial][0]\n",
    "                best_metrics.update({\n",
    "                    \"Seq_len\": seq_len,\n",
    "                    \"Best_Params\": study.best_trial.params,\n",
    "                    \"n_clusters\": n_clusters,\n",
    "                    \"cluster_id\": cluster_id\n",
    "                })\n",
    "\n",
    "                all_metrics.append(best_metrics)\n",
    "\n",
    "    return pd.DataFrame(all_metrics)\n",
    "\n",
    "# -----------------\n",
    "# Run\n",
    "# -----------------\n",
    "no_feature_lstm = run_lstm_univariate_with_clustering(\n",
    "    feature_eng_data,\n",
    "    cluster_range=(1,4),\n",
    "    seq_lens=[7, 14, 30, 90],\n",
    "    n_trials=20,\n",
    "    norm_method=\"log\"   # or \"minmax\"\n",
    ")\n",
    "\n",
    "print(no_feature_lstm)\n",
    "save_df_to_downloads(\"no_feature_lstm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b5a164",
   "metadata": {},
   "source": [
    "N-Beats with Feature Set 0 (No Feature Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad41414e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import optuna\n",
    "\n",
    "# -----------------\n",
    "# Dataset (target-only, drops zero sequences)\n",
    "# -----------------\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, y, seq_len=30):\n",
    "        self.samples = []\n",
    "        y = np.array(y, dtype=np.float32)\n",
    "\n",
    "        for i in range(len(y) - seq_len):\n",
    "            X_seq = y[i:i + seq_len]\n",
    "            y_seq = y[i + seq_len]\n",
    "\n",
    "            # Drop if target == 0\n",
    "            if y_seq == 0:\n",
    "                continue\n",
    "\n",
    "            self.samples.append((X_seq, y_seq))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X_seq, y_seq = self.samples[idx]\n",
    "        return (\n",
    "            torch.tensor(X_seq, dtype=torch.float32).view(-1),\n",
    "            torch.tensor(y_seq, dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "# -----------------\n",
    "# N-BEATS Model (Univariate)\n",
    "# -----------------\n",
    "class NBeatsBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim, theta_dim, backcast_length, forecast_length):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(backcast_length, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, theta_dim)\n",
    "        self.backcast_length = backcast_length\n",
    "        self.forecast_length = forecast_length\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        theta = self.fc4(x)\n",
    "        backcast = theta[:, :self.backcast_length]\n",
    "        forecast = theta[:, -self.forecast_length:]\n",
    "        return backcast, forecast\n",
    "\n",
    "\n",
    "class NBeats(nn.Module):\n",
    "    def __init__(self, hidden_dim, backcast_length, forecast_length, n_blocks=3):\n",
    "        super().__init__()\n",
    "        theta_dim = backcast_length + forecast_length\n",
    "        self.blocks = nn.ModuleList([\n",
    "            NBeatsBlock(hidden_dim, theta_dim, backcast_length, forecast_length)\n",
    "            for _ in range(n_blocks)\n",
    "        ])\n",
    "        self.forecast_length = forecast_length\n",
    "\n",
    "    def forward(self, x):\n",
    "        backcast = x.view(x.size(0), -1)\n",
    "        forecast = torch.zeros(x.size(0), self.forecast_length, device=x.device)\n",
    "        for block in self.blocks:\n",
    "            b, f = block(backcast)\n",
    "            backcast = backcast - b\n",
    "            forecast = forecast + f\n",
    "        return forecast.squeeze(-1)\n",
    "\n",
    "# -----------------\n",
    "# Runner with Optuna\n",
    "# -----------------\n",
    "def run_nbeats_univariate_with_clustering(df, cluster_range=(1, 4),\n",
    "                                          seq_lens=[7, 14, 30],\n",
    "                                          norm_method=\"minmax\",\n",
    "                                          n_trials=30):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    all_metrics = []\n",
    "\n",
    "    clustered_results = cluster_atms_range(df, cluster_range=cluster_range)\n",
    "\n",
    "    for n_clusters, (df_clustered, _) in clustered_results.items():\n",
    "        print(f\"\\n=== N-BEATS (Univariate) with {n_clusters} Clusters ===\")\n",
    "\n",
    "        for seq_len in seq_lens:\n",
    "            print(f\"\\n--- Seq_len={seq_len} ---\")\n",
    "\n",
    "            for cluster_id in sorted(df_clustered[\"cluster\"].dropna().unique()):\n",
    "                cluster_df = df_clustered[df_clustered[\"cluster\"] == cluster_id].copy()\n",
    "\n",
    "                # --- Use prepare_data for consistent split ---\n",
    "                _, _, y_train, y_val = prepare_data(cluster_df, test_size=0.2)\n",
    "\n",
    "                # --- Normalize target ---\n",
    "                y_train_norm, y_val_norm, scaler = normalize_target(y_train, y_val, method=norm_method)\n",
    "\n",
    "                train_dataset = SeqDataset(y_train_norm, seq_len)\n",
    "                val_dataset   = SeqDataset(y_val_norm, seq_len)\n",
    "\n",
    "                if len(train_dataset) == 0 or len(val_dataset) == 0:\n",
    "                    print(f\" Cluster {cluster_id}, seq_len {seq_len} has no valid (nonzero) sequences. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                # -----------------\n",
    "                # Optuna objective\n",
    "                # -----------------\n",
    "                def objective(trial):\n",
    "                    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 256, step=32)\n",
    "                    n_blocks   = trial.suggest_int(\"n_blocks\", 2, 6)\n",
    "                    lr         = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "                    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "\n",
    "                    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "                    val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "                    model = NBeats(hidden_dim=hidden_dim,\n",
    "                                   backcast_length=seq_len,\n",
    "                                   forecast_length=1,\n",
    "                                   n_blocks=n_blocks).to(device)\n",
    "                    criterion = nn.MSELoss()\n",
    "                    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "                    # Training\n",
    "                    for _ in range(20):\n",
    "                        model.train()\n",
    "                        for X_batch, y_batch in train_loader:\n",
    "                            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                            optimizer.zero_grad()\n",
    "                            preds = model(X_batch)\n",
    "                            loss = criterion(preds, y_batch)\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # Validation\n",
    "                    model.eval()\n",
    "                    preds_norm, trues_norm = [], []\n",
    "                    with torch.no_grad():\n",
    "                        for X_batch, y_batch in val_loader:\n",
    "                            X_batch = X_batch.to(device)\n",
    "                            out = model(X_batch).cpu().numpy()\n",
    "                            preds_norm.extend(out)\n",
    "                            trues_norm.extend(y_batch.numpy())\n",
    "\n",
    "                    preds = inverse_transform_predictions(np.array(preds_norm), method=norm_method, scaler=scaler)\n",
    "                    trues = inverse_transform_predictions(np.array(trues_norm), method=norm_method, scaler=scaler)\n",
    "\n",
    "                    smape = 100 * np.mean(2 * np.abs(preds - trues) / (np.abs(preds) + np.abs(trues) + 1e-9))\n",
    "                    return smape\n",
    "\n",
    "                # Run Optuna\n",
    "                study = optuna.create_study(direction=\"minimize\")\n",
    "                study.optimize(objective, n_trials=n_trials, show_progress_bar=False)\n",
    "\n",
    "                best_params = study.best_trial.params\n",
    "\n",
    "                # -----------------\n",
    "                # Retrain with best params\n",
    "                # -----------------\n",
    "                train_loader = DataLoader(train_dataset, batch_size=best_params[\"batch_size\"], shuffle=True)\n",
    "                val_loader   = DataLoader(val_dataset, batch_size=best_params[\"batch_size\"], shuffle=False)\n",
    "\n",
    "                model = NBeats(hidden_dim=best_params[\"hidden_dim\"],\n",
    "                               backcast_length=seq_len,\n",
    "                               forecast_length=1,\n",
    "                               n_blocks=best_params[\"n_blocks\"]).to(device)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=best_params[\"lr\"])\n",
    "                criterion = nn.MSELoss()\n",
    "\n",
    "                for _ in range(20):\n",
    "                    model.train()\n",
    "                    for X_batch, y_batch in train_loader:\n",
    "                        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                        optimizer.zero_grad()\n",
    "                        preds = model(X_batch)\n",
    "                        loss = criterion(preds, y_batch)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Final validation\n",
    "                model.eval()\n",
    "                preds_norm, trues_norm = [], []\n",
    "                with torch.no_grad():\n",
    "                    for X_batch, y_batch in val_loader:\n",
    "                        X_batch = X_batch.to(device)\n",
    "                        out = model(X_batch).cpu().numpy()\n",
    "                        preds_norm.extend(out)\n",
    "                        trues_norm.extend(y_batch.numpy())\n",
    "\n",
    "                preds = inverse_transform_predictions(np.array(preds_norm), method=norm_method, scaler=scaler)\n",
    "                trues = inverse_transform_predictions(np.array(trues_norm), method=norm_method, scaler=scaler)\n",
    "\n",
    "                metrics = calculate_forecast_metrics(\n",
    "                    trues, preds,\n",
    "                    model_name=f\"N-BEATS-Uni (k={n_clusters}, c={cluster_id}, seq={seq_len})\"\n",
    "                )\n",
    "                metrics.update({\n",
    "                    \"Seq_len\": seq_len,\n",
    "                    \"n_clusters\": n_clusters,\n",
    "                    \"cluster_id\": cluster_id,\n",
    "                    \"Best_Params\": best_params\n",
    "                })\n",
    "\n",
    "                all_metrics.append(metrics)\n",
    "\n",
    "    return pd.DataFrame(all_metrics)\n",
    "\n",
    "# -----------------\n",
    "# Example Run\n",
    "# -----------------\n",
    "no_feature_nbeats = run_nbeats_univariate_with_clustering(\n",
    "    feature_eng_data,\n",
    "    cluster_range=(1, 4),\n",
    "    seq_lens=[7, 14,30,90],\n",
    "    norm_method=\"log\",\n",
    "    n_trials=20\n",
    ")\n",
    "\n",
    "print(no_feature_nbeats)\n",
    "save_df_to_downloads(\"no_feature_nbeats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540a21de",
   "metadata": {},
   "source": [
    "Linear Regression with Feature Set 1 (Dummy Variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e36132",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def run_linear_regression_per_cluster(df, cluster_range=(1,4), test_size=0.2, norm_method=\"log\"):\n",
    "    # Drop zero withdrawals globally\n",
    "    df = df[df[\"WITHDRWLS\"] > 0].copy()\n",
    "\n",
    "    all_metrics = {}\n",
    "    clustered_results = cluster_atms_range(df, cluster_range=cluster_range)\n",
    "\n",
    "    for n_clusters, (df_clustered, _) in clustered_results.items():\n",
    "        print(f\"\\n=== Linear Regression with {n_clusters} Clusters ===\")\n",
    "\n",
    "        for cluster_id in sorted(df_clustered[\"cluster\"].dropna().unique()):\n",
    "            cluster_df = df_clustered[df_clustered[\"cluster\"] == cluster_id].copy()\n",
    "            if cluster_df.empty:\n",
    "                print(f\"Cluster {cluster_id} has no valid rows. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # --- Use unified prepare_data ---\n",
    "            X_train, X_test, y_train, y_test = prepare_data(cluster_df, test_size=test_size)\n",
    "\n",
    "            # --- Normalize AFTER split ---\n",
    "            y_train_norm, y_test_norm, scaler = normalize_target(y_train, y_test, method=norm_method)\n",
    "\n",
    "            # --- Train ---\n",
    "            model = LinearRegression()\n",
    "            model.fit(X_train, y_train_norm)\n",
    "\n",
    "            # --- Predict & inverse transform ---\n",
    "            y_pred_norm = model.predict(X_test)\n",
    "            y_pred_raw = inverse_transform_predictions(y_pred_norm, method=norm_method, scaler=scaler)\n",
    "\n",
    "            # --- Evaluate ---\n",
    "            metrics = calculate_forecast_metrics(\n",
    "                y_test, y_pred_raw,\n",
    "                model_name=f\"LR (k={n_clusters}, cluster={cluster_id})\"\n",
    "            )\n",
    "            metrics[\"n_clusters\"] = n_clusters\n",
    "            metrics[\"cluster_id\"] = cluster_id\n",
    "\n",
    "            all_metrics.setdefault(n_clusters, []).append(metrics)\n",
    "\n",
    "    # Flatten results\n",
    "    results = [m for cluster_metrics in all_metrics.values() for m in cluster_metrics]\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# --- Run example ---\n",
    "dummy_var_lin_reg = run_linear_regression_per_cluster(\n",
    "    feature_eng_data,\n",
    "    cluster_range=(1,4),\n",
    "    test_size=0.2,\n",
    "    norm_method=\"log\"\n",
    ")\n",
    "\n",
    "print(dummy_var_lin_reg)\n",
    "save_df_to_downloads(\"dummy_var_lin_reg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcaab85",
   "metadata": {},
   "source": [
    "XGBoost with Feature Set 1 (Dummy Variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4705ff1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def run_xgboost_per_cluster(df, cluster_range=(1,4), test_size=0.2, method=\"log\", n_trials=50):\n",
    "\n",
    "    # Drop zero withdrawals globally\n",
    "    df = df[df[\"WITHDRWLS\"] > 0].copy()\n",
    "\n",
    "    all_metrics = []\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "    clustered_results = cluster_atms_range(df, cluster_range=cluster_range)\n",
    "\n",
    "    for n_clusters, (df_clustered, _) in clustered_results.items():\n",
    "        print(f\"\\n=== XGBoost with {n_clusters} Clusters ===\")\n",
    "\n",
    "        for cluster_id in sorted(df_clustered[\"cluster\"].dropna().unique()):\n",
    "            cluster_df = df_clustered[df_clustered[\"cluster\"] == cluster_id].copy()\n",
    "            if cluster_df.empty:\n",
    "                print(f\"Cluster {cluster_id} has no valid rows. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # --- Use unified prepare_data ---\n",
    "            X_train, X_val, y_train, y_val = prepare_data(cluster_df, test_size=test_size)\n",
    "\n",
    "            # --- Normalize AFTER split ---\n",
    "            y_train_norm, y_val_norm, scaler = normalize_target(y_train, y_val, method=method)\n",
    "\n",
    "            # --- Objective for Optuna ---\n",
    "            def objective(trial):\n",
    "                params = {\n",
    "                    \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 1000),\n",
    "                    \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.1, log=True),\n",
    "                    \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "                    \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "                    \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n",
    "                    \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "                    \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "                    \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True),\n",
    "                    \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n",
    "                    \"random_state\": 42,\n",
    "                    \"tree_method\": \"hist\"\n",
    "                }\n",
    "\n",
    "                model = xgb.XGBRegressor(**params)\n",
    "                model.fit(X_train, y_train_norm,\n",
    "                          eval_set=[(X_val, y_val_norm)],\n",
    "                          verbose=False)\n",
    "\n",
    "                # Predict & inverse transform\n",
    "                y_pred_norm = model.predict(X_val)\n",
    "                y_pred_raw = inverse_transform_predictions(y_pred_norm, method=method, scaler=scaler)\n",
    "\n",
    "                # Evaluate\n",
    "                metrics = calculate_forecast_metrics(\n",
    "                    y_val, y_pred_raw,\n",
    "                    model_name=f\"XGB (k={n_clusters}, cluster={cluster_id})\"\n",
    "                )\n",
    "                trial.set_user_attr(\"mae\", metrics[\"MAE\"])\n",
    "                return metrics[\"SMAPE\"]\n",
    "\n",
    "            # Run Optuna\n",
    "            study = optuna.create_study(direction=\"minimize\")\n",
    "            study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "            # Collect best results\n",
    "            best_smape = study.best_value\n",
    "            best_mae   = study.best_trial.user_attrs[\"mae\"]\n",
    "            best_params = study.best_trial.params\n",
    "\n",
    "            print(f\"Cluster {cluster_id} | Best SMAPE={best_smape:.2f}% | MAE={best_mae:.2f}\")\n",
    "\n",
    "            all_metrics.append({\n",
    "                \"Model\": f\"XGB (k={n_clusters}, cluster={cluster_id})\",\n",
    "                \"Best_SMAPE\": best_smape,\n",
    "                \"Best_MAE\": best_mae,\n",
    "                \"Best_Params\": best_params,\n",
    "                \"n_clusters\": n_clusters,\n",
    "                \"cluster_id\": cluster_id\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(all_metrics)\n",
    "\n",
    "\n",
    "# Run\n",
    "dummy_var_xgb = run_xgboost_per_cluster(\n",
    "    feature_eng_data,\n",
    "    cluster_range=(1,4),\n",
    "    test_size=0.2,\n",
    "    method=\"log\",\n",
    "    n_trials=100\n",
    ")\n",
    "\n",
    "print(dummy_var_xgb)\n",
    "save_df_to_downloads(\"dummy_var_xgb\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab21580",
   "metadata": {},
   "source": [
    "LSTM with Feature Set 1 (Dummy Variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71720850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import optuna\n",
    "\n",
    "# -----------------\n",
    "# Dataset Class (drops sequences with zeros)\n",
    "# -----------------\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, X, y, seq_len=30):\n",
    "        self.samples = []\n",
    "        X = np.array(X, dtype=np.float32)\n",
    "        y = np.array(y, dtype=np.float32)\n",
    "\n",
    "        for i in range(len(X) - seq_len):\n",
    "            X_seq = X[i:i+seq_len]\n",
    "            y_seq = y[i+seq_len]\n",
    "\n",
    "            if y_seq == 0:\n",
    "                continue\n",
    "            self.samples.append((X_seq, y_seq))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X_seq, y_seq = self.samples[idx]\n",
    "        return torch.tensor(X_seq, dtype=torch.float32), torch.tensor(y_seq, dtype=torch.float32)\n",
    "\n",
    "# -----------------\n",
    "# LSTM Model\n",
    "# -----------------\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers,\n",
    "                            batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out.view(-1)  \n",
    "\n",
    "# -----------------\n",
    "# Runner (with Optuna + dropping zeros)\n",
    "# -----------------\n",
    "def run_lstm_with_clustering(df, cluster_range=(1,4), seq_lens=[7,14,30], norm_method=\"minmax\", n_trials=30):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    all_metrics = []\n",
    "\n",
    "    clustered_results = cluster_atms_range(df, cluster_range=cluster_range)\n",
    "\n",
    "    for n_clusters, (df_clustered, _) in clustered_results.items():\n",
    "        print(f\"\\n=== LSTM with {n_clusters} Clusters ===\")\n",
    "\n",
    "        for seq_len in seq_lens:\n",
    "            print(f\"\\n--- Seq_len={seq_len} ---\")\n",
    "\n",
    "            for cluster_id in sorted(df_clustered[\"cluster\"].dropna().unique()):\n",
    "                cluster_df = df_clustered[df_clustered[\"cluster\"] == cluster_id].copy()\n",
    "\n",
    "                # --- Train/val split\n",
    "                X_train, X_val, y_train, y_val = prepare_data(cluster_df, test_size=0.2)\n",
    "                X_train, X_val = X_train.astype(np.float32), X_val.astype(np.float32)\n",
    "\n",
    "                # --- Normalize target\n",
    "                y_train_norm, y_val_norm, scaler = normalize_target(y_train, y_val, method=norm_method)\n",
    "\n",
    "                # --- Build datasets (zeros dropped automatically inside SeqDataset)\n",
    "                train_dataset = SeqDataset(X_train.values, y_train_norm, seq_len)\n",
    "                val_dataset   = SeqDataset(X_val.values, y_val_norm, seq_len)\n",
    "\n",
    "                if len(train_dataset) == 0 or len(val_dataset) == 0:\n",
    "                    print(f\"Cluster {cluster_id}, seq_len {seq_len} has no valid (nonzero) sequences. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                # -----------------\n",
    "                # Optuna Objective\n",
    "                # -----------------\n",
    "                def objective(trial):\n",
    "                    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 128, step=16)\n",
    "                    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "                    dropout    = trial.suggest_float(\"dropout\", 0.0, 0.5)\n",
    "                    lr         = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "                    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "\n",
    "                    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "                    val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "                    model = LSTMModel(input_dim=X_train.shape[1], hidden_dim=hidden_dim,\n",
    "                                      num_layers=num_layers, dropout=dropout).to(device)\n",
    "                    criterion = nn.MSELoss()\n",
    "                    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "                    # Training\n",
    "                    for _ in range(20):\n",
    "                        model.train()\n",
    "                        for X_batch, y_batch in train_loader:\n",
    "                            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                            optimizer.zero_grad()\n",
    "                            preds = model(X_batch)\n",
    "                            loss = criterion(preds, y_batch)\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # Validation\n",
    "                    model.eval()\n",
    "                    preds_norm, trues_norm = [], []\n",
    "                    with torch.no_grad():\n",
    "                        for X_batch, y_batch in val_loader:\n",
    "                            X_batch = X_batch.to(device)\n",
    "                            out = model(X_batch).cpu().numpy()\n",
    "                            preds_norm.extend(out)\n",
    "                            trues_norm.extend(y_batch.numpy())\n",
    "\n",
    "                    preds = inverse_transform_predictions(np.array(preds_norm), method=norm_method, scaler=scaler)\n",
    "                    trues = inverse_transform_predictions(np.array(trues_norm), method=norm_method, scaler=scaler)\n",
    "\n",
    "                    smape = 100 * np.mean(2 * np.abs(preds - trues) / (np.abs(preds) + np.abs(trues) + 1e-9))\n",
    "                    return smape\n",
    "\n",
    "                # Run Optuna silently\n",
    "                study = optuna.create_study(direction=\"minimize\")\n",
    "                study.optimize(objective, n_trials=n_trials, show_progress_bar=False)\n",
    "\n",
    "                # Best params\n",
    "                best_params = study.best_trial.params\n",
    "\n",
    "                # Retrain with best params\n",
    "                batch_size = best_params[\"batch_size\"]\n",
    "                train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "                val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "                model = LSTMModel(input_dim=X_train.shape[1],\n",
    "                                  hidden_dim=best_params[\"hidden_dim\"],\n",
    "                                  num_layers=best_params[\"num_layers\"],\n",
    "                                  dropout=best_params[\"dropout\"]).to(device)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=best_params[\"lr\"])\n",
    "                criterion = nn.MSELoss()\n",
    "\n",
    "                for _ in range(20):\n",
    "                    model.train()\n",
    "                    for X_batch, y_batch in train_loader:\n",
    "                        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                        optimizer.zero_grad()\n",
    "                        preds = model(X_batch)\n",
    "                        loss = criterion(preds, y_batch)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Final validation evaluation\n",
    "                model.eval()\n",
    "                preds_norm, trues_norm = [], []\n",
    "                with torch.no_grad():\n",
    "                    for X_batch, y_batch in val_loader:\n",
    "                        X_batch = X_batch.to(device)\n",
    "                        out = model(X_batch).cpu().numpy()\n",
    "                        preds_norm.extend(out)\n",
    "                        trues_norm.extend(y_batch.numpy())\n",
    "\n",
    "                preds = inverse_transform_predictions(np.array(preds_norm), method=norm_method, scaler=scaler)\n",
    "                trues = inverse_transform_predictions(np.array(trues_norm), method=norm_method, scaler=scaler)\n",
    "\n",
    "                metrics = calculate_forecast_metrics(\n",
    "                    trues, preds,\n",
    "                    model_name=f\"LSTM (k={n_clusters}, c={cluster_id}, seq={seq_len})\"\n",
    "                )\n",
    "                metrics.update({\n",
    "                    \"Seq_len\": seq_len,\n",
    "                    \"n_clusters\": n_clusters,\n",
    "                    \"cluster_id\": cluster_id,\n",
    "                    \"Best_Params\": best_params\n",
    "                })\n",
    "\n",
    "                all_metrics.append(metrics)\n",
    "\n",
    "    return pd.DataFrame(all_metrics)\n",
    "\n",
    "# --- Run LSTM with dummy vars, dropping zero-contaminated sequences ---\n",
    "dummy_var_lstm = run_lstm_with_clustering(\n",
    "    feature_eng_data,          \n",
    "    cluster_range=(1,4),       \n",
    "    seq_lens=[7, 14,30,90],  \n",
    "    norm_method=\"log\",       \n",
    "    n_trials=20                 \n",
    ")\n",
    "\n",
    "print(dummy_var_lstm)\n",
    "\n",
    "# Save results if needed\n",
    "save_df_to_downloads(\"dummy_var_lstm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430ce4ec",
   "metadata": {},
   "source": [
    "N-Beats with Feature Set 1 (Dummy Variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7279ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "# -----------------\n",
    "# Dataset (drops sequences with zeros)\n",
    "# -----------------\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, X, y, seq_len=30):\n",
    "        self.samples = []\n",
    "        X = np.array(X, dtype=np.float32)\n",
    "        y = np.array(y, dtype=np.float32)\n",
    "\n",
    "        for i in range(len(X) - seq_len):\n",
    "            X_seq = X[i:i+seq_len]\n",
    "            y_seq = y[i+seq_len]\n",
    "\n",
    "            if y_seq == 0:\n",
    "                continue\n",
    "\n",
    "            self.samples.append((X_seq, y_seq))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X_seq, y_seq = self.samples[idx]\n",
    "        return torch.tensor(X_seq, dtype=torch.float32), torch.tensor(y_seq, dtype=torch.float32)\n",
    "\n",
    "# -----------------\n",
    "# N-BEATS Model\n",
    "# -----------------\n",
    "class NBeatsBlock(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, theta_dim, backcast_length, forecast_length):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(backcast_length * input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, theta_dim)\n",
    "        self.backcast_length = backcast_length\n",
    "        self.forecast_length = forecast_length\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        theta = self.fc4(x)\n",
    "\n",
    "        backcast = theta[:, :self.backcast_length * self.input_dim].reshape(\n",
    "            x.size(0), self.backcast_length, self.input_dim\n",
    "        )\n",
    "        forecast = theta[:, -self.forecast_length:]\n",
    "        return backcast, forecast\n",
    "\n",
    "class NBeats(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, backcast_length, forecast_length, n_blocks=3):\n",
    "        super().__init__()\n",
    "        theta_dim = backcast_length * input_dim + forecast_length\n",
    "        self.blocks = nn.ModuleList([\n",
    "            NBeatsBlock(input_dim, hidden_dim, theta_dim, backcast_length, forecast_length)\n",
    "            for _ in range(n_blocks)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        backcast = x\n",
    "        forecast = 0\n",
    "        for block in self.blocks:\n",
    "            b, f = block(backcast)\n",
    "            backcast = backcast - b\n",
    "            forecast = forecast + f\n",
    "        return forecast.view(-1)\n",
    "\n",
    "# -----------------\n",
    "# Runner (with Optuna)\n",
    "# -----------------\n",
    "def run_nbeats_with_clustering(df, cluster_range=(1,4), seq_lens=[7,14,30], norm_method=\"minmax\", n_trials=30):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    all_metrics = []\n",
    "\n",
    "    clustered_results = cluster_atms_range(df, cluster_range=cluster_range)\n",
    "\n",
    "    for n_clusters, (df_clustered, _) in clustered_results.items():\n",
    "        print(f\"\\n=== N-BEATS with {n_clusters} Clusters ===\")\n",
    "\n",
    "        for seq_len in seq_lens:\n",
    "            print(f\"\\n--- Seq_len={seq_len} ---\")\n",
    "\n",
    "            for cluster_id in sorted(df_clustered[\"cluster\"].dropna().unique()):\n",
    "                cluster_df = df_clustered[df_clustered[\"cluster\"] == cluster_id].copy()\n",
    "\n",
    "                # --- Train/val split\n",
    "                X_train, X_val, y_train, y_val = prepare_data(cluster_df, test_size=0.2)\n",
    "                X_train, X_val = X_train.astype(np.float32), X_val.astype(np.float32)\n",
    "\n",
    "                # --- Normalize target\n",
    "                y_train_norm, y_val_norm, scaler = normalize_target(y_train, y_val, method=norm_method)\n",
    "\n",
    "                # --- Build datasets\n",
    "                train_dataset = SeqDataset(X_train.values, y_train_norm, seq_len)\n",
    "                val_dataset   = SeqDataset(X_val.values, y_val_norm, seq_len)\n",
    "\n",
    "                if len(train_dataset) == 0 or len(val_dataset) == 0:\n",
    "                    print(f\"Cluster {cluster_id}, seq_len {seq_len} has no valid sequences. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                # -----------------\n",
    "                # Optuna Objective\n",
    "                # -----------------\n",
    "                def objective(trial):\n",
    "                    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 256, step=32)\n",
    "                    n_blocks   = trial.suggest_int(\"n_blocks\", 2, 6)\n",
    "                    lr         = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "                    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "\n",
    "                    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "                    val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "                    model = NBeats(input_dim=X_train.shape[1], hidden_dim=hidden_dim,\n",
    "                                   backcast_length=seq_len, forecast_length=1,\n",
    "                                   n_blocks=n_blocks).to(device)\n",
    "\n",
    "                    criterion = nn.MSELoss()\n",
    "                    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "                    # Training\n",
    "                    for _ in range(20):\n",
    "                        model.train()\n",
    "                        for X_batch, y_batch in train_loader:\n",
    "                            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                            optimizer.zero_grad()\n",
    "                            preds = model(X_batch)\n",
    "                            loss = criterion(preds, y_batch)\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # Validation\n",
    "                    model.eval()\n",
    "                    preds_norm, trues_norm = [], []\n",
    "                    with torch.no_grad():\n",
    "                        for X_batch, y_batch in val_loader:\n",
    "                            X_batch = X_batch.to(device)\n",
    "                            out = model(X_batch).cpu().numpy()\n",
    "                            preds_norm.extend(out)\n",
    "                            trues_norm.extend(y_batch.numpy())\n",
    "\n",
    "                    preds = inverse_transform_predictions(np.array(preds_norm), method=norm_method, scaler=scaler)\n",
    "                    trues = inverse_transform_predictions(np.array(trues_norm), method=norm_method, scaler=scaler)\n",
    "\n",
    "                    smape = 100 * np.mean(2 * np.abs(preds - trues) / (np.abs(preds) + np.abs(trues) + 1e-9))\n",
    "                    return smape\n",
    "\n",
    "                # Run Optuna silently\n",
    "                study = optuna.create_study(direction=\"minimize\")\n",
    "                study.optimize(objective, n_trials=n_trials, show_progress_bar=False)\n",
    "\n",
    "                # Best trial\n",
    "                best_params = study.best_trial.params\n",
    "\n",
    "                # --- Retrain with best params\n",
    "                train_loader = DataLoader(train_dataset, batch_size=best_params[\"batch_size\"], shuffle=True)\n",
    "                val_loader   = DataLoader(val_dataset, batch_size=best_params[\"batch_size\"], shuffle=False)\n",
    "\n",
    "                model = NBeats(input_dim=X_train.shape[1], hidden_dim=best_params[\"hidden_dim\"],\n",
    "                               backcast_length=seq_len, forecast_length=1,\n",
    "                               n_blocks=best_params[\"n_blocks\"]).to(device)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=best_params[\"lr\"])\n",
    "                criterion = nn.MSELoss()\n",
    "\n",
    "                for _ in range(20):\n",
    "                    model.train()\n",
    "                    for X_batch, y_batch in train_loader:\n",
    "                        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                        optimizer.zero_grad()\n",
    "                        preds = model(X_batch)\n",
    "                        loss = criterion(preds, y_batch)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # --- Final validation\n",
    "                model.eval()\n",
    "                preds_norm, trues_norm = [], []\n",
    "                with torch.no_grad():\n",
    "                    for X_batch, y_batch in val_loader:\n",
    "                        X_batch = X_batch.to(device)\n",
    "                        out = model(X_batch).cpu().numpy()\n",
    "                        preds_norm.extend(out)\n",
    "                        trues_norm.extend(y_batch.numpy())\n",
    "\n",
    "                preds = inverse_transform_predictions(np.array(preds_norm), method=norm_method, scaler=scaler)\n",
    "                trues = inverse_transform_predictions(np.array(trues_norm), method=norm_method, scaler=scaler)\n",
    "\n",
    "                # --- Metrics\n",
    "                metrics = calculate_forecast_metrics(\n",
    "                    trues, preds,\n",
    "                    model_name=f\"N-BEATS (k={n_clusters}, c={cluster_id}, seq={seq_len})\"\n",
    "                )\n",
    "                metrics.update({\n",
    "                    \"Seq_len\": seq_len,\n",
    "                    \"n_clusters\": n_clusters,\n",
    "                    \"cluster_id\": cluster_id,\n",
    "                    \"Best_Params\": best_params\n",
    "                })\n",
    "\n",
    "                all_metrics.append(metrics)\n",
    "\n",
    "    return pd.DataFrame(all_metrics)\n",
    "\n",
    "# --- Run\n",
    "dummy_var_nbeats = run_nbeats_with_clustering(\n",
    "    feature_eng_data,\n",
    "    cluster_range=(1,4),\n",
    "    seq_lens=[7, 14,30,90],\n",
    "    norm_method=\"log\",  \n",
    "    n_trials=20\n",
    ")\n",
    "\n",
    "print(dummy_var_nbeats)\n",
    "\n",
    "save_df_to_downloads(\"dummy_var_nbeats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e84b961",
   "metadata": {},
   "source": [
    "Feature Set Creation for Feature Set 2 (Lag, Rolling Features, Normalization, Reverse Transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41f1c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lag & Rolling logic\n",
    "# -----------------\n",
    "def get_lag_and_rolls(window):\n",
    "    base_lags = [1, 2, 3, 7, 14, 28]\n",
    "    base_rolls = [7, 14, 28]\n",
    "    lags = [lag for lag in base_lags if lag <= window]\n",
    "    rolls = [r for r in base_rolls if r <= window]\n",
    "    return lags, rolls\n",
    "\n",
    "# -----------------\n",
    "# Feature Engineering\n",
    "# -----------------\n",
    "def create_lag_roll_features(df, seq_len):\n",
    "    lags, rolls = get_lag_and_rolls(seq_len)\n",
    "    groups = []\n",
    "    for atm_id, group in df.groupby(\"CASHP_ID\"):\n",
    "        group = group.sort_values(\"DATE\").copy()\n",
    "        # Lags\n",
    "        for lag in lags:\n",
    "            group[f\"lag_{lag}\"] = group[\"WITHDRWLS\"].shift(lag)\n",
    "        # Rolling\n",
    "        for r in rolls:\n",
    "            group[f\"roll_mean_{r}\"] = group[\"WITHDRWLS\"].shift(1).rolling(r).mean()\n",
    "            group[f\"roll_std_{r}\"] = group[\"WITHDRWLS\"].shift(1).rolling(r).std()\n",
    "        groups.append(group)\n",
    "    df_feat = pd.concat(groups).reset_index(drop=True)\n",
    "    df_feat = df_feat.dropna().reset_index(drop=True)\n",
    "    return df_feat\n",
    "\n",
    "\n",
    "# -----------------\n",
    "# Normalization helpers\n",
    "# -----------------\n",
    "def normalize_data(train_df, val_df, feature_cols, target_col=\"WITHDRWLS\", method=\"log\"):\n",
    "    \"\"\"\n",
    "    Normalize both features (X) and target (y) using the same method.\n",
    "    - log: apply log1p\n",
    "    - minmax: apply MinMaxScaler\n",
    "    Returns: X_train, X_val, y_train, y_val, feature_scaler, target_scaler\n",
    "    \"\"\"\n",
    "    if method == \"log\":\n",
    "        # Features\n",
    "        X_train = np.log1p(train_df[feature_cols].values)\n",
    "        X_val   = np.log1p(val_df[feature_cols].values)\n",
    "        feat_scaler = None\n",
    "        # Target\n",
    "        y_train = np.log1p(train_df[target_col].values)\n",
    "        y_val   = np.log1p(val_df[target_col].values)\n",
    "        target_scaler = None\n",
    "\n",
    "    elif method == \"minmax\":\n",
    "        feat_scaler = MinMaxScaler()\n",
    "        target_scaler = MinMaxScaler()\n",
    "        # Features\n",
    "        X_train = feat_scaler.fit_transform(train_df[feature_cols])\n",
    "        X_val   = feat_scaler.transform(val_df[feature_cols])\n",
    "        # Target\n",
    "        y_train = target_scaler.fit_transform(train_df[[target_col]]).ravel()\n",
    "        y_val   = target_scaler.transform(val_df[[target_col]]).ravel()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"method must be 'log' or 'minmax'\")\n",
    "\n",
    "    return X_train, X_val, y_train, y_val, feat_scaler, target_scaler\n",
    "\n",
    "def inverse_transform_predictions(y_pred, method=\"log\", scaler=None):\n",
    "    \"\"\"Reverse normalization for predictions back to raw scale.\"\"\"\n",
    "    if method == \"log\":\n",
    "        return np.expm1(y_pred)\n",
    "    elif method == \"minmax\" and scaler is not None:\n",
    "        return scaler.inverse_transform(y_pred.reshape(-1, 1)).ravel()\n",
    "    else:\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415967a4",
   "metadata": {},
   "source": [
    "Linear Regression with Feature Set 2 (Lag and Rolling Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff41babe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "\n",
    "def run_linear_regression_with_new_features(\n",
    "    atm,\n",
    "    cluster_range=(1,3),\n",
    "    seq_lens=[7,14,28],\n",
    "    norm_method=\"log\",\n",
    "    test_size=0.2\n",
    "):\n",
    "    all_metrics = []\n",
    "\n",
    "    for seq_len in seq_lens:\n",
    "        # --- Create lag/rolling features\n",
    "        df_feat = create_lag_roll_features(atm, seq_len)\n",
    "\n",
    "        # Drop rows where target == 0 or any lag/roll == 0\n",
    "        lag_roll_cols = [c for c in df_feat.columns if c.startswith((\"lag_\", \"roll_mean_\", \"roll_std_\"))]\n",
    "        df_feat = df_feat[(df_feat[\"WITHDRWLS\"] > 0) & (df_feat[lag_roll_cols] > 0).all(axis=1)]\n",
    "        df_feat = df_feat.reset_index(drop=True)\n",
    "        if df_feat.empty:\n",
    "            continue\n",
    "\n",
    "        # --- Drop dummy/calendar cols (not needed here)\n",
    "        drop_cols = [c for c in df_feat.columns \n",
    "                     if c.startswith((\"dow_\", \"dom_\", \"month_\", \"day_school_holiday\", \"week_part\"))]\n",
    "        df_feat = df_feat.drop(columns=drop_cols, errors=\"ignore\")\n",
    "\n",
    "        # --- Clustering\n",
    "        clustered_results = cluster_atms_range(df_feat, cluster_range=cluster_range)\n",
    "\n",
    "        for n_clusters, (df_clustered, _) in clustered_results.items():\n",
    "            for cluster_id in sorted(df_clustered[\"cluster\"].dropna().unique()):\n",
    "                cluster_df = df_clustered[df_clustered[\"cluster\"] == cluster_id].copy()\n",
    "\n",
    "                # Drop zero-contaminated rows at cluster level\n",
    "                cluster_df = cluster_df[(cluster_df[\"WITHDRWLS\"] > 0) & (cluster_df[lag_roll_cols] > 0).all(axis=1)]\n",
    "                cluster_df = cluster_df.reset_index(drop=True)\n",
    "                if cluster_df.empty:\n",
    "                    continue\n",
    "\n",
    "                # --- Time split\n",
    "                split_idx = int(len(cluster_df) * (1 - test_size))\n",
    "                train_df, val_df = cluster_df.iloc[:split_idx], cluster_df.iloc[split_idx:]\n",
    "\n",
    "                # --- Features: drop IDs + target + cluster\n",
    "                feature_cols = [c for c in cluster_df.columns \n",
    "                                if c not in [\"CASHP_ID\", \"DATE\", \"WITHDRWLS\", \"cluster\"]]\n",
    "\n",
    "                # --- Normalize features + target\n",
    "                X_train, X_val, y_train, y_val, feat_scaler, target_scaler = normalize_data(\n",
    "                    train_df, val_df, feature_cols, target_col=\"WITHDRWLS\", method=norm_method\n",
    "                )\n",
    "\n",
    "                # --- Train model\n",
    "                model = LinearRegression()\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                # --- Predict\n",
    "                y_pred_norm = model.predict(X_val)\n",
    "\n",
    "                # --- Reverse-transform\n",
    "                y_pred_raw = inverse_transform_predictions(y_pred_norm, method=norm_method, scaler=target_scaler)\n",
    "                y_true_raw = inverse_transform_predictions(y_val, method=norm_method, scaler=target_scaler)\n",
    "\n",
    "                # --- Evaluate\n",
    "                metrics = calculate_forecast_metrics(\n",
    "                    y_true_raw, y_pred_raw,\n",
    "                    model_name=f\"LR (seq={seq_len}, k={n_clusters}, c={cluster_id})\"\n",
    "                )\n",
    "                metrics.update({\n",
    "                    \"Seq_Length\": seq_len,\n",
    "                    \"Num_Clusters\": n_clusters,\n",
    "                    \"Cluster_ID\": cluster_id,\n",
    "                    \"Normalization\": norm_method\n",
    "                })\n",
    "                all_metrics.append(metrics)\n",
    "\n",
    "    return pd.DataFrame(all_metrics)\n",
    "\n",
    "# --- Run & Save\n",
    "rolling_features_lin_reg = run_linear_regression_with_new_features(\n",
    "    atm,\n",
    "    cluster_range=(1,4),\n",
    "    seq_lens=[7,14,28,84],\n",
    "    norm_method=\"log\"\n",
    ")\n",
    "\n",
    "print(rolling_features_lin_reg)\n",
    "save_df_to_downloads(\"rolling_features_lin_reg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b094c46",
   "metadata": {},
   "source": [
    "XGBoost with Feature Set 2 (Lag and Rolling Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e2a566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import optuna\n",
    "import pandas as pd\n",
    "\n",
    "def run_xgboost_with_new_features(\n",
    "    atm,\n",
    "    cluster_range=(1,3),\n",
    "    seq_lens=[7,14,28],\n",
    "    n_trials=50,\n",
    "    norm_method=\"log\",\n",
    "    test_size=0.2\n",
    "):\n",
    "    all_metrics = []\n",
    "\n",
    "    for seq_len in seq_lens:\n",
    "        # Create lag/rolling features\n",
    "        df_feat = create_lag_roll_features(atm, seq_len)\n",
    "\n",
    "        # Drop rows where target == 0 or any lag/roll == 0\n",
    "        lag_roll_cols = [c for c in df_feat.columns if c.startswith((\"lag_\", \"roll_mean_\", \"roll_std_\"))]\n",
    "        df_feat = df_feat[(df_feat[\"WITHDRWLS\"] > 0) & (df_feat[lag_roll_cols] > 0).all(axis=1)]\n",
    "        df_feat = df_feat.reset_index(drop=True)\n",
    "        if df_feat.empty:\n",
    "            continue\n",
    "\n",
    "        # Drop dummy/calendar columns\n",
    "        drop_cols = [c for c in df_feat.columns \n",
    "                     if c.startswith((\"dow_\", \"dom_\", \"month_\", \"day_school_holiday\", \"week_part\"))]\n",
    "        df_feat = df_feat.drop(columns=drop_cols, errors=\"ignore\")\n",
    "        \n",
    "        # Clustering\n",
    "        clustered_results = cluster_atms_range(df_feat, cluster_range=cluster_range)\n",
    "\n",
    "        for n_clusters, (df_clustered, _) in clustered_results.items():\n",
    "            for cluster_id in sorted(df_clustered[\"cluster\"].dropna().unique()):\n",
    "                cluster_df = df_clustered[df_clustered[\"cluster\"] == cluster_id].copy()\n",
    "\n",
    "                # Drop zero-contaminated rows again at cluster level\n",
    "                cluster_df = cluster_df[(cluster_df[\"WITHDRWLS\"] > 0) & (cluster_df[lag_roll_cols] > 0).all(axis=1)]\n",
    "                cluster_df = cluster_df.reset_index(drop=True)\n",
    "                if cluster_df.empty:\n",
    "                    continue\n",
    "\n",
    "                # Train/val split\n",
    "                split_idx = int(len(cluster_df) * (1 - test_size))\n",
    "                train_df, val_df = cluster_df.iloc[:split_idx], cluster_df.iloc[split_idx:]\n",
    "\n",
    "                # Features\n",
    "                feature_cols = [c for c in cluster_df.columns \n",
    "                                if c not in [\"CASHP_ID\", \"DATE\", \"WITHDRWLS\", \"cluster\"]]\n",
    "                # Normalize\n",
    "                X_train, X_val, y_train, y_val, feat_scaler, target_scaler = normalize_data(\n",
    "                    train_df, val_df, feature_cols, target_col=\"WITHDRWLS\", method=norm_method\n",
    "                )\n",
    "\n",
    "                trial_metrics = []\n",
    "\n",
    "                # Optuna objective\n",
    "                def objective(trial):\n",
    "                    params = {\n",
    "                        \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 1000),\n",
    "                        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.1, log=True),\n",
    "                        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "                        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "                        \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n",
    "                        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "                        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "                        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True),\n",
    "                        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n",
    "                        \"random_state\": 42,\n",
    "                        \"tree_method\": \"hist\"\n",
    "                    }\n",
    "\n",
    "                    model = xgb.XGBRegressor(**params)\n",
    "                    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "\n",
    "                    # --- Predict\n",
    "                    y_pred_norm = model.predict(X_val)\n",
    "\n",
    "                    # --- Reverse-transform\n",
    "                    y_pred_raw = inverse_transform_predictions(y_pred_norm, method=norm_method, scaler=target_scaler)\n",
    "                    y_true_raw = inverse_transform_predictions(y_val, method=norm_method, scaler=target_scaler)\n",
    "\n",
    "                    # --- Evaluate\n",
    "                    metrics = calculate_forecast_metrics(\n",
    "                        y_true_raw, y_pred_raw,\n",
    "                        model_name=f\"XGB trial {trial.number} (seq={seq_len}, k={n_clusters}, c={cluster_id})\"\n",
    "                    )\n",
    "                    metrics[\"trial\"] = trial.number\n",
    "                    trial_metrics.append(metrics)\n",
    "                    return metrics[\"SMAPE\"]\n",
    "\n",
    "                # --- Run Optuna\n",
    "                study = optuna.create_study(direction=\"minimize\")\n",
    "                optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "                study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "                # --- Best trial\n",
    "                best_trial = study.best_trial.number\n",
    "                best_metrics = [m for m in trial_metrics if m[\"trial\"] == best_trial][0]\n",
    "                best_metrics.update({\n",
    "                    \"Seq_Length\": seq_len,\n",
    "                    \"Best_Params\": study.best_trial.params,\n",
    "                    \"Num_Clusters\": n_clusters,\n",
    "                    \"Cluster_ID\": cluster_id,\n",
    "                    \"Normalization\": norm_method\n",
    "                })\n",
    "                all_metrics.append(best_metrics)\n",
    "\n",
    "    return pd.DataFrame(all_metrics)\n",
    "\n",
    "# --- Run & Save\n",
    "rolling_features_xgboost = run_xgboost_with_new_features(\n",
    "    atm,\n",
    "    cluster_range=(1,4),\n",
    "    seq_lens=[7,14,28,84],\n",
    "    n_trials=100,\n",
    "    norm_method=\"log\"\n",
    ")\n",
    "\n",
    "print(rolling_features_xgboost)\n",
    "save_df_to_downloads(\"rolling_features_xgboost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b86c85a",
   "metadata": {},
   "source": [
    "LSTM with Feature Set 2 (Lag and Rolling Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b7f8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import optuna\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------\n",
    "# Dataset\n",
    "# -----------------\n",
    "class SeqFeatureDataset(Dataset):\n",
    "    def __init__(self, X, y, seq_len=30):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.float32)\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X_seq = self.X[idx:idx+self.seq_len]\n",
    "        y_seq = self.y[idx+self.seq_len]\n",
    "        return torch.tensor(X_seq, dtype=torch.float32), torch.tensor(y_seq, dtype=torch.float32)\n",
    "\n",
    "# -----------------\n",
    "# LSTM Model\n",
    "# -----------------\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers,\n",
    "                            batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])  \n",
    "        return out.view(-1)        \n",
    "\n",
    "# -----------------\n",
    "# Runner with Feature Engineering + Clustering + Optuna\n",
    "# -----------------\n",
    "def run_lstm_with_rolling_and_clusters(\n",
    "    atm,                       \n",
    "    cluster_range=(1,4), \n",
    "    seq_lens=[7,14,28], \n",
    "    n_trials=20,\n",
    "    norm_method=\"log\"\n",
    "):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    all_results = []\n",
    "\n",
    "    for seq_len in seq_lens:\n",
    "        print(f\"\\n--- Seq_len={seq_len} ---\")\n",
    "\n",
    "        # --- Feature engineering\n",
    "        df_feat = create_lag_roll_features(atm, seq_len)\n",
    "\n",
    "        # Drop sequences with zero target or zero lag/roll features\n",
    "        lag_roll_cols = [c for c in df_feat.columns if c.startswith((\"lag_\", \"roll_mean_\", \"roll_std_\"))]\n",
    "        df_feat = df_feat[(df_feat[\"WITHDRWLS\"] > 0) & (df_feat[lag_roll_cols] > 0).all(axis=1)]\n",
    "        df_feat = df_feat.reset_index(drop=True)\n",
    "        if df_feat.empty:\n",
    "            continue\n",
    "\n",
    "        # --- Clustering\n",
    "        clustered_results = cluster_atms_range(df_feat, cluster_range=cluster_range)\n",
    "\n",
    "        for n_clusters, (df_clustered, _) in clustered_results.items():\n",
    "            for cluster_id in sorted(df_clustered[\"cluster\"].dropna().unique()):\n",
    "                cluster_df = df_clustered[df_clustered[\"cluster\"] == cluster_id].copy()\n",
    "\n",
    "                # Drop zero-contaminated sequences again at cluster level\n",
    "                cluster_df = cluster_df[(cluster_df[\"WITHDRWLS\"] > 0) & (cluster_df[lag_roll_cols] > 0).all(axis=1)]\n",
    "                cluster_df = cluster_df.reset_index(drop=True)\n",
    "                if cluster_df.empty:\n",
    "                    continue\n",
    "\n",
    "                # --- Train/val split\n",
    "                split_idx = int(len(cluster_df) * 0.8)\n",
    "                train_df, val_df = cluster_df.iloc[:split_idx], cluster_df.iloc[split_idx:]\n",
    "\n",
    "                # --- Features\n",
    "                feature_cols = [c for c in cluster_df.columns \n",
    "                                if c not in [\"CASHP_ID\", \"DATE\", \"WITHDRWLS\", \"cluster\"]]\n",
    "                # --- Normalize\n",
    "                X_train, X_val, y_train, y_val, feat_scaler, target_scaler = normalize_data(\n",
    "                    train_df, val_df, feature_cols, target_col=\"WITHDRWLS\", method=norm_method\n",
    "                )\n",
    "\n",
    "                trial_metrics = []\n",
    "\n",
    "                def objective(trial):\n",
    "                    hidden_dim = trial.suggest_int(\"hidden_dim\", 16, 128, step=16)\n",
    "                    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "                    dropout    = trial.suggest_float(\"dropout\", 0.0, 0.5)\n",
    "                    lr         = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "                    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "\n",
    "                    train_dataset = SeqFeatureDataset(X_train, y_train, seq_len)\n",
    "                    val_dataset   = SeqFeatureDataset(X_val, y_val, seq_len)\n",
    "                    train_loader  = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "                    val_loader    = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "                    model = LSTMModel(input_dim=X_train.shape[1],\n",
    "                                      hidden_dim=hidden_dim,\n",
    "                                      num_layers=num_layers,\n",
    "                                      dropout=dropout).to(device)\n",
    "                    criterion = nn.MSELoss()\n",
    "                    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "                    # --- Training\n",
    "                    for _ in range(20):\n",
    "                        model.train()\n",
    "                        for X_batch, y_batch in train_loader:\n",
    "                            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                            optimizer.zero_grad()\n",
    "                            preds = model(X_batch)\n",
    "                            loss = criterion(preds, y_batch)\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # --- Validation\n",
    "                    model.eval()\n",
    "                    preds_norm, trues_norm = [], []\n",
    "                    with torch.no_grad():\n",
    "                        for X_batch, y_batch in val_loader:\n",
    "                            X_batch = X_batch.to(device)\n",
    "                            out = model(X_batch).cpu().numpy()\n",
    "                            preds_norm.extend(out)\n",
    "                            trues_norm.extend(y_batch.numpy())\n",
    "\n",
    "                    # Reverse-transform\n",
    "                    y_pred_raw = inverse_transform_predictions(np.array(preds_norm), method=norm_method, scaler=target_scaler)\n",
    "                    y_true_raw = inverse_transform_predictions(np.array(trues_norm), method=norm_method, scaler=target_scaler)\n",
    "\n",
    "                    metrics = calculate_forecast_metrics(\n",
    "                        y_true_raw, y_pred_raw,\n",
    "                        model_name=f\"LSTM trial {trial.number} (seq={seq_len}, k={n_clusters}, c={cluster_id})\"\n",
    "                    )\n",
    "                    metrics[\"trial\"] = trial.number\n",
    "                    trial_metrics.append(metrics)\n",
    "                    return metrics[\"SMAPE\"]\n",
    "\n",
    "                # --- Optuna search\n",
    "                optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "                study = optuna.create_study(direction=\"minimize\")\n",
    "                study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "                # --- Best trial\n",
    "                best_trial = study.best_trial.number\n",
    "                best_metrics = [m for m in trial_metrics if m[\"trial\"] == best_trial][0]\n",
    "                best_metrics.update({\n",
    "                    \"Seq_Length\": seq_len,\n",
    "                    \"Best_Params\": study.best_trial.params,\n",
    "                    \"Num_Clusters\": n_clusters,\n",
    "                    \"Cluster_ID\": cluster_id,\n",
    "                    \"Normalization\": norm_method\n",
    "                })\n",
    "                all_results.append(best_metrics)\n",
    "\n",
    "    return pd.DataFrame(all_results)\n",
    "\n",
    "# --- Run\n",
    "rolling_features_lstm = run_lstm_with_rolling_and_clusters(\n",
    "    atm,\n",
    "    cluster_range=(1,4),\n",
    "    seq_lens=[7,14,28,84],\n",
    "    n_trials=20,\n",
    "    norm_method=\"log\"\n",
    ")\n",
    "\n",
    "print(rolling_features_lstm)\n",
    "\n",
    "save_df_to_downloads(\"rolling_features_lstm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47f7a73",
   "metadata": {},
   "source": [
    "N-Beats with Feature Set 2 (Lag and Rolling Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e1c7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------\n",
    "# Dataset (multivariate sequences from engineered features)\n",
    "# -----------------\n",
    "class SeqFeatureDataset(Dataset):\n",
    "    def __init__(self, X, y, seq_len=30):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.float32)\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(0, len(self.X) - self.seq_len)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X_seq = self.X[idx:idx+self.seq_len]\n",
    "        y_seq = self.y[idx+self.seq_len]\n",
    "        return torch.tensor(X_seq, dtype=torch.float32), torch.tensor(y_seq, dtype=torch.float32)\n",
    "\n",
    "# -----------------\n",
    "# N-BEATS (simple multivariate, flatten seq -> MLP)\n",
    "# -----------------\n",
    "class NBeatsBlock(nn.Module):\n",
    "    def __init__(self, flat_dim, hidden_dim, theta_dim, num_layers=4):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_dim = flat_dim\n",
    "        for _ in range(num_layers):\n",
    "            layers += [nn.Linear(in_dim, hidden_dim), nn.ReLU()]\n",
    "            in_dim = hidden_dim\n",
    "        self.fc = nn.Sequential(*layers)\n",
    "        self.theta = nn.Linear(hidden_dim, theta_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return self.theta(x)\n",
    "\n",
    "class NBeatsModel(nn.Module):\n",
    "    def __init__(self, input_dim, seq_len, hidden_dim=128, num_layers=4):\n",
    "        super().__init__()\n",
    "        flat_dim = input_dim * seq_len\n",
    "        self.block = NBeatsBlock(flat_dim, hidden_dim, theta_dim=1, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, seq_len, dim = x.shape\n",
    "        x = x.reshape(b, -1)     \n",
    "        out = self.block(x)\n",
    "        return out.view(-1)      \n",
    "\n",
    "# -----------------\n",
    "# Runner\n",
    "# -----------------\n",
    "def run_nbeats_with_rolling_and_clusters(\n",
    "    atm,\n",
    "    cluster_range=(1,4),\n",
    "    seq_lens=[7,14,28],\n",
    "    n_trials=20,\n",
    "    norm_method=\"log\",\n",
    "    test_size=0.2\n",
    "):\n",
    "    \"\"\"\n",
    "    Train N-BEATS with lag/rolling features across clusters and sequence lengths.\n",
    "    - Drops rows where WITHDRWLS == 0 or any lag/roll feature == 0\n",
    "    - Never uses 'cluster' as a feature\n",
    "    - Normalizes features + target AFTER split (log or minmax)\n",
    "    - Evaluates on raw scale\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    all_results = []\n",
    "\n",
    "    # suppress Optuna logs\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "    for seq_len in seq_lens:\n",
    "        print(f\"\\n--- Seq_len={seq_len} ---\")\n",
    "\n",
    "        # --- Feature engineering\n",
    "        df_feat = create_lag_roll_features(atm, seq_len)\n",
    "\n",
    "        # Drop zero-contaminated rows (target or any lag/roll == 0)\n",
    "        lag_roll_cols = [c for c in df_feat.columns if c.startswith((\"lag_\", \"roll_mean_\", \"roll_std_\"))]\n",
    "        df_feat = df_feat[(df_feat[\"WITHDRWLS\"] > 0) & (df_feat[lag_roll_cols] > 0).all(axis=1)]\n",
    "        df_feat = df_feat.reset_index(drop=True)\n",
    "        if df_feat.empty:\n",
    "            continue\n",
    "\n",
    "        # (Optional) Drop calendar dummy columns if they exist in the input\n",
    "        drop_cols = [c for c in df_feat.columns if c.startswith((\"dow_\", \"dom_\", \"month_\", \"day_school_holiday\", \"week_part\"))]\n",
    "        df_feat = df_feat.drop(columns=drop_cols, errors=\"ignore\")\n",
    "\n",
    "        # --- Clustering\n",
    "        clustered_results = cluster_atms_range(df_feat, cluster_range=cluster_range)\n",
    "\n",
    "        for n_clusters, (df_clustered, _) in clustered_results.items():\n",
    "            for cluster_id in sorted(df_clustered[\"cluster\"].dropna().unique()):\n",
    "                cluster_df = df_clustered[df_clustered[\"cluster\"] == cluster_id].copy()\n",
    "\n",
    "                # Repeat zero filtering at cluster level\n",
    "                cluster_df = cluster_df[(cluster_df[\"WITHDRWLS\"] > 0) & (cluster_df[lag_roll_cols] > 0).all(axis=1)]\n",
    "                cluster_df = cluster_df.reset_index(drop=True)\n",
    "                if cluster_df.empty:\n",
    "                    continue\n",
    "\n",
    "                # --- Temporal split\n",
    "                split_idx = int(len(cluster_df) * (1 - test_size))\n",
    "                train_df, val_df = cluster_df.iloc[:split_idx], cluster_df.iloc[split_idx:]\n",
    "                if len(train_df) <= seq_len or len(val_df) <= seq_len:\n",
    "                    continue\n",
    "\n",
    "                # --- Features (exclude IDs, DATE, target, and cluster)\n",
    "                feature_cols = [c for c in cluster_df.columns if c not in [\"CASHP_ID\", \"DATE\", \"WITHDRWLS\", \"cluster\"]]\n",
    "\n",
    "                # --- Normalize features + target AFTER split\n",
    "                X_train, X_val, y_train, y_val, feat_scaler, target_scaler = normalize_data(\n",
    "                    train_df, val_df, feature_cols, target_col=\"WITHDRWLS\", method=norm_method\n",
    "                )\n",
    "\n",
    "                trial_metrics = []\n",
    "\n",
    "                # -----------------\n",
    "                # Optuna objective\n",
    "                # -----------------\n",
    "                def objective(trial):\n",
    "                    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 256, step=32)\n",
    "                    num_layers = trial.suggest_int(\"num_layers\", 2, 6)\n",
    "                    lr         = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "                    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "\n",
    "                    train_dataset = SeqFeatureDataset(X_train, y_train, seq_len)\n",
    "                    val_dataset   = SeqFeatureDataset(X_val, y_val, seq_len)\n",
    "                    if len(train_dataset) == 0 or len(val_dataset) == 0:\n",
    "                        return float(\"inf\")\n",
    "\n",
    "                    train_loader  = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "                    val_loader    = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "                    model = NBeatsModel(\n",
    "                        input_dim=X_train.shape[1],\n",
    "                        seq_len=seq_len,\n",
    "                        hidden_dim=hidden_dim,\n",
    "                        num_layers=num_layers\n",
    "                    ).to(device)\n",
    "                    criterion = nn.MSELoss()\n",
    "                    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "                    # --- Training\n",
    "                    for _ in range(20):\n",
    "                        model.train()\n",
    "                        for X_batch, y_batch in train_loader:\n",
    "                            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                            optimizer.zero_grad()\n",
    "                            preds = model(X_batch)\n",
    "                            loss = criterion(preds, y_batch)\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # --- Validation\n",
    "                    model.eval()\n",
    "                    preds_norm, trues_norm = [], []\n",
    "                    with torch.no_grad():\n",
    "                        for X_batch, y_batch in val_loader:\n",
    "                            X_batch = X_batch.to(device)\n",
    "                            out = model(X_batch).cpu().numpy()\n",
    "                            preds_norm.extend(out)\n",
    "                            trues_norm.extend(y_batch.numpy())\n",
    "\n",
    "                    # Reverse-transform to raw scale\n",
    "                    y_pred_raw = inverse_transform_predictions(np.array(preds_norm), method=norm_method, scaler=target_scaler)\n",
    "                    y_true_raw = inverse_transform_predictions(np.array(trues_norm), method=norm_method, scaler=target_scaler)\n",
    "\n",
    "                    metrics = calculate_forecast_metrics(\n",
    "                        y_true_raw, y_pred_raw,\n",
    "                        model_name=f\"N-BEATS trial {trial.number} (seq={seq_len}, k={n_clusters}, c={cluster_id})\"\n",
    "                    )\n",
    "                    metrics[\"trial\"] = trial.number\n",
    "                    trial_metrics.append(metrics)\n",
    "                    return metrics[\"SMAPE\"]\n",
    "\n",
    "                # --- Run Optuna\n",
    "                study = optuna.create_study(direction=\"minimize\")\n",
    "                study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "                # --- Best trial\n",
    "                best_trial = study.best_trial.number\n",
    "                best_metrics = [m for m in trial_metrics if m[\"trial\"] == best_trial][0]\n",
    "                best_metrics.update({\n",
    "                    \"Seq_Length\": seq_len,\n",
    "                    \"Best_Params\": study.best_trial.params,\n",
    "                    \"Num_Clusters\": n_clusters,\n",
    "                    \"Cluster_ID\": cluster_id,\n",
    "                    \"Normalization\": norm_method\n",
    "                })\n",
    "                all_results.append(best_metrics)\n",
    "\n",
    "    return pd.DataFrame(all_results)\n",
    "\n",
    "# --- Run\n",
    "rolling_features_nbeats = run_nbeats_with_rolling_and_clusters(\n",
    "    atm,\n",
    "    cluster_range=(1,4),\n",
    "    seq_lens=[7,14,28,84],\n",
    "    n_trials=20,\n",
    "    norm_method=\"log\",\n",
    "    test_size=0.2\n",
    ")\n",
    "\n",
    "print(rolling_features_nbeats)\n",
    "save_df_to_downloads(\"rolling_features_nbeats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53b113c",
   "metadata": {},
   "source": [
    "Feature Set Creation for Feature Set 3 (Lag, Rolling Features, Normalization, Reverse Transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082d828d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "def normalize_data_mixed(train_df, val_df, feature_cols, target_col=\"WITHDRWLS\", method=\"log\"):\n",
    "    \"\"\"\n",
    "    Normalize target and lag/roll features.\n",
    "    - log: apply log1p only to lag/roll and target columns\n",
    "    - minmax: scale everything\n",
    "    - dummy features (0/1) stay untouched for log\n",
    "    \"\"\"\n",
    "    # Separate feature groups\n",
    "    lag_roll_cols = [c for c in feature_cols if c.startswith((\"lag_\", \"roll_mean_\", \"roll_std_\"))]\n",
    "    other_cols = [c for c in feature_cols if c not in lag_roll_cols]\n",
    "\n",
    "    if method == \"log\":\n",
    "        # Features: log only lag/roll, keep dummies untouched\n",
    "        X_train = np.concatenate([\n",
    "            np.log1p(train_df[lag_roll_cols].values),\n",
    "            train_df[other_cols].values.astype(float)\n",
    "        ], axis=1)\n",
    "        X_val = np.concatenate([\n",
    "            np.log1p(val_df[lag_roll_cols].values),\n",
    "            val_df[other_cols].values.astype(float)\n",
    "        ], axis=1)\n",
    "        feat_scaler = None\n",
    "\n",
    "        # Target\n",
    "        y_train = np.log1p(train_df[target_col].values)\n",
    "        y_val   = np.log1p(val_df[target_col].values)\n",
    "        target_scaler = None\n",
    "\n",
    "    elif method == \"minmax\":\n",
    "        feat_scaler = MinMaxScaler()\n",
    "        target_scaler = MinMaxScaler()\n",
    "\n",
    "        # Features\n",
    "        X_train = feat_scaler.fit_transform(train_df[feature_cols])\n",
    "        X_val   = feat_scaler.transform(val_df[feature_cols])\n",
    "\n",
    "        # Target\n",
    "        y_train = target_scaler.fit_transform(train_df[[target_col]]).ravel()\n",
    "        y_val   = target_scaler.transform(val_df[[target_col]]).ravel()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"method must be 'log' or 'minmax'\")\n",
    "\n",
    "    return X_train, X_val, y_train, y_val, feat_scaler, target_scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e75e96",
   "metadata": {},
   "source": [
    "Linear Regression with Feature Set 3 (Merged Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d9a230",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "\n",
    "def run_linear_regression_with_both_features(\n",
    "    atm,\n",
    "    special_days, \n",
    "    half_days,\n",
    "    cluster_range=(1,4),\n",
    "    seq_lens=[7,14,28],\n",
    "    norm_method=\"log\",\n",
    "    test_size=0.2\n",
    "):\n",
    "    all_metrics = []\n",
    "\n",
    "    for seq_len in seq_lens:\n",
    "        print(f\"\\n--- Seq_len={seq_len} ---\")\n",
    "\n",
    "        # 1. Add calendar + holiday features\n",
    "        df_feat = feature_engineering_pipeline(atm, special_days, half_days)\n",
    "        \n",
    "        # 2. Add lag/rolling features\n",
    "        df_feat = create_lag_roll_features(df_feat, seq_len)\n",
    "\n",
    "        # Drop rows where target == 0 or any lag/roll == 0\n",
    "        lag_roll_cols = [c for c in df_feat.columns if c.startswith((\"lag_\", \"roll_mean_\", \"roll_std_\"))]\n",
    "        df_feat = df_feat[(df_feat[\"WITHDRWLS\"] > 0) & (df_feat[lag_roll_cols] > 0).all(axis=1)]\n",
    "        df_feat = df_feat.reset_index(drop=True)\n",
    "        if df_feat.empty:\n",
    "            continue\n",
    "\n",
    "        # Clustering\n",
    "        clustered_results = cluster_atms_range(df_feat, cluster_range=cluster_range)\n",
    "\n",
    "        for n_clusters, (df_clustered, _) in clustered_results.items():\n",
    "            for cluster_id in sorted(df_clustered[\"cluster\"].dropna().unique()):\n",
    "                cluster_df = df_clustered[df_clustered[\"cluster\"] == cluster_id].copy()\n",
    "\n",
    "                # Drop again at cluster level\n",
    "                cluster_df = cluster_df[(cluster_df[\"WITHDRWLS\"] > 0) & (cluster_df[lag_roll_cols] > 0).all(axis=1)]\n",
    "                cluster_df = cluster_df.reset_index(drop=True)\n",
    "                if cluster_df.empty:\n",
    "                    continue\n",
    "\n",
    "                # Temporal split\n",
    "                split_idx = int(len(cluster_df) * (1 - test_size))\n",
    "                train_df, val_df = cluster_df.iloc[:split_idx], cluster_df.iloc[split_idx:]\n",
    "\n",
    "                # Features () exclude IDs, target, cluster)\n",
    "                feature_cols = [\n",
    "                    c for c in cluster_df.columns\n",
    "                    if c not in [\"CASHP_ID\", \"DATE\", \"WITHDRWLS\", \"cluster\"]\n",
    "                ]\n",
    "\n",
    "                # Normalize\n",
    "                X_train, X_val, y_train, y_val, feat_scaler, target_scaler = normalize_data_mixed(\n",
    "                    train_df, val_df, feature_cols, target_col=\"WITHDRWLS\", method=norm_method\n",
    "                )\n",
    "\n",
    "                # Train\n",
    "                model = LinearRegression()\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                # Predict\n",
    "                y_pred_norm = model.predict(X_val)\n",
    "\n",
    "                # Reverse-transform\n",
    "                y_pred_raw = inverse_transform_predictions(y_pred_norm, method=norm_method, scaler=target_scaler)\n",
    "                y_true_raw = inverse_transform_predictions(y_val, method=norm_method, scaler=target_scaler)\n",
    "\n",
    "                # Evaluate\n",
    "                metrics = calculate_forecast_metrics(\n",
    "                    y_true_raw, y_pred_raw,\n",
    "                    model_name=f\"LR (both features, seq={seq_len}, k={n_clusters}, c={cluster_id})\"\n",
    "                )\n",
    "                metrics.update({\n",
    "                    \"Seq_Length\": seq_len,\n",
    "                    \"Num_Clusters\": n_clusters,\n",
    "                    \"Cluster_ID\": cluster_id,\n",
    "                    \"Normalization\": norm_method\n",
    "                })\n",
    "                all_metrics.append(metrics)\n",
    "\n",
    "    return pd.DataFrame(all_metrics)\n",
    "\n",
    "\n",
    "# Run\n",
    "both_features_lin_reg = run_linear_regression_with_both_features(\n",
    "    atm,\n",
    "    special_days,\n",
    "    half_days,\n",
    "    cluster_range=(1,4),\n",
    "    seq_lens=[7,14,28,84],\n",
    "    norm_method=\"log\"\n",
    ")\n",
    "\n",
    "print(both_features_lin_reg)\n",
    "save_df_to_downloads(\"both_features_lin_reg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96af3b78",
   "metadata": {},
   "source": [
    "XGBoost with Feature Set 3 (Merged Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbee526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import optuna\n",
    "import pandas as pd\n",
    "\n",
    "def run_xgboost_with_both_features(\n",
    "    atm,\n",
    "    official_holidays,\n",
    "    half_days,\n",
    "    cluster_range=(1,4),\n",
    "    seq_lens=[7,14,28],\n",
    "    n_trials=50,\n",
    "    norm_method=\"log\",\n",
    "    test_size=0.2\n",
    "):\n",
    "    all_metrics = []\n",
    "\n",
    "    # Silence Optuna logs\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "    for seq_len in seq_lens:\n",
    "        print(f\"\\n--- Seq_len={seq_len} ---\")\n",
    "\n",
    "        # 1. Add calendar + holiday features\n",
    "        df_feat = feature_engineering_pipeline(atm, official_holidays, half_days)\n",
    "\n",
    "        # 2. Add lag/rolling features\n",
    "        df_feat = create_lag_roll_features(df_feat, seq_len)\n",
    "\n",
    "        # Drop zero-contaminated rows\n",
    "        lag_roll_cols = [c for c in df_feat.columns if c.startswith((\"lag_\", \"roll_mean_\", \"roll_std_\"))]\n",
    "        df_feat = df_feat[(df_feat[\"WITHDRWLS\"] > 0) & (df_feat[lag_roll_cols] > 0).all(axis=1)]\n",
    "        df_feat = df_feat.reset_index(drop=True)\n",
    "        if df_feat.empty:\n",
    "            continue\n",
    "\n",
    "        # Clustering\n",
    "        clustered_results = cluster_atms_range(df_feat, cluster_range=cluster_range)\n",
    "\n",
    "        for n_clusters, (df_clustered, _) in clustered_results.items():\n",
    "            for cluster_id in sorted(df_clustered[\"cluster\"].dropna().unique()):\n",
    "                cluster_df = df_clustered[df_clustered[\"cluster\"] == cluster_id].copy()\n",
    "\n",
    "                # Drop again at cluster level\n",
    "                cluster_df = cluster_df[(cluster_df[\"WITHDRWLS\"] > 0) & (cluster_df[lag_roll_cols] > 0).all(axis=1)]\n",
    "                cluster_df = cluster_df.reset_index(drop=True)\n",
    "                if cluster_df.empty:\n",
    "                    continue\n",
    "\n",
    "                # Temporal split\n",
    "                split_idx = int(len(cluster_df) * (1 - test_size))\n",
    "                train_df, val_df = cluster_df.iloc[:split_idx], cluster_df.iloc[split_idx:]\n",
    "\n",
    "                # Features ( exclude IDs, target, cluster)\n",
    "                feature_cols = [\n",
    "                    c for c in cluster_df.columns\n",
    "                    if c not in [\"CASHP_ID\", \"DATE\", \"WITHDRWLS\", \"cluster\"]\n",
    "                ]\n",
    "\n",
    "                # Normalize (fixed version: only log-transform lag/rolls + target)\n",
    "                X_train, X_val, y_train, y_val, feat_scaler, target_scaler = normalize_data_mixed(\n",
    "                    train_df, val_df, feature_cols, target_col=\"WITHDRWLS\", method=norm_method\n",
    "                )\n",
    "\n",
    "                trial_metrics = []\n",
    "\n",
    "                # Optuna objective\n",
    "                def objective(trial):\n",
    "                    params = {\n",
    "                        \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 1000),\n",
    "                        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.1, log=True),\n",
    "                        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "                        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "                        \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n",
    "                        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "                        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "                        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True),\n",
    "                        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n",
    "                        \"random_state\": 42,\n",
    "                        \"tree_method\": \"hist\"\n",
    "                    }\n",
    "\n",
    "                    model = xgb.XGBRegressor(**params)\n",
    "                    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "\n",
    "                    # Predict\n",
    "                    y_pred_norm = model.predict(X_val)\n",
    "\n",
    "                    # Reverse-transform\n",
    "                    y_pred_raw = inverse_transform_predictions(y_pred_norm, method=norm_method, scaler=target_scaler)\n",
    "                    y_true_raw = inverse_transform_predictions(y_val, method=norm_method, scaler=target_scaler)\n",
    "\n",
    "                    # Evaluate\n",
    "                    metrics = calculate_forecast_metrics(\n",
    "                        y_true_raw, y_pred_raw,\n",
    "                        model_name=f\"XGB trial {trial.number} (both features, seq={seq_len}, k={n_clusters}, c={cluster_id})\"\n",
    "                    )\n",
    "                    metrics[\"trial\"] = trial.number\n",
    "                    trial_metrics.append(metrics)\n",
    "                    return metrics[\"SMAPE\"]\n",
    "\n",
    "                # Run Optuna\n",
    "                study = optuna.create_study(direction=\"minimize\")\n",
    "                study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "                # Best trial\n",
    "                best_trial = study.best_trial.number\n",
    "                best_metrics = [m for m in trial_metrics if m[\"trial\"] == best_trial][0]\n",
    "                best_metrics.update({\n",
    "                    \"Seq_Length\": seq_len,\n",
    "                    \"Best_Params\": study.best_trial.params,\n",
    "                    \"Num_Clusters\": n_clusters,\n",
    "                    \"Cluster_ID\": cluster_id,\n",
    "                    \"Normalization\": norm_method\n",
    "                })\n",
    "                all_metrics.append(best_metrics)\n",
    "\n",
    "    return pd.DataFrame(all_metrics)\n",
    "\n",
    "# Run\n",
    "both_features_xgboost = run_xgboost_with_both_features(\n",
    "    atm,\n",
    "    special_days,\n",
    "    half_days,\n",
    "    cluster_range=(1,4),\n",
    "    seq_lens=[7,14,28,84],\n",
    "    n_trials=100,\n",
    "    norm_method=\"log\"\n",
    ")\n",
    "\n",
    "print(both_features_xgboost)\n",
    "save_df_to_downloads(\"both_features_xgboost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74b1e42",
   "metadata": {},
   "source": [
    "LSTM with Feature Set 3 (Merged Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d685a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------\n",
    "# Dataset (with feature sequences)\n",
    "# -----------------\n",
    "class SeqFeatureDataset(Dataset):\n",
    "    def __init__(self, X, y, seq_len=30):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.float32)\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X_seq = self.X[idx:idx+self.seq_len]\n",
    "        y_seq = self.y[idx+self.seq_len]\n",
    "        return torch.tensor(X_seq, dtype=torch.float32), torch.tensor(y_seq, dtype=torch.float32)\n",
    "\n",
    "# -----------------\n",
    "# LSTM Model\n",
    "# -----------------\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, hidden_dim, num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0 \n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out.view(-1)          \n",
    "\n",
    "# -----------------\n",
    "# Runner with both features\n",
    "# -----------------\n",
    "def run_lstm_with_both_features(\n",
    "    atm,\n",
    "    special_days,\n",
    "    half_days,\n",
    "    cluster_range=(1,4),\n",
    "    seq_lens=[7,14,28],\n",
    "    n_trials=20,\n",
    "    norm_method=\"log\"\n",
    "):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    all_results = []\n",
    "\n",
    "    # Silence Optuna logs\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "    for seq_len in seq_lens:\n",
    "        print(f\"\\n--- Seq_len={seq_len} ---\")\n",
    "\n",
    "        # 1. Add calendar + holiday features\n",
    "        df_feat = feature_engineering_pipeline(atm, special_days, half_days)\n",
    "\n",
    "        # 2. Add lag/rolling features\n",
    "        df_feat = create_lag_roll_features(df_feat, seq_len)\n",
    "\n",
    "        # Drop zero-contaminated rows\n",
    "        lag_roll_cols = [c for c in df_feat.columns if c.startswith((\"lag_\", \"roll_mean_\", \"roll_std_\"))]\n",
    "        df_feat = df_feat[(df_feat[\"WITHDRWLS\"] > 0) & (df_feat[lag_roll_cols] > 0).all(axis=1)]\n",
    "        df_feat = df_feat.reset_index(drop=True)\n",
    "        if df_feat.empty:\n",
    "            continue\n",
    "\n",
    "        # Clustering\n",
    "        clustered_results = cluster_atms_range(df_feat, cluster_range=cluster_range)\n",
    "\n",
    "        for n_clusters, (df_clustered, _) in clustered_results.items():\n",
    "            for cluster_id in sorted(df_clustered[\"cluster\"].dropna().unique()):\n",
    "                cluster_df = df_clustered[df_clustered[\"cluster\"] == cluster_id].copy()\n",
    "\n",
    "                # Drop again at cluster level\n",
    "                cluster_df = cluster_df[(cluster_df[\"WITHDRWLS\"] > 0) & (cluster_df[lag_roll_cols] > 0).all(axis=1)]\n",
    "                cluster_df = cluster_df.reset_index(drop=True)\n",
    "                if cluster_df.empty:\n",
    "                    continue\n",
    "\n",
    "                # Train/val split\n",
    "                split_idx = int(len(cluster_df) * 0.8)\n",
    "                train_df, val_df = cluster_df.iloc[:split_idx], cluster_df.iloc[split_idx:]\n",
    "\n",
    "                # Features\n",
    "                feature_cols = [\n",
    "                    c for c in cluster_df.columns\n",
    "                    if c not in [\"CASHP_ID\", \"DATE\", \"WITHDRWLS\", \"cluster\"]\n",
    "                ]\n",
    "\n",
    "                # Normalize (log only for lags/rolls + target)\n",
    "                X_train, X_val, y_train, y_val, feat_scaler, target_scaler = normalize_data_mixed(\n",
    "                    train_df, val_df, feature_cols, target_col=\"WITHDRWLS\", method=norm_method\n",
    "                )\n",
    "\n",
    "                trial_metrics = []\n",
    "\n",
    "                # Optuna objective\n",
    "                def objective(trial):\n",
    "                    hidden_dim = trial.suggest_int(\"hidden_dim\", 16, 128, step=16)\n",
    "                    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "                    dropout    = trial.suggest_float(\"dropout\", 0.0, 0.5)\n",
    "                    lr         = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "                    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "\n",
    "                    train_dataset = SeqFeatureDataset(X_train, y_train, seq_len)\n",
    "                    val_dataset   = SeqFeatureDataset(X_val, y_val, seq_len)\n",
    "                    train_loader  = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "                    val_loader    = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "                    model = LSTMModel(\n",
    "                        input_dim=X_train.shape[1],\n",
    "                        hidden_dim=hidden_dim,\n",
    "                        num_layers=num_layers,\n",
    "                        dropout=dropout\n",
    "                    ).to(device)\n",
    "\n",
    "                    criterion = nn.MSELoss()\n",
    "                    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "                    # Training\n",
    "                    for _ in range(20):\n",
    "                        model.train()\n",
    "                        for X_batch, y_batch in train_loader:\n",
    "                            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                            optimizer.zero_grad()\n",
    "                            preds = model(X_batch)\n",
    "                            loss = criterion(preds, y_batch)\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # Validation\n",
    "                    model.eval()\n",
    "                    preds_norm, trues_norm = [], []\n",
    "                    with torch.no_grad():\n",
    "                        for X_batch, y_batch in val_loader:\n",
    "                            X_batch = X_batch.to(device)\n",
    "                            out = model(X_batch).cpu().numpy()\n",
    "                            preds_norm.extend(out)\n",
    "                            trues_norm.extend(y_batch.numpy())\n",
    "\n",
    "                    # Reverse-transform\n",
    "                    y_pred_raw = inverse_transform_predictions(np.array(preds_norm), method=norm_method, scaler=target_scaler)\n",
    "                    y_true_raw = inverse_transform_predictions(np.array(trues_norm), method=norm_method, scaler=target_scaler)\n",
    "\n",
    "                    metrics = calculate_forecast_metrics(\n",
    "                        y_true_raw, y_pred_raw,\n",
    "                        model_name=f\"LSTM trial {trial.number} (both features, seq={seq_len}, k={n_clusters}, c={cluster_id})\"\n",
    "                    )\n",
    "                    metrics[\"trial\"] = trial.number\n",
    "                    trial_metrics.append(metrics)\n",
    "                    return metrics[\"SMAPE\"]\n",
    "\n",
    "                # Run Optuna\n",
    "                study = optuna.create_study(direction=\"minimize\")\n",
    "                study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "                # Best trial\n",
    "                best_trial = study.best_trial.number\n",
    "                best_metrics = [m for m in trial_metrics if m[\"trial\"] == best_trial][0]\n",
    "                best_metrics.update({\n",
    "                    \"Seq_Length\": seq_len,\n",
    "                    \"Best_Params\": study.best_trial.params,\n",
    "                    \"Num_Clusters\": n_clusters,\n",
    "                    \"Cluster_ID\": cluster_id,\n",
    "                    \"Normalization\": norm_method\n",
    "                })\n",
    "                all_results.append(best_metrics)\n",
    "\n",
    "    return pd.DataFrame(all_results)\n",
    "\n",
    "# Run\n",
    "both_features_lstm = run_lstm_with_both_features(\n",
    "    atm,\n",
    "    special_days,\n",
    "    half_days,\n",
    "    cluster_range=(1,4),\n",
    "    seq_lens=[7,14,28,84],\n",
    "    n_trials=100,\n",
    "    norm_method=\"log\"\n",
    ")\n",
    "\n",
    "print(both_features_lstm)\n",
    "save_df_to_downloads(\"both_features_lstm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c536e5",
   "metadata": {},
   "source": [
    "N-Beats with Feature Set 3 (Merged Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ec3cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------\n",
    "# Dataset\n",
    "# -----------------\n",
    "class SeqFeatureDataset(Dataset):\n",
    "    def __init__(self, X, y, seq_len=30):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.float32)\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X_seq = self.X[idx:idx+self.seq_len]\n",
    "        y_seq = self.y[idx+self.seq_len]\n",
    "        return torch.tensor(X_seq, dtype=torch.float32), torch.tensor(y_seq, dtype=torch.float32)\n",
    "\n",
    "# -----------------\n",
    "# N-BEATS Model\n",
    "# -----------------\n",
    "class NBeatsBlock(nn.Module):\n",
    "    def __init__(self, flat_dim, hidden_dim, theta_dim, num_layers=4):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_dim = flat_dim\n",
    "        for _ in range(num_layers):\n",
    "            layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_dim = hidden_dim\n",
    "        self.fc = nn.Sequential(*layers)\n",
    "        self.theta = nn.Linear(hidden_dim, theta_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.theta(self.fc(x))\n",
    "\n",
    "class NBeatsModel(nn.Module):\n",
    "    def __init__(self, input_dim, seq_len, hidden_dim=128, num_layers=4):\n",
    "        super().__init__()\n",
    "        flat_dim = input_dim * seq_len\n",
    "        self.block = NBeatsBlock(flat_dim, hidden_dim, theta_dim=1, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, seq_len, dim = x.shape\n",
    "        x = x.reshape(b, -1)     \n",
    "        out = self.block(x)\n",
    "        return out.view(-1)  \n",
    "\n",
    "# -----------------\n",
    "# Runner\n",
    "# -----------------\n",
    "def run_nbeats_with_both_features(\n",
    "    atm,\n",
    "    special_days,\n",
    "    half_days,\n",
    "    cluster_range=(1,4),\n",
    "    seq_lens=[7,14,28],\n",
    "    n_trials=20,\n",
    "    norm_method=\"log\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Train N-BEATS with calendar + lag/roll features for multiple clusters and sequence lengths.\n",
    "    Drops zero-contaminated sequences (target or lag/roll features).\n",
    "    Suppresses Optuna trial logs.\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    all_results = []\n",
    "\n",
    "    # silence Optuna logs\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "    for seq_len in seq_lens:\n",
    "        print(f\"\\n--- Seq_len={seq_len} ---\")\n",
    "\n",
    "        # 1. Add calendar + holiday features\n",
    "        df_feat = feature_engineering_pipeline(atm, special_days, half_days)\n",
    "\n",
    "        # 2. Add lag/rolling features\n",
    "        df_feat = create_lag_roll_features(df_feat, seq_len)\n",
    "\n",
    "        # Drop zero-contaminated rows\n",
    "        lag_roll_cols = [c for c in df_feat.columns if c.startswith((\"lag_\", \"roll_mean_\", \"roll_std_\"))]\n",
    "        df_feat = df_feat[(df_feat[\"WITHDRWLS\"] > 0) & (df_feat[lag_roll_cols] > 0).all(axis=1)]\n",
    "        df_feat = df_feat.reset_index(drop=True)\n",
    "        if df_feat.empty:\n",
    "            continue\n",
    "\n",
    "        # Clustering\n",
    "        clustered_results = cluster_atms_range(df_feat, cluster_range=cluster_range)\n",
    "\n",
    "        for n_clusters, (df_clustered, _) in clustered_results.items():\n",
    "            for cluster_id in sorted(df_clustered[\"cluster\"].dropna().unique()):\n",
    "                cluster_df = df_clustered[df_clustered[\"cluster\"] == cluster_id].copy()\n",
    "\n",
    "                # Drop again at cluster level\n",
    "                cluster_df = cluster_df[(cluster_df[\"WITHDRWLS\"] > 0) & (cluster_df[lag_roll_cols] > 0).all(axis=1)]\n",
    "                cluster_df = cluster_df.reset_index(drop=True)\n",
    "                if cluster_df.empty:\n",
    "                    continue\n",
    "\n",
    "                # Train/val split\n",
    "                split_idx = int(len(cluster_df) * 0.8)\n",
    "                train_df, val_df = cluster_df.iloc[:split_idx], cluster_df.iloc[split_idx:]\n",
    "\n",
    "                # Features\n",
    "                feature_cols = [\n",
    "                    c for c in cluster_df.columns\n",
    "                    if c not in [\"CASHP_ID\", \"DATE\", \"WITHDRWLS\", \"cluster\"]\n",
    "                ]\n",
    "\n",
    "                # Normalize\n",
    "                X_train, X_val, y_train, y_val, feat_scaler, target_scaler = normalize_data_mixed(\n",
    "                    train_df, val_df, feature_cols, target_col=\"WITHDRWLS\", method=norm_method\n",
    "                )\n",
    "\n",
    "                trial_metrics = []\n",
    "\n",
    "                def objective(trial):\n",
    "                    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 256, step=32)\n",
    "                    num_layers = trial.suggest_int(\"num_layers\", 2, 6)\n",
    "                    lr         = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "                    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "\n",
    "                    train_dataset = SeqFeatureDataset(X_train, y_train, seq_len)\n",
    "                    val_dataset   = SeqFeatureDataset(X_val, y_val, seq_len)\n",
    "                    train_loader  = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "                    val_loader    = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "                    model = NBeatsModel(input_dim=X_train.shape[1],\n",
    "                                        seq_len=seq_len,\n",
    "                                        hidden_dim=hidden_dim,\n",
    "                                        num_layers=num_layers).to(device)\n",
    "                    criterion = nn.MSELoss()\n",
    "                    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "                    # Training\n",
    "                    for _ in range(20):\n",
    "                        model.train()\n",
    "                        for X_batch, y_batch in train_loader:\n",
    "                            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                            optimizer.zero_grad()\n",
    "                            preds = model(X_batch)\n",
    "                            loss = criterion(preds, y_batch)\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # Validation\n",
    "                    model.eval()\n",
    "                    preds_norm, trues_norm = [], []\n",
    "                    with torch.no_grad():\n",
    "                        for X_batch, y_batch in val_loader:\n",
    "                            X_batch = X_batch.to(device)\n",
    "                            out = model(X_batch).cpu().numpy()\n",
    "                            preds_norm.extend(out)\n",
    "                            trues_norm.extend(y_batch.numpy())\n",
    "\n",
    "                    # Reverse-transform\n",
    "                    y_pred_raw = inverse_transform_predictions(np.array(preds_norm), method=norm_method, scaler=target_scaler)\n",
    "                    y_true_raw = inverse_transform_predictions(np.array(trues_norm), method=norm_method, scaler=target_scaler)\n",
    "\n",
    "                    metrics = calculate_forecast_metrics(\n",
    "                        y_true_raw, y_pred_raw,\n",
    "                        model_name=f\"N-BEATS trial {trial.number} (both features, seq={seq_len}, k={n_clusters}, c={cluster_id})\"\n",
    "                    )\n",
    "                    metrics[\"trial\"] = trial.number\n",
    "                    trial_metrics.append(metrics)\n",
    "                    return metrics[\"SMAPE\"]\n",
    "\n",
    "                # Optuna tuning\n",
    "                study = optuna.create_study(direction=\"minimize\")\n",
    "                study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "                # Best trial\n",
    "                best_trial = study.best_trial.number\n",
    "                best_metrics = [m for m in trial_metrics if m[\"trial\"] == best_trial][0]\n",
    "                best_metrics.update({\n",
    "                    \"Seq_Length\": seq_len,\n",
    "                    \"Best_Params\": study.best_trial.params,\n",
    "                    \"Num_Clusters\": n_clusters,\n",
    "                    \"Cluster_ID\": cluster_id,\n",
    "                    \"Normalization\": norm_method\n",
    "                })\n",
    "                all_results.append(best_metrics)\n",
    "\n",
    "    return pd.DataFrame(all_results)\n",
    "\n",
    "# Run\n",
    "both_features_nbeats = run_nbeats_with_both_features(\n",
    "    atm,\n",
    "    special_days,\n",
    "    half_days,\n",
    "    cluster_range=(1,4),\n",
    "    seq_lens=[7,14,28,84],\n",
    "    n_trials=20,\n",
    "    norm_method=\"log\"\n",
    ")\n",
    "\n",
    "print(both_features_nbeats)\n",
    "save_df_to_downloads(\"both_features_nbeats\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-25.04",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
