{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "161cb217",
   "metadata": {},
   "source": [
    "Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed5bc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import xgboost as xgb\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cce94b",
   "metadata": {},
   "source": [
    "Seed Setting and Saver Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41d298b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Seed Setting\n",
    "SEED = 42\n",
    "\n",
    "# Python built-in\n",
    "random.seed(SEED)\n",
    "\n",
    "# NumPy\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# PyTorch\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Ensure deterministic runs\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def save_df_to_downloads(df_name: str):\n",
    "    if df_name in globals():\n",
    "        df = globals()[df_name]\n",
    "        path = f\"/mnt/c/Users/baris/Downloads/{df_name}.pkl\" # Change this for saving results \n",
    "        df.to_pickle(path)\n",
    "        print(f\" Saved {df_name} to {path}\")\n",
    "    else:\n",
    "        print(f\" No DataFrame named '{df_name}' found in globals().\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b91dfc",
   "metadata": {},
   "source": [
    "Data Loader and Holiday Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee956ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your ATM data\n",
    "atm_path = \"/mnt/c/Users/baris/Downloads/2024-12-09_ATM_Branch_Data.xlsx\"\n",
    "atm = pd.read_excel(atm_path, sheet_name=\"ATM\")\n",
    "\n",
    "# Official Holidays (special_days)\n",
    "special_days = [\n",
    "    # 2006\n",
    "    '2006-01-01', '2006-01-10', '2006-01-11', '2006-01-12', '2006-01-13',\n",
    "    '2006-04-23', '2006-05-19', '2006-08-30',\n",
    "    '2006-10-23', '2006-10-24', '2006-10-25',\n",
    "    '2006-10-29', '2006-12-31'\n",
    "\n",
    "    # 2007\n",
    "    '2007-01-01', '2007-01-02', '2007-01-03',\n",
    "    '2007-04-23', '2007-05-19', '2007-08-30',\n",
    "    '2007-10-12', '2007-10-13', '2007-10-14',\n",
    "    '2007-10-29',\n",
    "    '2007-12-20', '2007-12-21', '2007-12-22', '2007-12-23',\n",
    "    '2007-12-31',\n",
    "\n",
    "    # 2008\n",
    "    '2008-01-01', '2008-04-23', '2008-05-19', '2008-08-30',\n",
    "    '2008-09-30', '2008-10-01', '2008-10-02',\n",
    "    '2008-10-29',\n",
    "    '2008-12-08', '2008-12-09', '2008-12-10', '2008-12-11',\n",
    "    '2008-12-31'\n",
    "]\n",
    "\n",
    "# Half Working Days (half_days)\n",
    "half_days = [\n",
    "    '2006-01-09', '2006-10-22', '2006-12-30',\n",
    "    '2007-10-11', '2007-12-19',\n",
    "    '2008-09-29', '2008-12-07'\n",
    "]\n",
    "\n",
    "feature_eng_data = feature_engineering_pipeline(atm, special_days, half_days)\n",
    "\n",
    "print(feature_eng_data)\n",
    "print(feature_eng_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6176ca11",
   "metadata": {},
   "source": [
    "Calendar and Holiday Features Creation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba94a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Active Flag\n",
    "# -----------------\n",
    "def add_active_flag(df):\n",
    "    groups = []\n",
    "    for atm_id, group in df.groupby(\"CASHP_ID\"):\n",
    "        group = group.sort_values(\"DATE\").copy()\n",
    "        first_active_idx = group[group[\"WITHDRWLS\"] > 0].index.min()\n",
    "        if pd.notna(first_active_idx):\n",
    "            group[\"is_active\"] = (group.index >= first_active_idx).astype(int)\n",
    "        else:\n",
    "            group[\"is_active\"] = 0\n",
    "        groups.append(group)\n",
    "    return pd.concat(groups).reset_index(drop=True)\n",
    "\n",
    "# -----------------\n",
    "# Calendar Features\n",
    "# -----------------\n",
    "def add_calendar_features(df):\n",
    "    df[\"day_of_month\"] = df[\"DATE\"].dt.day\n",
    "    df[\"month\"] = df[\"DATE\"].dt.month\n",
    "    df[\"day_of_week\"] = df[\"DATE\"].dt.dayofweek\n",
    "\n",
    "    # Day of month dummies\n",
    "    dom_dummies = pd.get_dummies(df[\"day_of_month\"], prefix=\"dom\")\n",
    "    dom_dummies = dom_dummies.drop(columns=[\"dom_1\"], errors=\"ignore\")\n",
    "\n",
    "    # Month dummies\n",
    "    month_dummies = pd.get_dummies(df[\"month\"], prefix=\"month\")\n",
    "    month_dummies = month_dummies.drop(columns=[\"month_12\"], errors=\"ignore\")\n",
    "\n",
    "    # Day of week dummies\n",
    "    dow_dummies = pd.get_dummies(df[\"day_of_week\"], prefix=\"dow\")\n",
    "    dow_dummies = dow_dummies.drop(columns=[\"dow_0\"], errors=\"ignore\")  \n",
    "\n",
    "    # Drop raw columns\n",
    "    df = df.drop(columns=[\"day_of_month\", \"month\", \"day_of_week\"])\n",
    "    df = pd.concat([df, dom_dummies, month_dummies, dow_dummies], axis=1)\n",
    "    return df\n",
    "\n",
    "# -----------------\n",
    "# Holiday Features\n",
    "# -----------------\n",
    "def add_holiday_features(df, official_holidays, half_days):\n",
    "    df[\"DATE\"] = pd.to_datetime(df[\"DATE\"])\n",
    "    official_holidays = pd.to_datetime(official_holidays)\n",
    "    half_days = pd.to_datetime(half_days)\n",
    "\n",
    "    df[\"day_official_holiday\"] = df[\"DATE\"].isin(official_holidays).astype(int)\n",
    "    df[\"day_half_day\"] = df[\"DATE\"].isin(half_days).astype(int)\n",
    "\n",
    "    # Everything else = normal day\n",
    "    df[\"day_normal_day\"] = (\n",
    "        (df[\"day_official_holiday\"] == 0) &\n",
    "        (df[\"day_half_day\"] == 0)\n",
    "    ).astype(int)\n",
    "    return df\n",
    "\n",
    "# -----------------\n",
    "# Final Wrapper\n",
    "# -----------------\n",
    "def feature_engineering_pipeline(atm, official_holidays, half_days):\n",
    "    df = atm.copy()\n",
    "    df = add_active_flag(df)\n",
    "    df = add_calendar_features(df)  \n",
    "    df = add_holiday_features(df, official_holidays, half_days)\n",
    "    return df.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c43014",
   "metadata": {},
   "source": [
    "Helper Functions (Clustering, Data Prep, Forecast Metrics, Normalization, Reverse Transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a35b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# 1. Data Preparation\n",
    "# -----------------\n",
    "def prepare_data(df, test_size=0.2):\n",
    "    feature_cols = [col for col in df.columns \n",
    "                    if col not in [\"CASHP_ID\", \"DATE\", \"WITHDRWLS\",\"cluster\",\"is_active\"]]\n",
    "\n",
    "    X = df[feature_cols]\n",
    "    y = df[\"WITHDRWLS\"] \n",
    "\n",
    "    split_idx = int(len(df) * (1 - test_size))\n",
    "    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# -----------------\n",
    "# 2. ATM Clustering (range of clusters)\n",
    "# -----------------\n",
    "def cluster_atms_range(df, cluster_range=(1, 4)):\n",
    "    results = {}\n",
    "\n",
    "    # Only use active days for clustering\n",
    "    active_df = df[df[\"is_active\"] == 1]\n",
    "\n",
    "    # ATM profiles: average withdrawals during active period\n",
    "    atm_profiles = active_df.groupby(\"CASHP_ID\")[\"WITHDRWLS\"].mean().to_frame(\"avg_withdrawals\")\n",
    "\n",
    "    for n in range(cluster_range[0], cluster_range[1] + 1):\n",
    "        kmeans = KMeans(n_clusters=n, random_state=42, n_init=10)\n",
    "        atm_profiles[f\"cluster_{n}\"] = kmeans.fit_predict(atm_profiles[[\"avg_withdrawals\"]])\n",
    "\n",
    "        # Merge back cluster labels into full df\n",
    "        df_clustered = df.merge(atm_profiles[f\"cluster_{n}\"], on=\"CASHP_ID\", how=\"left\")\n",
    "        df_clustered = df_clustered.rename(columns={f\"cluster_{n}\": \"cluster\"})\n",
    "\n",
    "        results[n] = (df_clustered, kmeans)\n",
    "\n",
    "    return results\n",
    "# -----------------\n",
    "# 3. Forecast Metrics\n",
    "# -----------------\n",
    "def calculate_forecast_metrics(y_true, y_pred, model_name=\"Model\"):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "\n",
    "    mae  = np.mean(np.abs(y_true - y_pred))\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "    # Zero-aware MAPE\n",
    "    nz = y_true != 0\n",
    "    if nz.any():\n",
    "        mape = np.mean(np.abs((y_true[nz] - y_pred[nz]) / np.abs(y_true[nz]))) * 100\n",
    "    else:\n",
    "        mape = np.nan  \n",
    "\n",
    "    # sMAPE\n",
    "    smape = np.mean(\n",
    "        2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + 1e-9)\n",
    "    ) * 100\n",
    "\n",
    "    # WMAPE-\n",
    "    denom = np.sum(np.abs(y_true))\n",
    "    wmape = (np.sum(np.abs(y_true - y_pred)) / denom * 100) if denom > 0 else np.nan\n",
    "\n",
    "    return {\n",
    "        \"Model\": model_name,\n",
    "        \"MAE\": mae,\n",
    "        \"RMSE\": rmse,\n",
    "        \"MAPE\": mape,  \n",
    "        \"SMAPE\": smape,\n",
    "        \"WMAPE\": wmape,  \n",
    "    }\n",
    "# -----------------\n",
    "# 4. Normalization (target only, applied after split)\n",
    "# -----------------\n",
    "def normalize_target(y_train, y_test, method=\"log\"):\n",
    "    if method == \"log\":\n",
    "        y_train_norm = np.log1p(y_train)\n",
    "        y_test_norm  = np.log1p(y_test)\n",
    "        scaler = None\n",
    "\n",
    "    elif method == \"minmax\":\n",
    "        scaler = MinMaxScaler()\n",
    "        y_train_norm = scaler.fit_transform(y_train.to_frame()).ravel()\n",
    "        y_test_norm  = scaler.transform(y_test.to_frame()).ravel()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"method must be 'log' or 'minmax'\")\n",
    "\n",
    "    return y_train_norm, y_test_norm, scaler\n",
    "\n",
    "# -----------------\n",
    "# 5. Reverse transform predictions\n",
    "# -----------------\n",
    "def inverse_transform_predictions(y_pred_norm, method=\"log\", scaler=None):\n",
    "    if method == \"log\":\n",
    "        return np.expm1(y_pred_norm)\n",
    "    elif method == \"minmax\" and scaler is not None:\n",
    "        return scaler.inverse_transform(y_pred_norm.reshape(-1,1)).ravel()\n",
    "    else:\n",
    "        return y_pred_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d651693",
   "metadata": {},
   "source": [
    "LSTM with Feature Set 0 (No Feature Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36d3cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Dataset\n",
    "# -----------------\n",
    "class SeqOnlyDataset(Dataset):\n",
    "    def __init__(self, y, seq_len=30):\n",
    "        self.y = np.array(y, dtype=np.float32)\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X_seq = self.y[idx:idx+self.seq_len]\n",
    "        y_seq = self.y[idx+self.seq_len]\n",
    "        return (\n",
    "            torch.tensor(X_seq, dtype=torch.float32).unsqueeze(-1), \n",
    "            torch.tensor(y_seq, dtype=torch.float32)                 \n",
    "        )\n",
    "\n",
    "# -----------------\n",
    "# LSTM Model\n",
    "# -----------------\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers,\n",
    "                            batch_first=True,\n",
    "                            dropout=dropout if num_layers > 1 else 0.0)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out.view(-1)          \n",
    "\n",
    "# -----------------\n",
    "# Runner with Optuna\n",
    "# -----------------\n",
    "def run_lstm_univariate_with_clustering(df, cluster_range=(1,4),\n",
    "                                        seq_lens=[7,14,30],\n",
    "                                        n_trials=30,\n",
    "                                        norm_method=\"minmax\"):\n",
    "    \"\"\"\n",
    "    Runs univariate (target-only) LSTM per cluster.\n",
    "    - Keeps zero withdrawals\n",
    "    - Drops only inactive rows (is_active == 0)\n",
    "    - Clusters on active periods\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    all_metrics = []\n",
    "\n",
    "    # keep only active rows\n",
    "    df = df[df[\"is_active\"] == 1].copy()\n",
    "\n",
    "    clustered_results = cluster_atms_range(df, cluster_range=cluster_range)\n",
    "\n",
    "    for n_clusters, (df_clustered, _) in clustered_results.items():\n",
    "        print(f\"\\n=== LSTM (Univariate) with {n_clusters} Clusters ===\")\n",
    "\n",
    "        for seq_len in seq_lens:\n",
    "            print(f\"\\n--- Seq_len={seq_len} ---\")\n",
    "\n",
    "            for cluster_id in sorted(df_clustered[\"cluster\"].dropna().unique()):\n",
    "                cluster_df = df_clustered[df_clustered[\"cluster\"] == cluster_id].copy()\n",
    "                if cluster_df.empty:\n",
    "                    continue\n",
    "\n",
    "                # --- Train/val split (temporal)\n",
    "                y_train, y_val = np.split(cluster_df[\"WITHDRWLS\"].values,\n",
    "                                          [int(len(cluster_df)*0.8)])\n",
    "\n",
    "                # --- Normalize\n",
    "                y_train_norm, y_val_norm, scaler = normalize_target(\n",
    "                    pd.Series(y_train), pd.Series(y_val), method=norm_method\n",
    "                )\n",
    "\n",
    "                trial_metrics = []\n",
    "\n",
    "                def objective(trial):\n",
    "                    hidden_dim = trial.suggest_int(\"hidden_dim\", 16, 128, step=16)\n",
    "                    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "                    dropout    = trial.suggest_float(\"dropout\", 0.0, 0.5)\n",
    "                    lr         = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "                    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "\n",
    "                    train_dataset = SeqOnlyDataset(y_train_norm, seq_len)\n",
    "                    val_dataset   = SeqOnlyDataset(y_val_norm, seq_len)\n",
    "                    train_loader  = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "                    val_loader    = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "                    model = LSTMModel(input_dim=1,\n",
    "                                      hidden_dim=hidden_dim,\n",
    "                                      num_layers=num_layers,\n",
    "                                      dropout=dropout).to(device)\n",
    "                    criterion = nn.MSELoss()\n",
    "                    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "                    # Training\n",
    "                    for _ in range(20):\n",
    "                        model.train()\n",
    "                        for X_batch, y_batch in train_loader:\n",
    "                            X_batch, y_batch = X_batch.to(device), y_batch.to(device).view(-1)\n",
    "                            optimizer.zero_grad()\n",
    "                            preds = model(X_batch)\n",
    "                            loss = criterion(preds, y_batch)\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # Validation\n",
    "                    model.eval()\n",
    "                    preds_norm, trues_norm = [], []\n",
    "                    with torch.no_grad():\n",
    "                        for X_batch, y_batch in val_loader:\n",
    "                            X_batch = X_batch.to(device)\n",
    "                            out = model(X_batch).cpu().numpy()\n",
    "                            preds_norm.extend(out)\n",
    "                            trues_norm.extend(y_batch.numpy())\n",
    "\n",
    "                    preds = inverse_transform_predictions(np.array(preds_norm), method=norm_method, scaler=scaler)\n",
    "                    trues = inverse_transform_predictions(np.array(trues_norm), method=norm_method, scaler=scaler)\n",
    "\n",
    "                    preds = np.nan_to_num(preds, nan=0.0, posinf=1e10, neginf=0.0)\n",
    "                    trues = np.nan_to_num(trues, nan=0.0, posinf=1e10, neginf=0.0)\n",
    "\n",
    "                    metrics = calculate_forecast_metrics(trues, preds,\n",
    "                                                         model_name=f\"LSTM trial {trial.number}\")\n",
    "                    metrics[\"trial\"] = trial.number\n",
    "                    trial_metrics.append(metrics)\n",
    "\n",
    "                    return metrics[\"SMAPE\"]\n",
    "\n",
    "                # Run Optuna\n",
    "                optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "                study = optuna.create_study(direction=\"minimize\")\n",
    "                study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "                # Best trial\n",
    "                best_trial = study.best_trial.number\n",
    "                best_metrics = [m for m in trial_metrics if m[\"trial\"] == best_trial][0]\n",
    "                best_metrics.update({\n",
    "                    \"Seq_len\": seq_len,\n",
    "                    \"Best_Params\": study.best_trial.params,\n",
    "                    \"n_clusters\": n_clusters,\n",
    "                    \"cluster_id\": cluster_id\n",
    "                })\n",
    "\n",
    "                all_metrics.append(best_metrics)\n",
    "\n",
    "    return pd.DataFrame(all_metrics)\n",
    "\n",
    "# Run\n",
    "no_feature_lstm_with0 = run_lstm_univariate_with_clustering(\n",
    "    feature_eng_data,\n",
    "    cluster_range=(1,4),\n",
    "    seq_lens=[7, 14, 30, 90],\n",
    "    n_trials=20,\n",
    "    norm_method=\"log\"\n",
    ")\n",
    "\n",
    "print(no_feature_lstm_with0)\n",
    "save_df_to_downloads(\"no_feature_lstm_with0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47b13d3",
   "metadata": {},
   "source": [
    "N-Beats with Feature Set 0 (No Feature Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad41414e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Dataset (target-only)\n",
    "# -----------------\n",
    "class SeqOnlyDataset(Dataset):\n",
    "    def __init__(self, y, seq_len=30):\n",
    "        self.y = np.array(y, dtype=np.float32)\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X_seq = self.y[idx:idx+self.seq_len]\n",
    "        y_seq = self.y[idx+self.seq_len]\n",
    "        return (\n",
    "            torch.tensor(X_seq, dtype=torch.float32).unsqueeze(-1),  \n",
    "            torch.tensor(y_seq, dtype=torch.float32)                \n",
    "        )\n",
    "\n",
    "# -----------------\n",
    "# N-BEATS Model (Univariate)\n",
    "# -----------------\n",
    "class NBeatsBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim, theta_dim, backcast_length, forecast_length):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(backcast_length, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, theta_dim)\n",
    "        self.backcast_length = backcast_length\n",
    "        self.forecast_length = forecast_length\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        theta = self.fc4(x)\n",
    "        backcast = theta[:, :self.backcast_length]\n",
    "        forecast = theta[:, -self.forecast_length:]\n",
    "        return backcast, forecast\n",
    "\n",
    "class NBeats(nn.Module):\n",
    "    def __init__(self, hidden_dim, backcast_length, forecast_length, n_blocks=3):\n",
    "        super().__init__()\n",
    "        theta_dim = backcast_length + forecast_length\n",
    "        self.blocks = nn.ModuleList([\n",
    "            NBeatsBlock(hidden_dim, theta_dim, backcast_length, forecast_length)\n",
    "            for _ in range(n_blocks)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        backcast = x\n",
    "        forecast = torch.zeros(x.size(0), 1, device=x.device)\n",
    "        for block in self.blocks:\n",
    "            b, f = block(backcast)\n",
    "            backcast = backcast - b\n",
    "            forecast = forecast + f\n",
    "        return forecast.view(-1)\n",
    "\n",
    "# -----------------\n",
    "# Runner with Optuna\n",
    "# -----------------\n",
    "def run_nbeats_univariate_with_clustering(df, cluster_range=(1,4),\n",
    "                                          seq_lens=[7,14,30],\n",
    "                                          n_trials=30,\n",
    "                                          norm_method=\"minmax\"):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    all_metrics = []\n",
    "\n",
    "    # Drop inactive days\n",
    "    df = df[df[\"is_active\"] == 1].copy()\n",
    "\n",
    "    clustered_results = cluster_atms_range(df, cluster_range=cluster_range)\n",
    "\n",
    "    for n_clusters, (df_clustered, _) in clustered_results.items():\n",
    "        print(f\"\\n=== N-BEATS (Univariate) with {n_clusters} Clusters ===\")\n",
    "\n",
    "        for seq_len in seq_lens:\n",
    "            print(f\"\\n--- Seq_len={seq_len} ---\")\n",
    "\n",
    "            for cluster_id in sorted(df_clustered[\"cluster\"].dropna().unique()):\n",
    "                cluster_df = df_clustered[df_clustered[\"cluster\"] == cluster_id].copy()\n",
    "                if cluster_df.empty:\n",
    "                    continue\n",
    "\n",
    "                # Target split\n",
    "                y_train, y_val = np.split(cluster_df[\"WITHDRWLS\"].values,\n",
    "                                          [int(len(cluster_df)*0.8)])\n",
    "\n",
    "                # --- Normalize\n",
    "                y_train_norm, y_val_norm, scaler = normalize_target(\n",
    "                    pd.Series(y_train), pd.Series(y_val), method=norm_method\n",
    "                )\n",
    "\n",
    "                trial_metrics = []\n",
    "\n",
    "                def objective(trial):\n",
    "                    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 256, step=32)\n",
    "                    n_blocks   = trial.suggest_int(\"n_blocks\", 2, 6)\n",
    "                    lr         = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "                    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "\n",
    "                    train_dataset = SeqOnlyDataset(y_train_norm, seq_len)\n",
    "                    val_dataset   = SeqOnlyDataset(y_val_norm, seq_len)\n",
    "                    train_loader  = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "                    val_loader    = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "                    model = NBeats(hidden_dim=hidden_dim,\n",
    "                                   backcast_length=seq_len,\n",
    "                                   forecast_length=1,\n",
    "                                   n_blocks=n_blocks).to(device)\n",
    "\n",
    "                    criterion = nn.MSELoss()\n",
    "                    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "                    # Training\n",
    "                    for _ in range(20):\n",
    "                        model.train()\n",
    "                        for X_batch, y_batch in train_loader:\n",
    "                            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                            optimizer.zero_grad()\n",
    "                            preds = model(X_batch)\n",
    "                            loss = criterion(preds, y_batch)\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # Validation\n",
    "                    model.eval()\n",
    "                    preds_norm, trues_norm = [], []\n",
    "                    with torch.no_grad():\n",
    "                        for X_batch, y_batch in val_loader:\n",
    "                            X_batch = X_batch.to(device)\n",
    "                            out = model(X_batch).cpu().numpy()\n",
    "                            preds_norm.extend(out)\n",
    "                            trues_norm.extend(y_batch.numpy())\n",
    "\n",
    "                    preds = inverse_transform_predictions(np.array(preds_norm), method=norm_method, scaler=scaler)\n",
    "                    trues = inverse_transform_predictions(np.array(trues_norm), method=norm_method, scaler=scaler)\n",
    "\n",
    "                    preds = np.nan_to_num(preds, nan=0.0, posinf=1e10, neginf=0.0)\n",
    "                    trues = np.nan_to_num(trues, nan=0.0, posinf=1e10, neginf=0.0)\n",
    "\n",
    "                    metrics = calculate_forecast_metrics(trues, preds,\n",
    "                                                         model_name=f\"N-BEATS trial {trial.number}\")\n",
    "                    metrics[\"trial\"] = trial.number\n",
    "                    trial_metrics.append(metrics)\n",
    "\n",
    "                    return metrics[\"SMAPE\"]\n",
    "\n",
    "                # Optuna\n",
    "                optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "                study = optuna.create_study(direction=\"minimize\")\n",
    "                study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "                # Best trial\n",
    "                best_trial = study.best_trial.number\n",
    "                best_metrics = [m for m in trial_metrics if m[\"trial\"] == best_trial][0]\n",
    "                best_metrics.update({\n",
    "                    \"Seq_len\": seq_len,\n",
    "                    \"Best_Params\": study.best_trial.params,\n",
    "                    \"n_clusters\": n_clusters,\n",
    "                    \"cluster_id\": cluster_id\n",
    "                })\n",
    "\n",
    "                all_metrics.append(best_metrics)\n",
    "\n",
    "    return pd.DataFrame(all_metrics)\n",
    "\n",
    "# Run\n",
    "no_feature_nbeats_with0 = run_nbeats_univariate_with_clustering(\n",
    "    feature_eng_data,\n",
    "    cluster_range=(1,4),\n",
    "    seq_lens=[7, 14, 30, 90],\n",
    "    n_trials=20,\n",
    "    norm_method=\"log\"\n",
    ")\n",
    "\n",
    "print(no_feature_nbeats_with0)\n",
    "save_df_to_downloads(\"no_feature_nbeats_with0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd72d34",
   "metadata": {},
   "source": [
    "Linear Regression with Feature Set 1 (Dummy Variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e36132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Runner with Optuna\n",
    "# -----------------\n",
    "def run_linear_regression_per_cluster(df, cluster_range=(1,4), test_size=0.2, norm_method=\"log\"):\n",
    "    \"\"\"\n",
    "    Run KMeans clustering for a range of clusters, then\n",
    "    train and evaluate Linear Regression separately for each cluster.\n",
    "    \n",
    "    Steps:\n",
    "    - Keep only active rows (is_active == 1)\n",
    "    - Clusters ATMs by demand level\n",
    "    - Splits into train/test by time (via prepare_data)\n",
    "    - Normalizes target AFTER split (to prevent leakage)\n",
    "    - Fits model and back-transforms predictions\n",
    "    - Evaluates in original WITHDRWLS units\n",
    "    \"\"\"\n",
    "    # Keep only active rows\n",
    "    df = df[df[\"is_active\"] == 1].copy()\n",
    "\n",
    "    all_metrics = {}\n",
    "\n",
    "    clustered_results = cluster_atms_range(df, cluster_range=cluster_range)\n",
    "    \n",
    "    for n_clusters, (df_clustered, _) in clustered_results.items():\n",
    "        print(f\"\\n=== Linear Regression with {n_clusters} Clusters ===\")\n",
    "\n",
    "        for cluster_id in sorted(df_clustered[\"cluster\"].dropna().unique()):\n",
    "            cluster_df = df_clustered[df_clustered[\"cluster\"] == cluster_id].copy()\n",
    "\n",
    "            if cluster_df.empty:\n",
    "                print(f\"Cluster {cluster_id} has no active rows. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Use prepare_data\n",
    "            X_train, X_test, y_train, y_test = prepare_data(cluster_df, test_size=test_size)\n",
    "\n",
    "            # Normalize AFTER split\n",
    "            y_train_norm, y_test_norm, scaler = normalize_target(y_train, y_test, method=norm_method)\n",
    "\n",
    "            # Train\n",
    "            model = LinearRegression()\n",
    "            model.fit(X_train, y_train_norm)\n",
    "\n",
    "            # Predict in normalized space\n",
    "            y_pred_norm = model.predict(X_test)\n",
    "\n",
    "            # Back-transform\n",
    "            y_pred_raw = inverse_transform_predictions(y_pred_norm, method=norm_method, scaler=scaler)\n",
    "\n",
    "            # Evaluate\n",
    "            metrics = calculate_forecast_metrics(\n",
    "                y_test, y_pred_raw,\n",
    "                model_name=f\"LR (k={n_clusters}, cluster={cluster_id})\"\n",
    "            )\n",
    "            metrics[\"n_clusters\"] = n_clusters\n",
    "            metrics[\"cluster_id\"] = cluster_id\n",
    "\n",
    "            all_metrics.setdefault(n_clusters, []).append(metrics)\n",
    "\n",
    "    # Flatten results\n",
    "    results = [m for cluster_metrics in all_metrics.values() for m in cluster_metrics]\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run\n",
    "dummy_var_lin_reg_with0 = run_linear_regression_per_cluster(\n",
    "    feature_eng_data,\n",
    "    cluster_range=(1,4),\n",
    "    test_size=0.2,\n",
    "    norm_method=\"log\"\n",
    ")\n",
    "\n",
    "print(dummy_var_lin_reg_with0)\n",
    "save_df_to_downloads(\"dummy_var_lin_reg_with0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684496cd",
   "metadata": {},
   "source": [
    "XGBoost with Feature Set 1 (Dummy Variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4705ff1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Runner with Optuna\n",
    "# -----------------\n",
    "def run_xgboost_per_cluster(df, cluster_range=(1,4), test_size=0.2, method=\"log\", n_trials=50):\n",
    "    \"\"\"\n",
    "    For each cluster setting (k), and for each cluster:\n",
    "      - Keep only active rows (is_active == 1)\n",
    "      - Split train/val temporally (via prepare_data)\n",
    "      - Normalize target AFTER split using chosen method\n",
    "      - Train & tune XGBoost with Optuna\n",
    "      - Reverse transform predictions\n",
    "      - Evaluate with forecast metrics (raw scale)\n",
    "    \"\"\"\n",
    "    # Keep only active rows\n",
    "    df = df[df[\"is_active\"] == 1].copy()\n",
    "\n",
    "    all_metrics = []\n",
    "\n",
    "    clustered_results = cluster_atms_range(df, cluster_range=cluster_range)\n",
    "\n",
    "    for n_clusters, (df_clustered, _) in clustered_results.items():\n",
    "        print(f\"\\n=== XGBoost with {n_clusters} Clusters ===\")\n",
    "\n",
    "        for cluster_id in sorted(df_clustered[\"cluster\"].dropna().unique()):\n",
    "            cluster_df = df_clustered[df_clustered[\"cluster\"] == cluster_id].copy()\n",
    "\n",
    "            if cluster_df.empty:\n",
    "                print(f\"Cluster {cluster_id} has no active rows. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Use prepare_data\n",
    "            X_train, X_val, y_train, y_val = prepare_data(cluster_df, test_size=test_size)\n",
    "            # Normalize target\n",
    "            y_train_norm, y_val_norm, scaler = normalize_target(y_train, y_val, method=method)\n",
    "\n",
    "            # Optuna objective\n",
    "            def objective(trial):\n",
    "                params = {\n",
    "                    \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 1000),\n",
    "                    \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.1, log=True),\n",
    "                    \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "                    \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "                    \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n",
    "                    \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "                    \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "                    \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True),\n",
    "                    \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n",
    "                    \"random_state\": 42,\n",
    "                    \"tree_method\": \"hist\"\n",
    "                }\n",
    "\n",
    "                model = xgb.XGBRegressor(**params)\n",
    "                model.fit(X_train, y_train_norm,\n",
    "                          eval_set=[(X_val, y_val_norm)],\n",
    "                          verbose=False)\n",
    "\n",
    "                # Predict in normalized space\n",
    "                y_pred_norm = model.predict(X_val)\n",
    "\n",
    "                # Reverse transform\n",
    "                y_pred_raw = inverse_transform_predictions(y_pred_norm, method=method, scaler=scaler)\n",
    "\n",
    "                # Evaluate\n",
    "                metrics = calculate_forecast_metrics(\n",
    "                    y_val, y_pred_raw,\n",
    "                    model_name=f\"XGB (k={n_clusters}, cluster={cluster_id})\"\n",
    "                )\n",
    "\n",
    "                trial.set_user_attr(\"mae\", metrics[\"MAE\"])\n",
    "                return metrics[\"SMAPE\"]\n",
    "\n",
    "            # Run Optuna\n",
    "            study = optuna.create_study(direction=\"minimize\")\n",
    "            study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "            best_smape = study.best_value\n",
    "            best_mae   = study.best_trial.user_attrs[\"mae\"]\n",
    "            best_params = study.best_params\n",
    "\n",
    "            all_metrics.append({\n",
    "                \"Model\": f\"XGB (k={n_clusters}, cluster={cluster_id})\",\n",
    "                \"Best_SMAPE\": best_smape,\n",
    "                \"Best_MAE\": best_mae,\n",
    "                \"Best_Params\": best_params,\n",
    "                \"n_clusters\": n_clusters,\n",
    "                \"cluster_id\": cluster_id\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(all_metrics)\n",
    "\n",
    "# Run\n",
    "dummy_var_xgb_with0 = run_xgboost_per_cluster(\n",
    "    feature_eng_data,\n",
    "    cluster_range=(1,4),\n",
    "    test_size=0.2,\n",
    "    method=\"log\",\n",
    "    n_trials=100\n",
    ")\n",
    "\n",
    "print(dummy_var_xgb_with0)\n",
    "save_df_to_downloads(\"dummy_var_xgb_with0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194d55f3",
   "metadata": {},
   "source": [
    "LSTM with Feature Set 1 (Dummy Variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71720850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Dataset Class\n",
    "# -----------------\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, X, y, seq_len=30):\n",
    "        self.X = np.array(X, dtype=np.float32)\n",
    "        self.y = np.array(y, dtype=np.float32)\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X_seq = self.X[idx:idx+self.seq_len]\n",
    "        y_seq = self.y[idx+self.seq_len]\n",
    "        return torch.tensor(X_seq, dtype=torch.float32), torch.tensor(y_seq, dtype=torch.float32)\n",
    "\n",
    "# -----------------\n",
    "# LSTM Model\n",
    "# -----------------\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, hidden_dim, num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out.view(-1)\n",
    "\n",
    "# -----------------\n",
    "# Runner with Optuna\n",
    "# -----------------\n",
    "def run_lstm_with_clustering(df, cluster_range=(1,4), seq_lens=[7,14,30], n_trials=30, norm_method=\"minmax\"):\n",
    "    \"\"\"\n",
    "    Runs LSTM with Optuna per cluster and sequence length.\n",
    "    Keeps zeros, filters only inactive rows (is_active == 0).\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    all_metrics = []\n",
    "\n",
    "    # Only active rows\n",
    "    df = df[df[\"is_active\"] == 1].copy()\n",
    "\n",
    "    clustered_results = cluster_atms_range(df, cluster_range=cluster_range)\n",
    "\n",
    "    for n_clusters, (df_clustered, _) in clustered_results.items():\n",
    "        print(f\"\\n=== LSTM with {n_clusters} Clusters ===\")\n",
    "\n",
    "        for seq_len in seq_lens:\n",
    "            print(f\"\\n--- Seq_len={seq_len} ---\")\n",
    "\n",
    "            for cluster_id in sorted(df_clustered[\"cluster\"].dropna().unique()):\n",
    "                cluster_df = df_clustered[df_clustered[\"cluster\"] == cluster_id].copy()\n",
    "                if cluster_df.empty:\n",
    "                    continue\n",
    "\n",
    "                # Train/test split\n",
    "                X_train, X_val, y_train, y_val = prepare_data(cluster_df, test_size=0.2)\n",
    "                X_train, X_val = X_train.astype(np.float32), X_val.astype(np.float32)\n",
    "\n",
    "                # Normalize target\n",
    "                y_train_norm, y_val_norm, scaler = normalize_target(y_train, y_val, method=norm_method)\n",
    "\n",
    "                # Optuna objective\n",
    "                def objective(trial):\n",
    "                    hidden_dim = trial.suggest_int(\"hidden_dim\", 16, 128, step=16)\n",
    "                    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "                    dropout    = trial.suggest_float(\"dropout\", 0.0, 0.5)\n",
    "                    lr         = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "                    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "\n",
    "                    train_dataset = SeqDataset(X_train.values, y_train_norm, seq_len)\n",
    "                    val_dataset   = SeqDataset(X_val.values, y_val_norm, seq_len)\n",
    "                    train_loader  = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "                    val_loader    = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "                    model = LSTMModel(input_dim=X_train.shape[1], hidden_dim=hidden_dim,\n",
    "                                      num_layers=num_layers, dropout=dropout).to(device)\n",
    "                    criterion = nn.MSELoss()\n",
    "                    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "                    for _ in range(20):\n",
    "                        model.train()\n",
    "                        for X_batch, y_batch in train_loader:\n",
    "                            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                            optimizer.zero_grad()\n",
    "                            preds = model(X_batch)\n",
    "                            loss = criterion(preds, y_batch)\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # Validation\n",
    "                    model.eval()\n",
    "                    preds_norm, trues_norm = [], []\n",
    "                    with torch.no_grad():\n",
    "                        for X_batch, y_batch in val_loader:\n",
    "                            X_batch = X_batch.to(device)\n",
    "                            out = model(X_batch).cpu().numpy()\n",
    "                            preds_norm.extend(out)\n",
    "                            trues_norm.extend(y_batch.numpy())\n",
    "\n",
    "                    preds = inverse_transform_predictions(np.array(preds_norm), method=norm_method, scaler=scaler)\n",
    "                    trues = inverse_transform_predictions(np.array(trues_norm), method=norm_method, scaler=scaler)\n",
    "\n",
    "                    smape = 100 * np.mean(2 * np.abs(preds - trues) /\n",
    "                                          (np.abs(preds) + np.abs(trues) + 1e-9))\n",
    "                    return smape\n",
    "\n",
    "                # Run Optuna\n",
    "                optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "                study = optuna.create_study(direction=\"minimize\")\n",
    "                study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "                best_params = study.best_params\n",
    "\n",
    "                # Retrain with best params\n",
    "                train_dataset = SeqDataset(X_train.values, y_train_norm, seq_len)\n",
    "                val_dataset   = SeqDataset(X_val.values, y_val_norm, seq_len)\n",
    "                train_loader  = DataLoader(train_dataset, batch_size=best_params[\"batch_size\"], shuffle=True)\n",
    "                val_loader    = DataLoader(val_dataset, batch_size=best_params[\"batch_size\"], shuffle=False)\n",
    "\n",
    "                model = LSTMModel(input_dim=X_train.shape[1], hidden_dim=best_params[\"hidden_dim\"],\n",
    "                                  num_layers=best_params[\"num_layers\"], dropout=best_params[\"dropout\"]).to(device)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=best_params[\"lr\"])\n",
    "                criterion = nn.MSELoss()\n",
    "\n",
    "                for _ in range(20):\n",
    "                    model.train()\n",
    "                    for X_batch, y_batch in train_loader:\n",
    "                        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                        optimizer.zero_grad()\n",
    "                        preds = model(X_batch)\n",
    "                        loss = criterion(preds, y_batch)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Final evaluation\n",
    "                model.eval()\n",
    "                preds_norm, trues_norm = [], []\n",
    "                with torch.no_grad():\n",
    "                    for X_batch, y_batch in val_loader:\n",
    "                        X_batch = X_batch.to(device)\n",
    "                        out = model(X_batch).cpu().numpy()\n",
    "                        preds_norm.extend(out)\n",
    "                        trues_norm.extend(y_batch.numpy())\n",
    "\n",
    "                preds = inverse_transform_predictions(np.array(preds_norm), method=norm_method, scaler=scaler)\n",
    "                trues = inverse_transform_predictions(np.array(trues_norm), method=norm_method, scaler=scaler)\n",
    "\n",
    "                metrics = calculate_forecast_metrics(\n",
    "                    trues, preds,\n",
    "                    model_name=f\"LSTM (k={n_clusters}, c={cluster_id}, seq={seq_len})\"\n",
    "                )\n",
    "                metrics.update({\n",
    "                    \"Seq_len\": seq_len,\n",
    "                    \"Best_Params\": best_params,\n",
    "                    \"n_clusters\": n_clusters,\n",
    "                    \"cluster_id\": cluster_id\n",
    "                })\n",
    "\n",
    "                all_metrics.append(metrics)\n",
    "\n",
    "    return pd.DataFrame(all_metrics)\n",
    "\n",
    "# Run\n",
    "dummy_var_lstm_with0 = run_lstm_with_clustering(\n",
    "    feature_eng_data,\n",
    "    cluster_range=(1,4),\n",
    "    seq_lens=[7,14,30,90],\n",
    "    n_trials=20,\n",
    "    norm_method=\"log\"\n",
    ")\n",
    "\n",
    "print(dummy_var_lstm_with0)\n",
    "save_df_to_downloads(\"dummy_var_lstm_with0\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eb46f9",
   "metadata": {},
   "source": [
    "N-Beats with Feature Set 1 (Dummy Variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7279ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# N-BEATS Model\n",
    "# -----------------\n",
    "class NBeatsBlock(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, theta_dim, backcast_length, forecast_length):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(backcast_length * input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, theta_dim)\n",
    "        self.backcast_length = backcast_length\n",
    "        self.forecast_length = forecast_length\n",
    "        self.theta_dim = theta_dim\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.size(0), -1) \n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        theta = self.fc4(x)\n",
    "\n",
    "        backcast = theta[:, :self.backcast_length * self.input_dim].reshape(\n",
    "            x.size(0), self.backcast_length, self.input_dim\n",
    "        )\n",
    "        forecast = theta[:, -self.forecast_length:]\n",
    "        return backcast, forecast\n",
    "\n",
    "class NBeats(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, backcast_length, forecast_length, n_blocks=3):\n",
    "        super().__init__()\n",
    "        theta_dim = backcast_length * input_dim + forecast_length\n",
    "        self.blocks = nn.ModuleList([\n",
    "            NBeatsBlock(input_dim, hidden_dim, theta_dim, backcast_length, forecast_length)\n",
    "            for _ in range(n_blocks)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        backcast = x\n",
    "        forecast = 0\n",
    "        for block in self.blocks:\n",
    "            b, f = block(backcast)\n",
    "            backcast = backcast - b  \n",
    "            forecast = forecast + f \n",
    "        return forecast.view(-1)\n",
    "\n",
    "# -----------------\n",
    "# Runner with Optuna\n",
    "# -----------------\n",
    "def run_nbeats_with_clustering(df, cluster_range=(1,4), seq_lens=[7,14,30], n_trials=30, norm_method=\"minmax\"):\n",
    "    \"\"\"\n",
    "    Runs N-BEATS with Optuna per cluster and sequence length.\n",
    "    Keeps zeros, drops only inactive rows (is_active == 0).\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    all_metrics = []\n",
    "\n",
    "    # filter inactive rows\n",
    "    df = df[df[\"is_active\"] == 1].copy()\n",
    "\n",
    "    clustered_results = cluster_atms_range(df, cluster_range=cluster_range)\n",
    "\n",
    "    for n_clusters, (df_clustered, _) in clustered_results.items():\n",
    "        for seq_len in seq_lens:\n",
    "            for cluster_id in sorted(df_clustered[\"cluster\"].dropna().unique()):\n",
    "                cluster_df = df_clustered[df_clustered[\"cluster\"] == cluster_id].copy()\n",
    "                if cluster_df.empty:\n",
    "                    continue\n",
    "\n",
    "                # Train/test split\n",
    "                X_train, X_val, y_train, y_val = prepare_data(cluster_df, test_size=0.2)\n",
    "                X_train, X_val = X_train.astype(np.float32), X_val.astype(np.float32)\n",
    "\n",
    "                # Normalize target\n",
    "                y_train_norm, y_val_norm, scaler = normalize_target(y_train, y_val, method=norm_method)\n",
    "\n",
    "                trial_metrics = []\n",
    "\n",
    "                def objective(trial):\n",
    "                    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 256, step=32)\n",
    "                    n_blocks   = trial.suggest_int(\"n_blocks\", 2, 6)\n",
    "                    lr         = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "                    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "\n",
    "                    train_dataset = SeqDataset(X_train.values, y_train_norm, seq_len)\n",
    "                    val_dataset   = SeqDataset(X_val.values, y_val_norm, seq_len)\n",
    "                    train_loader  = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "                    val_loader    = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "                    model = NBeats(\n",
    "                        input_dim=X_train.shape[1],\n",
    "                        hidden_dim=hidden_dim,\n",
    "                        backcast_length=seq_len,\n",
    "                        forecast_length=1,\n",
    "                        n_blocks=n_blocks\n",
    "                    ).to(device)\n",
    "\n",
    "                    criterion = nn.MSELoss()\n",
    "                    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "                    for _ in range(20):\n",
    "                        model.train()\n",
    "                        for X_batch, y_batch in train_loader:\n",
    "                            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                            optimizer.zero_grad()\n",
    "                            preds = model(X_batch)\n",
    "                            loss = criterion(preds, y_batch)\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # Validation\n",
    "                    model.eval()\n",
    "                    preds_norm, trues_norm = [], []\n",
    "                    with torch.no_grad():\n",
    "                        for X_batch, y_batch in val_loader:\n",
    "                            X_batch = X_batch.to(device)\n",
    "                            out = model(X_batch).cpu().numpy()\n",
    "                            preds_norm.extend(out)\n",
    "                            trues_norm.extend(y_batch.numpy())\n",
    "\n",
    "                    preds = inverse_transform_predictions(np.array(preds_norm), method=norm_method, scaler=scaler)\n",
    "                    trues = inverse_transform_predictions(np.array(trues_norm), method=norm_method, scaler=scaler)\n",
    "\n",
    "                    preds = np.nan_to_num(preds, nan=0.0, posinf=1e10, neginf=0.0)\n",
    "                    trues = np.nan_to_num(trues, nan=0.0, posinf=1e10, neginf=0.0)\n",
    "\n",
    "                    metrics = calculate_forecast_metrics(trues, preds,\n",
    "                                                         model_name=f\"N-BEATS trial {trial.number}\")\n",
    "                    metrics[\"trial\"] = trial.number\n",
    "                    trial_metrics.append(metrics)\n",
    "                    return metrics[\"SMAPE\"]\n",
    "\n",
    "                # Run Optuna\n",
    "                optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "                study = optuna.create_study(direction=\"minimize\")\n",
    "                study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "                # Best trial\n",
    "                best_trial = study.best_trial.number\n",
    "                best_metrics = [m for m in trial_metrics if m[\"trial\"] == best_trial][0]\n",
    "                best_metrics.update({\n",
    "                    \"Seq_len\": seq_len,\n",
    "                    \"Best_Params\": study.best_trial.params,\n",
    "                    \"n_clusters\": n_clusters,\n",
    "                    \"cluster_id\": cluster_id\n",
    "                })\n",
    "\n",
    "                all_metrics.append(best_metrics)\n",
    "\n",
    "    return pd.DataFrame(all_metrics)\n",
    "\n",
    "# Run\n",
    "dummy_var_nbeats_with0 = run_nbeats_with_clustering(\n",
    "    feature_eng_data,\n",
    "    cluster_range=(1,4),\n",
    "    seq_lens=[7, 14, 30, 90],\n",
    "    n_trials=20,\n",
    "    norm_method=\"log\"\n",
    ")\n",
    "\n",
    "print(dummy_var_nbeats_with0)\n",
    "save_df_to_downloads(\"dummy_var_nbeats_with0\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a6e61a",
   "metadata": {},
   "source": [
    "Feature Set Creation for Feature Set 2 (Lag, Rolling Features, Normalization, Reverse Transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41f1c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Lag & Rolling logic\n",
    "# -----------------\n",
    "def get_lag_and_rolls(window):\n",
    "    base_lags = [1, 2, 3, 7, 14, 28]\n",
    "    base_rolls = [7, 14, 28]\n",
    "    lags = [lag for lag in base_lags if lag <= window]\n",
    "    rolls = [r for r in base_rolls if r <= window]\n",
    "    return lags, rolls\n",
    "\n",
    "# -----------------\n",
    "# Feature Engineering\n",
    "# -----------------\n",
    "def create_lag_roll_features(df, seq_len):\n",
    "    lags, rolls = get_lag_and_rolls(seq_len)\n",
    "    groups = []\n",
    "    for atm_id, group in df.groupby(\"CASHP_ID\"):\n",
    "        group = group.sort_values(\"DATE\").copy()\n",
    "        # Lags\n",
    "        for lag in lags:\n",
    "            group[f\"lag_{lag}\"] = group[\"WITHDRWLS\"].shift(lag)\n",
    "        # Rolling\n",
    "        for r in rolls:\n",
    "            group[f\"roll_mean_{r}\"] = group[\"WITHDRWLS\"].shift(1).rolling(r).mean()\n",
    "            group[f\"roll_std_{r}\"] = group[\"WITHDRWLS\"].shift(1).rolling(r).std()\n",
    "        groups.append(group)\n",
    "    df_feat = pd.concat(groups).reset_index(drop=True)\n",
    "    df_feat = df_feat.dropna().reset_index(drop=True)\n",
    "    return df_feat\n",
    "\n",
    "# -----------------\n",
    "# Normalization helpers\n",
    "# -----------------\n",
    "def normalize_data(train_df, val_df, feature_cols, target_col=\"WITHDRWLS\", method=\"log\"):\n",
    "    \"\"\"\n",
    "    Normalize both features (X) and target (y) using the same method.\n",
    "    - log: apply log1p\n",
    "    - minmax: apply MinMaxScaler\n",
    "    Returns: X_train, X_val, y_train, y_val, feature_scaler, target_scaler\n",
    "    \"\"\"\n",
    "    if method == \"log\":\n",
    "        # Features\n",
    "        X_train = np.log1p(train_df[feature_cols].values)\n",
    "        X_val   = np.log1p(val_df[feature_cols].values)\n",
    "        feat_scaler = None\n",
    "        # Target\n",
    "        y_train = np.log1p(train_df[target_col].values)\n",
    "        y_val   = np.log1p(val_df[target_col].values)\n",
    "        target_scaler = None\n",
    "\n",
    "    elif method == \"minmax\":\n",
    "        feat_scaler = MinMaxScaler()\n",
    "        target_scaler = MinMaxScaler()\n",
    "        # Features\n",
    "        X_train = feat_scaler.fit_transform(train_df[feature_cols])\n",
    "        X_val   = feat_scaler.transform(val_df[feature_cols])\n",
    "        # Target\n",
    "        y_train = target_scaler.fit_transform(train_df[[target_col]]).ravel()\n",
    "        y_val   = target_scaler.transform(val_df[[target_col]]).ravel()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"method must be 'log' or 'minmax'\")\n",
    "\n",
    "    return X_train, X_val, y_train, y_val, feat_scaler, target_scaler\n",
    "\n",
    "def inverse_transform_predictions(y_pred, method=\"log\", scaler=None):\n",
    "    \"\"\"Reverse normalization for predictions back to raw scale.\"\"\"\n",
    "    if method == \"log\":\n",
    "        return np.expm1(y_pred)\n",
    "    elif method == \"minmax\" and scaler is not None:\n",
    "        return scaler.inverse_transform(y_pred.reshape(-1, 1)).ravel()\n",
    "    else:\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bd5bc6",
   "metadata": {},
   "source": [
    "Linear Regression with Feature Set 2 (Lag and Rolling Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff41babe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Runner with Optuna\n",
    "# -----------------\n",
    "def run_linear_regression_with_lag_roll_with0(\n",
    "    atm,\n",
    "    cluster_range=(1,4),\n",
    "    seq_lens=[7,14,28,84],\n",
    "    norm_method=\"log\",\n",
    "    test_size=0.2\n",
    "):\n",
    "    all_metrics = []\n",
    "\n",
    "    # --- Add active flag once\n",
    "    atm_with_active = add_active_flag(atm)\n",
    "\n",
    "    for seq_len in seq_lens:\n",
    "        print(f\"\\n--- Seq_len={seq_len} ---\")\n",
    "\n",
    "        # --- Create lag/rolling features\n",
    "        df_feat = create_lag_roll_features(atm_with_active, seq_len)\n",
    "\n",
    "        # --- Drop dummy/calendar columns (keep only numeric features)\n",
    "        drop_cols = [c for c in df_feat.columns \n",
    "                     if c.startswith((\"dow_\", \"dom_\", \"month_\", \"day_school_holiday\", \"week_part\"))]\n",
    "        df_feat = df_feat.drop(columns=drop_cols, errors=\"ignore\")\n",
    "\n",
    "        # --- Clustering\n",
    "        clustered_results = cluster_atms_range(df_feat, cluster_range=cluster_range)\n",
    "\n",
    "        for n_clusters, (df_clustered, _) in clustered_results.items():\n",
    "            print(f\"\\n=== Linear Regression with {n_clusters} Clusters (Seq={seq_len}) ===\")\n",
    "\n",
    "            for cluster_id in sorted(df_clustered[\"cluster\"].dropna().unique()):\n",
    "                cluster_df = df_clustered[df_clustered[\"cluster\"] == cluster_id].copy()\n",
    "\n",
    "                # --- Keep only active periods\n",
    "                cluster_df = cluster_df[cluster_df[\"is_active\"] == 1].reset_index(drop=True)\n",
    "                if cluster_df.empty:\n",
    "                    print(f\"Cluster {cluster_id} has no active data. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                # --- Train/val split\n",
    "                split_idx = int(len(cluster_df) * (1 - test_size))\n",
    "                train_df, val_df = cluster_df.iloc[:split_idx], cluster_df.iloc[split_idx:]\n",
    "\n",
    "                # --- Features\n",
    "                feature_cols = [c for c in cluster_df.columns \n",
    "                                if c not in [\"CASHP_ID\", \"DATE\", \"WITHDRWLS\", \"cluster\"]]\n",
    "\n",
    "                # --- Normalize (features + target)\n",
    "                X_train, X_val, y_train, y_val, feat_scaler, target_scaler = normalize_data(\n",
    "                    train_df, val_df, feature_cols, target_col=\"WITHDRWLS\", method=norm_method\n",
    "                )\n",
    "\n",
    "                # --- Train Linear Regression\n",
    "                model = LinearRegression()\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                # --- Predict\n",
    "                y_pred_norm = model.predict(X_val)\n",
    "\n",
    "                # --- Reverse-transform to raw scale\n",
    "                y_pred_raw = inverse_transform_predictions(y_pred_norm, method=norm_method, scaler=target_scaler)\n",
    "                y_true_raw = inverse_transform_predictions(y_val, method=norm_method, scaler=target_scaler)\n",
    "\n",
    "                # --- Evaluate\n",
    "                metrics = calculate_forecast_metrics(\n",
    "                    y_true_raw, y_pred_raw,\n",
    "                    model_name=f\"LR (seq={seq_len}, k={n_clusters}, c={cluster_id})\"\n",
    "                )\n",
    "                metrics.update({\n",
    "                    \"Seq_Length\": seq_len,\n",
    "                    \"Num_Clusters\": n_clusters,\n",
    "                    \"Cluster_ID\": cluster_id,\n",
    "                    \"Normalization\": norm_method\n",
    "                })\n",
    "\n",
    "                all_metrics.append(metrics)\n",
    "\n",
    "    return pd.DataFrame(all_metrics)\n",
    "\n",
    "# --- Run & Save\n",
    "rolling_features_lin_reg_with0 = run_linear_regression_with_lag_roll_with0(\n",
    "    atm,\n",
    "    cluster_range=(1,4),\n",
    "    seq_lens=[7,14,28,84],\n",
    "    norm_method=\"log\"\n",
    ")\n",
    "\n",
    "print(rolling_features_lin_reg_with0)\n",
    "save_df_to_downloads(\"rolling_features_lin_reg_with0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19370fc8",
   "metadata": {},
   "source": [
    "XGBoost with Feature Set 2 (Lag and Rolling Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e2a566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Runner with Optuna\n",
    "# -----------------\n",
    "def run_xgboost_with_new_features(\n",
    "    atm,\n",
    "    cluster_range=(1,4),\n",
    "    seq_lens=[7,14,28,84],\n",
    "    n_trials=20,\n",
    "    norm_method=\"log\",\n",
    "    test_size=0.2\n",
    "):\n",
    "    all_metrics = []\n",
    "\n",
    "    # add active flag once\n",
    "    atm_with_active = add_active_flag(atm)\n",
    "\n",
    "    for seq_len in seq_lens:\n",
    "        print(f\"\\n--- Seq_len={seq_len} ---\")\n",
    "\n",
    "        # lag/rolling features\n",
    "        df_feat = create_lag_roll_features(atm_with_active, seq_len)\n",
    "\n",
    "        # drop dummy/calendar columns\n",
    "        drop_cols = [c for c in df_feat.columns\n",
    "                     if c.startswith((\"dow_\", \"dom_\", \"month_\", \"day_school_holiday\", \"week_part\"))]\n",
    "        df_feat = df_feat.drop(columns=drop_cols, errors=\"ignore\")\n",
    "\n",
    "        # clustering (uses only active days internally)\n",
    "        clustered_results = cluster_atms_range(df_feat, cluster_range=cluster_range)\n",
    "\n",
    "        for n_clusters, (df_clustered, _) in clustered_results.items():\n",
    "            print(f\"=== XGBoost with {n_clusters} Clusters (Seq={seq_len}) ===\")\n",
    "\n",
    "            for cluster_id in sorted(df_clustered[\"cluster\"].dropna().unique()):\n",
    "                cluster_df = df_clustered[df_clustered[\"cluster\"] == cluster_id].copy()\n",
    "\n",
    "                # train only on active rows; keep zeros\n",
    "                cluster_df = cluster_df[cluster_df[\"is_active\"] == 1].reset_index(drop=True)\n",
    "                if cluster_df.empty:\n",
    "                    continue\n",
    "\n",
    "                # temporal split\n",
    "                split_idx = int(len(cluster_df) * (1 - test_size))\n",
    "                train_df, val_df = cluster_df.iloc[:split_idx], cluster_df.iloc[split_idx:]\n",
    "\n",
    "                # features: exclude identifiers + cluster + is_active\n",
    "                feature_cols = [\n",
    "                    c for c in cluster_df.columns\n",
    "                    if c not in [\"CASHP_ID\", \"DATE\", \"WITHDRWLS\", \"cluster\", \"is_active\"]\n",
    "                ]\n",
    "\n",
    "                # normalize (features + target)\n",
    "                X_train, X_val, y_train, y_val, feat_scaler, target_scaler = normalize_data(\n",
    "                    train_df, val_df, feature_cols, target_col=\"WITHDRWLS\", method=norm_method\n",
    "                )\n",
    "\n",
    "                trial_metrics = []\n",
    "\n",
    "                def objective(trial):\n",
    "                    params = {\n",
    "                        \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 800),\n",
    "                        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.1, log=True),\n",
    "                        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "                        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "                        \"gamma\": trial.suggest_float(\"gamma\", 0.0, 5.0),\n",
    "                        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "                        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "                        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True),\n",
    "                        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n",
    "                        \"random_state\": 42,\n",
    "                        \"tree_method\": \"hist\",\n",
    "                    }\n",
    "\n",
    "                    model = xgb.XGBRegressor(**params)\n",
    "                    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "\n",
    "                    y_pred_norm = model.predict(X_val)\n",
    "\n",
    "                    # back to raw\n",
    "                    y_pred_raw = inverse_transform_predictions(y_pred_norm, method=norm_method, scaler=target_scaler)\n",
    "                    y_true_raw = inverse_transform_predictions(y_val,        method=norm_method, scaler=target_scaler)\n",
    "\n",
    "                    # metrics on raw\n",
    "                    metrics = calculate_forecast_metrics(\n",
    "                        y_true_raw, y_pred_raw,\n",
    "                        model_name=f\"XGB trial {trial.number} (seq={seq_len}, k={n_clusters}, c={cluster_id})\"\n",
    "                    )\n",
    "                    metrics[\"trial\"] = trial.number\n",
    "                    trial_metrics.append(metrics)\n",
    "                    return metrics[\"SMAPE\"]\n",
    "\n",
    "                study = optuna.create_study(direction=\"minimize\")\n",
    "                study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "                # best trial\n",
    "                best_trial = study.best_trial.number\n",
    "                best_metrics = [m for m in trial_metrics if m[\"trial\"] == best_trial][0]\n",
    "                best_metrics.update({\n",
    "                    \"Seq_Length\": seq_len,\n",
    "                    \"Best_Params\": study.best_trial.params,\n",
    "                    \"Num_Clusters\": n_clusters,\n",
    "                    \"Cluster_ID\": cluster_id,\n",
    "                    \"Normalization\": norm_method\n",
    "                })\n",
    "                all_metrics.append(best_metrics)\n",
    "\n",
    "    return pd.DataFrame(all_metrics)\n",
    "\n",
    "# Run & Save\n",
    "rolling_features_xgboost_with0 = run_xgboost_with_new_features(\n",
    "    atm,\n",
    "    cluster_range=(1,4),\n",
    "    seq_lens=[7,14,28,84],\n",
    "    n_trials=100,\n",
    "    norm_method=\"log\"\n",
    ")\n",
    "\n",
    "print(rolling_features_xgboost_with0)\n",
    "save_df_to_downloads(\"rolling_features_xgboost_with0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f09125",
   "metadata": {},
   "source": [
    "LSTM with Feature Set 2 (Lag and Rolling Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b7f8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Dataset\n",
    "# -----------------\n",
    "class SeqFeatureDataset(Dataset):\n",
    "    def __init__(self, X, y, seq_len=30):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.float32)\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X_seq = self.X[idx:idx+self.seq_len]\n",
    "        y_seq = self.y[idx+self.seq_len]\n",
    "        return torch.tensor(X_seq, dtype=torch.float32), torch.tensor(y_seq, dtype=torch.float32)\n",
    "\n",
    "# -----------------\n",
    "# LSTM Model\n",
    "# -----------------\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers,\n",
    "                            batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])  \n",
    "        return out.view(-1)           \n",
    "\n",
    "# -----------------\n",
    "# Runner with Optuna\n",
    "# -----------------\n",
    "def run_lstm_with_rolling_and_clusters(\n",
    "    atm,\n",
    "    cluster_range=(1,4),\n",
    "    seq_lens=[7,14,28],\n",
    "    n_trials=20,\n",
    "    norm_method=\"log\"\n",
    "):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    all_results = []\n",
    "\n",
    "    # Add active flag once\n",
    "    atm_with_active = add_active_flag(atm)\n",
    "\n",
    "    for seq_len in seq_lens:\n",
    "        print(f\"\\n--- Seq_len={seq_len} ---\")\n",
    "\n",
    "        # Feature engineering for this seq_len\n",
    "        df_feat = create_lag_roll_features(atm_with_active, seq_len)\n",
    "\n",
    "        # Clustering\n",
    "        clustered_results = cluster_atms_range(df_feat, cluster_range=cluster_range)\n",
    "\n",
    "        for n_clusters, (df_clustered, _) in clustered_results.items():\n",
    "            print(f\"=== LSTM with {n_clusters} Clusters (Seq={seq_len}) ===\")\n",
    "\n",
    "            for cluster_id in sorted(df_clustered[\"cluster\"].dropna().unique()):\n",
    "                cluster_df = df_clustered[df_clustered[\"cluster\"] == cluster_id].copy()\n",
    "\n",
    "                # Keep zeros, but only use active rows\n",
    "                cluster_df = cluster_df[cluster_df[\"is_active\"] == 1].reset_index(drop=True)\n",
    "                if cluster_df.empty:\n",
    "                    continue\n",
    "\n",
    "                # Temporal split\n",
    "                split_idx = int(len(cluster_df) * 0.8)\n",
    "                train_df, val_df = cluster_df.iloc[:split_idx], cluster_df.iloc[split_idx:]\n",
    "\n",
    "                # Features (drop identifiers, cluster, active flag)\n",
    "                feature_cols = [\n",
    "                    c for c in cluster_df.columns\n",
    "                    if c not in [\"CASHP_ID\", \"DATE\", \"WITHDRWLS\", \"cluster\", \"is_active\"]\n",
    "                ]\n",
    "\n",
    "                # Normalize\n",
    "                X_train, X_val, y_train, y_val, feat_scaler, target_scaler = normalize_data(\n",
    "                    train_df, val_df, feature_cols, target_col=\"WITHDRWLS\", method=norm_method\n",
    "                )\n",
    "\n",
    "                trial_metrics = []\n",
    "\n",
    "                def objective(trial):\n",
    "                    hidden_dim = trial.suggest_int(\"hidden_dim\", 16, 128, step=16)\n",
    "                    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "                    dropout    = trial.suggest_float(\"dropout\", 0.0, 0.5)\n",
    "                    lr         = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "                    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "\n",
    "                    train_dataset = SeqFeatureDataset(X_train, y_train, seq_len)\n",
    "                    val_dataset   = SeqFeatureDataset(X_val, y_val, seq_len)\n",
    "                    train_loader  = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "                    val_loader    = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "                    model = LSTMModel(input_dim=X_train.shape[1],\n",
    "                                      hidden_dim=hidden_dim,\n",
    "                                      num_layers=num_layers,\n",
    "                                      dropout=dropout).to(device)\n",
    "                    criterion = nn.MSELoss()\n",
    "                    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "                    # Train\n",
    "                    for _ in range(20):\n",
    "                        model.train()\n",
    "                        for X_batch, y_batch in train_loader:\n",
    "                            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                            optimizer.zero_grad()\n",
    "                            preds = model(X_batch)\n",
    "                            loss = criterion(preds, y_batch)\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # Validate\n",
    "                    model.eval()\n",
    "                    preds_norm, trues_norm = [], []\n",
    "                    with torch.no_grad():\n",
    "                        for X_batch, y_batch in val_loader:\n",
    "                            X_batch = X_batch.to(device)\n",
    "                            out = model(X_batch).cpu().numpy()\n",
    "                            preds_norm.extend(out)\n",
    "                            trues_norm.extend(y_batch.numpy())\n",
    "\n",
    "                    # Back-transform\n",
    "                    y_pred_raw = inverse_transform_predictions(np.array(preds_norm), method=norm_method, scaler=target_scaler)\n",
    "                    y_true_raw = inverse_transform_predictions(np.array(trues_norm), method=norm_method, scaler=target_scaler)\n",
    "\n",
    "                    metrics = calculate_forecast_metrics(\n",
    "                        y_true_raw, y_pred_raw,\n",
    "                        model_name=f\"LSTM trial {trial.number} (seq={seq_len}, k={n_clusters}, c={cluster_id})\"\n",
    "                    )\n",
    "                    metrics[\"trial\"] = trial.number\n",
    "                    trial_metrics.append(metrics)\n",
    "                    return metrics[\"SMAPE\"]\n",
    "\n",
    "                # Optuna\n",
    "                optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "                study = optuna.create_study(direction=\"minimize\")\n",
    "                study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "                # Best trial\n",
    "                best_trial = study.best_trial.number\n",
    "                best_metrics = [m for m in trial_metrics if m[\"trial\"] == best_trial][0]\n",
    "                best_metrics.update({\n",
    "                    \"Seq_Length\": seq_len,\n",
    "                    \"Best_Params\": study.best_trial.params,\n",
    "                    \"Num_Clusters\": n_clusters,\n",
    "                    \"Cluster_ID\": cluster_id,\n",
    "                    \"Normalization\": norm_method\n",
    "                })\n",
    "\n",
    "                all_results.append(best_metrics)\n",
    "\n",
    "    return pd.DataFrame(all_results)\n",
    "\n",
    "# Run & Save\n",
    "rolling_features_lstm_with0 = run_lstm_with_rolling_and_clusters(\n",
    "    atm,\n",
    "    cluster_range=(1,4),\n",
    "    seq_lens=[7,14,28,84],\n",
    "    n_trials=20,\n",
    "    norm_method=\"log\"\n",
    ")\n",
    "\n",
    "print(rolling_features_lstm_with0)\n",
    "save_df_to_downloads(\"rolling_features_lstm_with0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2ca1f6",
   "metadata": {},
   "source": [
    "N-Beats with Feature Set 2 (Lag and Rolling Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e1c7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Dataset\n",
    "# -----------------\n",
    "class SeqFeatureDataset(Dataset):\n",
    "    def __init__(self, X, y, seq_len=30):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.float32)\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X_seq = self.X[idx:idx+self.seq_len]\n",
    "        y_seq = self.y[idx+self.seq_len]\n",
    "        return torch.tensor(X_seq, dtype=torch.float32), torch.tensor(y_seq, dtype=torch.float32)\n",
    "\n",
    "# -----------------\n",
    "# N-BEATS Model\n",
    "# -----------------\n",
    "class NBeatsBlock(nn.Module):\n",
    "    def __init__(self, flat_dim, hidden_dim, theta_dim, num_layers=4):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_dim = flat_dim\n",
    "        for _ in range(num_layers):\n",
    "            layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_dim = hidden_dim\n",
    "        self.fc = nn.Sequential(*layers)\n",
    "        self.theta = nn.Linear(hidden_dim, theta_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return self.theta(x)\n",
    "\n",
    "class NBeatsModel(nn.Module):\n",
    "    def __init__(self, input_dim, seq_len, hidden_dim=128, num_layers=4):\n",
    "        super().__init__()\n",
    "        flat_dim = input_dim * seq_len\n",
    "        self.block = NBeatsBlock(flat_dim, hidden_dim, theta_dim=1, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, seq_len, dim = x.shape\n",
    "        x = x.reshape(b, -1)\n",
    "        out = self.block(x)\n",
    "        return out.view(-1)\n",
    "\n",
    "# -----------------\n",
    "# Runner with Optuna\n",
    "# -----------------\n",
    "def run_nbeats_with_rolling_and_clusters(\n",
    "    atm,\n",
    "    cluster_range=(1,4),\n",
    "    seq_lens=[7,14,28],\n",
    "    n_trials=20,\n",
    "    norm_method=\"log\"\n",
    "):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    all_results = []\n",
    "\n",
    "    # Add active flag once\n",
    "    atm_with_active = add_active_flag(atm)\n",
    "\n",
    "    for seq_len in seq_lens:\n",
    "        print(f\"\\n--- Seq_len={seq_len} ---\")\n",
    "\n",
    "        # Feature engineering\n",
    "        df_feat = create_lag_roll_features(atm_with_active, seq_len)\n",
    "\n",
    "        # Clustering\n",
    "        clustered_results = cluster_atms_range(df_feat, cluster_range=cluster_range)\n",
    "\n",
    "        for n_clusters, (df_clustered, _) in clustered_results.items():\n",
    "            print(f\"N-BEATS with {n_clusters} Clusters (Seq={seq_len})\")\n",
    "\n",
    "            for cluster_id in sorted(df_clustered[\"cluster\"].dropna().unique()):\n",
    "                cluster_df = df_clustered[df_clustered[\"cluster\"] == cluster_id].copy()\n",
    "\n",
    "                # Keep zeros, but only active rows\n",
    "                cluster_df = cluster_df[cluster_df[\"is_active\"] == 1].reset_index(drop=True)\n",
    "                if cluster_df.empty:\n",
    "                    continue\n",
    "\n",
    "                # Temporal split\n",
    "                split_idx = int(len(cluster_df) * 0.8)\n",
    "                train_df, val_df = cluster_df.iloc[:split_idx], cluster_df.iloc[split_idx:]\n",
    "\n",
    "                # Features (exclude identifiers, cluster, is_active)\n",
    "                feature_cols = [\n",
    "                    c for c in cluster_df.columns\n",
    "                    if c not in [\"CASHP_ID\", \"DATE\", \"WITHDRWLS\", \"cluster\", \"is_active\"]\n",
    "                ]\n",
    "\n",
    "                # Normalize\n",
    "                X_train, X_val, y_train, y_val, feat_scaler, target_scaler = normalize_data(\n",
    "                    train_df, val_df, feature_cols, target_col=\"WITHDRWLS\", method=norm_method\n",
    "                )\n",
    "\n",
    "                trial_metrics = []\n",
    "\n",
    "                def objective(trial):\n",
    "                    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 256, step=32)\n",
    "                    num_layers = trial.suggest_int(\"num_layers\", 2, 6)\n",
    "                    lr         = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "                    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "\n",
    "                    train_dataset = SeqFeatureDataset(X_train, y_train, seq_len)\n",
    "                    val_dataset   = SeqFeatureDataset(X_val, y_val, seq_len)\n",
    "                    train_loader  = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "                    val_loader    = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "                    model = NBeatsModel(input_dim=X_train.shape[1],\n",
    "                                        seq_len=seq_len,\n",
    "                                        hidden_dim=hidden_dim,\n",
    "                                        num_layers=num_layers).to(device)\n",
    "                    criterion = nn.MSELoss()\n",
    "                    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "                    # Training\n",
    "                    for _ in range(20):\n",
    "                        model.train()\n",
    "                        for X_batch, y_batch in train_loader:\n",
    "                            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                            optimizer.zero_grad()\n",
    "                            preds = model(X_batch)\n",
    "                            loss = criterion(preds, y_batch)\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # Validation\n",
    "                    model.eval()\n",
    "                    preds_norm, trues_norm = [], []\n",
    "                    with torch.no_grad():\n",
    "                        for X_batch, y_batch in val_loader:\n",
    "                            X_batch = X_batch.to(device)\n",
    "                            out = model(X_batch).cpu().numpy()\n",
    "                            preds_norm.extend(out)\n",
    "                            trues_norm.extend(y_batch.numpy())\n",
    "\n",
    "                    # Back-transform\n",
    "                    y_pred_raw = inverse_transform_predictions(np.array(preds_norm), method=norm_method, scaler=target_scaler)\n",
    "                    y_true_raw = inverse_transform_predictions(np.array(trues_norm), method=norm_method, scaler=target_scaler)\n",
    "\n",
    "                    metrics = calculate_forecast_metrics(\n",
    "                        y_true_raw, y_pred_raw,\n",
    "                        model_name=f\"N-BEATS trial {trial.number} (seq={seq_len}, k={n_clusters}, c={cluster_id})\"\n",
    "                    )\n",
    "                    metrics[\"trial\"] = trial.number\n",
    "                    trial_metrics.append(metrics)\n",
    "\n",
    "                    return metrics[\"SMAPE\"]\n",
    "\n",
    "                # Optuna\n",
    "                optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "                study = optuna.create_study(direction=\"minimize\")\n",
    "                study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "                # Best trial\n",
    "                best_trial = study.best_trial.number\n",
    "                best_metrics = [m for m in trial_metrics if m[\"trial\"] == best_trial][0]\n",
    "                best_metrics.update({\n",
    "                    \"Seq_Length\": seq_len,\n",
    "                    \"Best_Params\": study.best_trial.params,\n",
    "                    \"Num_Clusters\": n_clusters,\n",
    "                    \"Cluster_ID\": cluster_id,\n",
    "                    \"Normalization\": norm_method\n",
    "                })\n",
    "\n",
    "                all_results.append(best_metrics)\n",
    "\n",
    "    return pd.DataFrame(all_results)\n",
    "\n",
    "# Run\n",
    "rolling_features_nbeats_with0 = run_nbeats_with_rolling_and_clusters(\n",
    "    atm,\n",
    "    cluster_range=(1,4),\n",
    "    seq_lens=[7,14,28,84],\n",
    "    n_trials=20,\n",
    "    norm_method=\"log\"\n",
    ")\n",
    "\n",
    "print(rolling_features_nbeats_with0)\n",
    "save_df_to_downloads(\"rolling_features_nbeats_with0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746f3366",
   "metadata": {},
   "source": [
    "Feature Set Creation for Feature Set 3 (Lag, Rolling Features, Normalization, Reverse Transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9985cb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_lag_roll_and_target(train_df, val_df, feature_cols, target_col=\"WITHDRWLS\", method=\"log\"):\n",
    "    \"\"\"\n",
    "    Normalize ONLY lag/rolling features and target. \n",
    "    Dummy/calendar/holiday features are left as-is.\n",
    "    \"\"\"\n",
    "    lag_roll_cols = [c for c in feature_cols if c.startswith((\"lag_\", \"roll_mean_\", \"roll_std_\"))]\n",
    "    other_cols    = [c for c in feature_cols if c not in lag_roll_cols]\n",
    "\n",
    "    if method == \"log\":\n",
    "        # Lags & rolls\n",
    "        X_train_lr = np.log1p(train_df[lag_roll_cols].values)\n",
    "        X_val_lr   = np.log1p(val_df[lag_roll_cols].values)\n",
    "\n",
    "        # Other features untouched\n",
    "        X_train_oth = train_df[other_cols].values\n",
    "        X_val_oth   = val_df[other_cols].values\n",
    "\n",
    "        # Combine back\n",
    "        X_train = np.hstack([X_train_lr, X_train_oth])\n",
    "        X_val   = np.hstack([X_val_lr, X_val_oth])\n",
    "\n",
    "        # Target\n",
    "        y_train = np.log1p(train_df[target_col].values)\n",
    "        y_val   = np.log1p(val_df[target_col].values)\n",
    "\n",
    "        feat_scaler, target_scaler = None, None\n",
    "\n",
    "    elif method == \"minmax\":\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        feat_scaler = MinMaxScaler()\n",
    "        target_scaler = MinMaxScaler()\n",
    "\n",
    "        X_train = feat_scaler.fit_transform(train_df[feature_cols])\n",
    "        X_val   = feat_scaler.transform(val_df[feature_cols])\n",
    "\n",
    "        y_train = target_scaler.fit_transform(train_df[[target_col]]).ravel()\n",
    "        y_val   = target_scaler.transform(val_df[[target_col]]).ravel()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"method must be 'log' or 'minmax'\")\n",
    "\n",
    "    return X_train, X_val, y_train, y_val, feat_scaler, target_scaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0567251",
   "metadata": {},
   "source": [
    "Linear Regression with Feature Set 3 (Merged Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be28614e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Runner with Optuna\n",
    "# -----------------\n",
    "def run_linear_regression_with_both_features(\n",
    "    atm,\n",
    "    official_holidays,\n",
    "    half_days,\n",
    "    cluster_range=(1,3),\n",
    "    seq_lens=[7,14,28],\n",
    "    norm_method=\"log\",\n",
    "    test_size=0.2\n",
    "):\n",
    "    all_metrics = []\n",
    "    atm_with_active = add_active_flag(atm)\n",
    "\n",
    "    for seq_len in seq_lens:\n",
    "        print(f\"\\n--- Seq_len={seq_len} ---\")\n",
    "\n",
    "        # --- Feature pipelines\n",
    "        df_calendar = feature_engineering_pipeline(atm_with_active, official_holidays, half_days)\n",
    "        df_lagroll  = create_lag_roll_features(atm_with_active, seq_len)\n",
    "\n",
    "        # Merge on CASHP_ID + DATE\n",
    "        df_feat = pd.merge(df_calendar, df_lagroll, on=[\"CASHP_ID\", \"DATE\", \"WITHDRWLS\", \"is_active\"], how=\"inner\")\n",
    "\n",
    "        # --- Clustering\n",
    "        clustered_results = cluster_atms_range(df_feat, cluster_range=cluster_range)\n",
    "\n",
    "        for n_clusters, (df_clustered, _) in clustered_results.items():\n",
    "            print(f\"LR with {n_clusters} Clusters (Seq={seq_len})\")\n",
    "\n",
    "            for cluster_id in sorted(df_clustered[\"cluster\"].dropna().unique()):\n",
    "                cluster_df = df_clustered[df_clustered[\"cluster\"] == cluster_id].copy()\n",
    "\n",
    "                # Keep zeros but only active\n",
    "                cluster_df = cluster_df[cluster_df[\"is_active\"] == 1].reset_index(drop=True)\n",
    "                if cluster_df.empty:\n",
    "                    continue\n",
    "\n",
    "                # Train/val split\n",
    "                split_idx = int(len(cluster_df) * (1 - test_size))\n",
    "                train_df, val_df = cluster_df.iloc[:split_idx], cluster_df.iloc[split_idx:]\n",
    "\n",
    "                # Features\n",
    "                feature_cols = [\n",
    "                    c for c in cluster_df.columns\n",
    "                    if c not in [\"CASHP_ID\", \"DATE\", \"WITHDRWLS\", \"cluster\", \"is_active\"]\n",
    "                ]\n",
    "\n",
    "                # Normalize (only lag/roll + target if log)\n",
    "                X_train, X_val, y_train, y_val, feat_scaler, target_scaler = normalize_lag_roll_and_target(\n",
    "                    train_df, val_df, feature_cols, target_col=\"WITHDRWLS\", method=norm_method\n",
    "                )\n",
    "\n",
    "                # Train\n",
    "                model = LinearRegression()\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                # Predict\n",
    "                y_pred_norm = model.predict(X_val)\n",
    "\n",
    "                # Reverse-transform\n",
    "                y_pred_raw = inverse_transform_predictions(y_pred_norm, method=norm_method, scaler=target_scaler)\n",
    "                y_true_raw = inverse_transform_predictions(y_val, method=norm_method, scaler=target_scaler)\n",
    "\n",
    "                # Evaluate\n",
    "                metrics = calculate_forecast_metrics(\n",
    "                    y_true_raw, y_pred_raw,\n",
    "                    model_name=f\"LR (both, seq={seq_len}, k={n_clusters}, c={cluster_id})\"\n",
    "                )\n",
    "                metrics.update({\n",
    "                    \"Seq_Length\": seq_len,\n",
    "                    \"Num_Clusters\": n_clusters,\n",
    "                    \"Cluster_ID\": cluster_id,\n",
    "                    \"Normalization\": norm_method\n",
    "                })\n",
    "\n",
    "                all_metrics.append(metrics)\n",
    "\n",
    "    return pd.DataFrame(all_metrics)\n",
    "\n",
    "# Run\n",
    "both_features_lin_reg_with0 = run_linear_regression_with_both_features(\n",
    "    atm,\n",
    "    special_days,\n",
    "    half_days,\n",
    "    cluster_range=(1,4),\n",
    "    seq_lens=[7,14,28,84],\n",
    "    norm_method=\"log\"\n",
    ")\n",
    "\n",
    "print(both_features_lin_reg_with0)\n",
    "save_df_to_downloads(\"both_features_lin_reg_with0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da8836c",
   "metadata": {},
   "source": [
    "XGBoost with Feature Set 3 (Merged Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b76a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Runner with Optuna\n",
    "# -----------------\n",
    "def run_xgboost_with_both_features(\n",
    "    atm,\n",
    "    official_holidays,\n",
    "    half_days,\n",
    "    cluster_range=(1,3),\n",
    "    seq_lens=[7,14,28],\n",
    "    n_trials=50,\n",
    "    norm_method=\"log\",\n",
    "    test_size=0.2\n",
    "):\n",
    "    all_metrics = []\n",
    "    atm_with_active = add_active_flag(atm)\n",
    "\n",
    "    for seq_len in seq_lens:\n",
    "        print(f\"\\n--- Seq_len={seq_len} ---\")\n",
    "\n",
    "        # Feature pipelines\n",
    "        df_calendar = feature_engineering_pipeline(atm_with_active, official_holidays, half_days)\n",
    "        df_lagroll  = create_lag_roll_features(atm_with_active, seq_len)\n",
    "\n",
    "        # Merge on CASHP_ID + DATE\n",
    "        df_feat = pd.merge(df_calendar, df_lagroll, on=[\"CASHP_ID\", \"DATE\", \"WITHDRWLS\", \"is_active\"], how=\"inner\")\n",
    "\n",
    "        # Clustering\n",
    "        clustered_results = cluster_atms_range(df_feat, cluster_range=cluster_range)\n",
    "\n",
    "        for n_clusters, (df_clustered, _) in clustered_results.items():\n",
    "            print(f\"XGB with {n_clusters} Clusters (Seq={seq_len})\")\n",
    "\n",
    "            for cluster_id in sorted(df_clustered[\"cluster\"].dropna().unique()):\n",
    "                cluster_df = df_clustered[df_clustered[\"cluster\"] == cluster_id].copy()\n",
    "\n",
    "                # Keep zeros but only active\n",
    "                cluster_df = cluster_df[cluster_df[\"is_active\"] == 1].reset_index(drop=True)\n",
    "                if cluster_df.empty:\n",
    "                    continue\n",
    "\n",
    "                # Train/val split\n",
    "                split_idx = int(len(cluster_df) * (1 - test_size))\n",
    "                train_df, val_df = cluster_df.iloc[:split_idx], cluster_df.iloc[split_idx:]\n",
    "\n",
    "                # Features\n",
    "                feature_cols = [\n",
    "                    c for c in cluster_df.columns\n",
    "                    if c not in [\"CASHP_ID\", \"DATE\", \"WITHDRWLS\", \"cluster\", \"is_active\"]\n",
    "                ]\n",
    "\n",
    "                # Normalize (only lag/roll + target if log)\n",
    "                X_train, X_val, y_train, y_val, feat_scaler, target_scaler = normalize_lag_roll_and_target(\n",
    "                    train_df, val_df, feature_cols, target_col=\"WITHDRWLS\", method=norm_method\n",
    "                )\n",
    "\n",
    "                trial_metrics = []\n",
    "\n",
    "                def objective(trial):\n",
    "                    params = {\n",
    "                        \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 800),\n",
    "                        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.1, log=True),\n",
    "                        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "                        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "                        \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n",
    "                        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "                        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "                        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True),\n",
    "                        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n",
    "                        \"random_state\": 42,\n",
    "                        \"tree_method\": \"hist\"\n",
    "                    }\n",
    "\n",
    "                    model = xgb.XGBRegressor(**params)\n",
    "                    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "\n",
    "                    # Predict\n",
    "                    y_pred_norm = model.predict(X_val)\n",
    "\n",
    "                    # Reverse-transform\n",
    "                    y_pred_raw = inverse_transform_predictions(y_pred_norm, method=norm_method, scaler=target_scaler)\n",
    "                    y_true_raw = inverse_transform_predictions(y_val, method=norm_method, scaler=target_scaler)\n",
    "\n",
    "                    metrics = calculate_forecast_metrics(\n",
    "                        y_true_raw, y_pred_raw,\n",
    "                        model_name=f\"XGB trial {trial.number} (both, seq={seq_len}, k={n_clusters}, c={cluster_id})\"\n",
    "                    )\n",
    "                    metrics[\"trial\"] = trial.number\n",
    "                    trial_metrics.append(metrics)\n",
    "                    return metrics[\"SMAPE\"]\n",
    "\n",
    "                # Optuna search\n",
    "                study = optuna.create_study(direction=\"minimize\")\n",
    "                study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "                # Best trial\n",
    "                best_trial = study.best_trial.number\n",
    "                best_metrics = [m for m in trial_metrics if m[\"trial\"] == best_trial][0]\n",
    "                best_metrics.update({\n",
    "                    \"Seq_Length\": seq_len,\n",
    "                    \"Best_Params\": study.best_trial.params,\n",
    "                    \"Num_Clusters\": n_clusters,\n",
    "                    \"Cluster_ID\": cluster_id,\n",
    "                    \"Normalization\": norm_method\n",
    "                })\n",
    "\n",
    "                all_metrics.append(best_metrics)\n",
    "\n",
    "    return pd.DataFrame(all_metrics)\n",
    "\n",
    "# Run\n",
    "both_features_xgboost_with0 = run_xgboost_with_both_features(\n",
    "    atm,\n",
    "    special_days,\n",
    "    half_days,\n",
    "    cluster_range=(1,4),\n",
    "    seq_lens=[7,14,28,84],\n",
    "    n_trials=100,\n",
    "    norm_method=\"log\"\n",
    ")\n",
    "\n",
    "print(both_features_xgboost_with0)\n",
    "save_df_to_downloads(\"both_features_xgboost_with0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f21683",
   "metadata": {},
   "source": [
    "LSTM with Feature Set 3 (Merged Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96ec9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Dataset\n",
    "# -----------------\n",
    "class SeqFeatureDataset(Dataset):\n",
    "    def __init__(self, X, y, seq_len=30):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.float32)\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X_seq = self.X[idx:idx+self.seq_len]\n",
    "        y_seq = self.y[idx+self.seq_len]\n",
    "        return torch.tensor(X_seq, dtype=torch.float32), torch.tensor(y_seq, dtype=torch.float32)\n",
    "\n",
    "# -----------------\n",
    "# LSTM Model\n",
    "# -----------------\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers,\n",
    "                            batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])  \n",
    "        return out.view(-1)        \n",
    "\n",
    "# -----------------\n",
    "# Runner with Optuna\n",
    "# -----------------\n",
    "def run_lstm_with_both_features(\n",
    "    atm,\n",
    "    official_holidays,\n",
    "    half_days,\n",
    "    cluster_range=(1,4),\n",
    "    seq_lens=[7,14,28],\n",
    "    n_trials=30,\n",
    "    norm_method=\"log\"\n",
    "):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    all_results = []\n",
    "    atm_with_active = add_active_flag(atm)\n",
    "\n",
    "    for seq_len in seq_lens:\n",
    "        print(f\"\\n--- Seq_len={seq_len} ---\")\n",
    "\n",
    "        # Feature pipelines\n",
    "        df_calendar = feature_engineering_pipeline(atm_with_active, official_holidays, half_days)\n",
    "        df_lagroll  = create_lag_roll_features(atm_with_active, seq_len)\n",
    "\n",
    "        # Merge features\n",
    "        df_feat = pd.merge(df_calendar, df_lagroll, on=[\"CASHP_ID\", \"DATE\", \"WITHDRWLS\", \"is_active\"], how=\"inner\")\n",
    "\n",
    "        # Clustering\n",
    "        clustered_results = cluster_atms_range(df_feat, cluster_range=cluster_range)\n",
    "\n",
    "        for n_clusters, (df_clustered, _) in clustered_results.items():\n",
    "            print(f\"LSTM with {n_clusters} Clusters (Seq={seq_len})\")\n",
    "\n",
    "            for cluster_id in sorted(df_clustered[\"cluster\"].dropna().unique()):\n",
    "                cluster_df = df_clustered[df_clustered[\"cluster\"] == cluster_id].copy()\n",
    "\n",
    "                # Keep only active days\n",
    "                cluster_df = cluster_df[cluster_df[\"is_active\"] == 1].reset_index(drop=True)\n",
    "                if cluster_df.empty:\n",
    "                    continue\n",
    "\n",
    "                # Train/val split\n",
    "                split_idx = int(len(cluster_df) * 0.8)\n",
    "                train_df, val_df = cluster_df.iloc[:split_idx], cluster_df.iloc[split_idx:]\n",
    "\n",
    "                # Features\n",
    "                feature_cols = [\n",
    "                    c for c in cluster_df.columns\n",
    "                    if c not in [\"CASHP_ID\", \"DATE\", \"WITHDRWLS\", \"cluster\", \"is_active\"]\n",
    "                ]\n",
    "\n",
    "                # Normalize (special handling for lag/roll + target if log)\n",
    "                X_train, X_val, y_train, y_val, feat_scaler, target_scaler = normalize_lag_roll_and_target(\n",
    "                    train_df, val_df, feature_cols, target_col=\"WITHDRWLS\", method=norm_method\n",
    "                )\n",
    "\n",
    "                trial_metrics = []\n",
    "\n",
    "                def objective(trial):\n",
    "                    hidden_dim = trial.suggest_int(\"hidden_dim\", 16, 128, step=16)\n",
    "                    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "                    dropout    = trial.suggest_float(\"dropout\", 0.0, 0.5)\n",
    "                    lr         = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "                    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "\n",
    "                    train_dataset = SeqFeatureDataset(X_train, y_train, seq_len)\n",
    "                    val_dataset   = SeqFeatureDataset(X_val, y_val, seq_len)\n",
    "                    train_loader  = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "                    val_loader    = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "                    model = LSTMModel(input_dim=X_train.shape[1],\n",
    "                                      hidden_dim=hidden_dim,\n",
    "                                      num_layers=num_layers,\n",
    "                                      dropout=dropout).to(device)\n",
    "                    criterion = nn.MSELoss()\n",
    "                    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "                    # Training\n",
    "                    for _ in range(20):\n",
    "                        model.train()\n",
    "                        for X_batch, y_batch in train_loader:\n",
    "                            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                            optimizer.zero_grad()\n",
    "                            preds = model(X_batch)\n",
    "                            loss = criterion(preds, y_batch)\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # Validation\n",
    "                    model.eval()\n",
    "                    preds_norm, trues_norm = [], []\n",
    "                    with torch.no_grad():\n",
    "                        for X_batch, y_batch in val_loader:\n",
    "                            X_batch = X_batch.to(device)\n",
    "                            out = model(X_batch).cpu().numpy()\n",
    "                            preds_norm.extend(out)\n",
    "                            trues_norm.extend(y_batch.numpy())\n",
    "\n",
    "                    # Reverse-transform\n",
    "                    y_pred_raw = inverse_transform_predictions(np.array(preds_norm), method=norm_method, scaler=target_scaler)\n",
    "                    y_true_raw = inverse_transform_predictions(np.array(trues_norm), method=norm_method, scaler=target_scaler)\n",
    "\n",
    "                    metrics = calculate_forecast_metrics(\n",
    "                        y_true_raw, y_pred_raw,\n",
    "                        model_name=f\"LSTM trial {trial.number} (both, seq={seq_len}, k={n_clusters}, c={cluster_id})\"\n",
    "                    )\n",
    "                    metrics[\"trial\"] = trial.number\n",
    "                    trial_metrics.append(metrics)\n",
    "                    return metrics[\"SMAPE\"]\n",
    "\n",
    "                # Optuna\n",
    "                study = optuna.create_study(direction=\"minimize\")\n",
    "                study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "                # Best trial\n",
    "                best_trial = study.best_trial.number\n",
    "                best_metrics = [m for m in trial_metrics if m[\"trial\"] == best_trial][0]\n",
    "                best_metrics.update({\n",
    "                    \"Seq_Length\": seq_len,\n",
    "                    \"Best_Params\": study.best_trial.params,\n",
    "                    \"Num_Clusters\": n_clusters,\n",
    "                    \"Cluster_ID\": cluster_id,\n",
    "                    \"Normalization\": norm_method\n",
    "                })\n",
    "\n",
    "                all_results.append(best_metrics)\n",
    "\n",
    "    return pd.DataFrame(all_results)\n",
    "\n",
    "# Run\n",
    "both_features_lstm_with0 = run_lstm_with_both_features(\n",
    "    atm,\n",
    "    special_days,\n",
    "    half_days,\n",
    "    cluster_range=(1,4),\n",
    "    seq_lens=[7,14,28,84],\n",
    "    n_trials=20,\n",
    "    norm_method=\"log\"\n",
    ")\n",
    "\n",
    "print(both_features_lstm_with0)\n",
    "save_df_to_downloads(\"both_features_lstm_with0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dca5b9",
   "metadata": {},
   "source": [
    "N-Beats with Feature Set 3 (Merged Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cc8850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Dataset\n",
    "# -----------------\n",
    "class SeqFeatureDataset(Dataset):\n",
    "    def __init__(self, X, y, seq_len=30):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.float32)\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X_seq = self.X[idx:idx+self.seq_len]\n",
    "        y_seq = self.y[idx+self.seq_len]\n",
    "        return torch.tensor(X_seq, dtype=torch.float32), torch.tensor(y_seq, dtype=torch.float32)\n",
    "\n",
    "# -----------------\n",
    "# N-BEATS Model\n",
    "# -----------------\n",
    "class NBeatsBlock(nn.Module):\n",
    "    def __init__(self, flat_dim, hidden_dim, theta_dim, num_layers=4):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_dim = flat_dim\n",
    "        for _ in range(num_layers):\n",
    "            layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_dim = hidden_dim\n",
    "        self.fc = nn.Sequential(*layers)\n",
    "        self.theta = nn.Linear(hidden_dim, theta_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return self.theta(x)\n",
    "\n",
    "class NBeatsModel(nn.Module):\n",
    "    def __init__(self, input_dim, seq_len, hidden_dim=128, num_layers=4):\n",
    "        super().__init__()\n",
    "        flat_dim = input_dim * seq_len\n",
    "        self.block = NBeatsBlock(flat_dim, hidden_dim, theta_dim=1, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, seq_len, dim = x.shape\n",
    "        x = x.reshape(b, -1)    \n",
    "        out = self.block(x)\n",
    "        return out.view(-1)\n",
    "\n",
    "# -----------------\n",
    "# Runner with Optuna\n",
    "# -----------------\n",
    "def run_nbeats_with_both_features(\n",
    "    atm,\n",
    "    official_holidays,\n",
    "    half_days,\n",
    "    cluster_range=(1,4),\n",
    "    seq_lens=[7,14,28],\n",
    "    n_trials=30,\n",
    "    norm_method=\"log\"\n",
    "):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    all_results = []\n",
    "    atm_with_active = add_active_flag(atm)\n",
    "\n",
    "    for seq_len in seq_lens:\n",
    "        print(f\"\\n--- Seq_len={seq_len} ---\")\n",
    "\n",
    "        # Feature pipelines\n",
    "        df_calendar = feature_engineering_pipeline(atm_with_active, official_holidays, half_days)\n",
    "        df_lagroll  = create_lag_roll_features(atm_with_active, seq_len)\n",
    "\n",
    "        # Merge\n",
    "        df_feat = pd.merge(df_calendar, df_lagroll,\n",
    "                           on=[\"CASHP_ID\", \"DATE\", \"WITHDRWLS\", \"is_active\"],\n",
    "                           how=\"inner\")\n",
    "\n",
    "        # Clustering\n",
    "        clustered_results = cluster_atms_range(df_feat, cluster_range=cluster_range)\n",
    "\n",
    "        for n_clusters, (df_clustered, _) in clustered_results.items():\n",
    "            print(f\"N-BEATS with {n_clusters} Clusters (Seq={seq_len})\")\n",
    "\n",
    "            for cluster_id in sorted(df_clustered[\"cluster\"].dropna().unique()):\n",
    "                cluster_df = df_clustered[df_clustered[\"cluster\"] == cluster_id].copy()\n",
    "\n",
    "                # Keep only active\n",
    "                cluster_df = cluster_df[cluster_df[\"is_active\"] == 1].reset_index(drop=True)\n",
    "                if cluster_df.empty:\n",
    "                    continue\n",
    "\n",
    "                # Train/val split\n",
    "                split_idx = int(len(cluster_df) * 0.8)\n",
    "                train_df, val_df = cluster_df.iloc[:split_idx], cluster_df.iloc[split_idx:]\n",
    "\n",
    "                # Features\n",
    "                feature_cols = [\n",
    "                    c for c in cluster_df.columns\n",
    "                    if c not in [\"CASHP_ID\", \"DATE\", \"WITHDRWLS\", \"cluster\", \"is_active\"]\n",
    "                ]\n",
    "\n",
    "                # Normalize\n",
    "                X_train, X_val, y_train, y_val, feat_scaler, target_scaler = normalize_lag_roll_and_target(\n",
    "                    train_df, val_df, feature_cols, target_col=\"WITHDRWLS\", method=norm_method\n",
    "                )\n",
    "\n",
    "                trial_metrics = []\n",
    "\n",
    "                def objective(trial):\n",
    "                    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 256, step=32)\n",
    "                    num_layers = trial.suggest_int(\"num_layers\", 2, 6)\n",
    "                    lr         = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "                    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "\n",
    "                    train_dataset = SeqFeatureDataset(X_train, y_train, seq_len)\n",
    "                    val_dataset   = SeqFeatureDataset(X_val, y_val, seq_len)\n",
    "                    train_loader  = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "                    val_loader    = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "                    model = NBeatsModel(input_dim=X_train.shape[1],\n",
    "                                        seq_len=seq_len,\n",
    "                                        hidden_dim=hidden_dim,\n",
    "                                        num_layers=num_layers).to(device)\n",
    "                    criterion = nn.MSELoss()\n",
    "                    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "                    # Training\n",
    "                    for _ in range(20):\n",
    "                        model.train()\n",
    "                        for X_batch, y_batch in train_loader:\n",
    "                            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                            optimizer.zero_grad()\n",
    "                            preds = model(X_batch)\n",
    "                            loss = criterion(preds, y_batch)\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # Validation\n",
    "                    model.eval()\n",
    "                    preds_norm, trues_norm = [], []\n",
    "                    with torch.no_grad():\n",
    "                        for X_batch, y_batch in val_loader:\n",
    "                            X_batch = X_batch.to(device)\n",
    "                            out = model(X_batch).cpu().numpy()\n",
    "                            preds_norm.extend(out)\n",
    "                            trues_norm.extend(y_batch.numpy())\n",
    "\n",
    "                    y_pred_raw = inverse_transform_predictions(np.array(preds_norm), method=norm_method, scaler=target_scaler)\n",
    "                    y_true_raw = inverse_transform_predictions(np.array(trues_norm), method=norm_method, scaler=target_scaler)\n",
    "\n",
    "                    metrics = calculate_forecast_metrics(\n",
    "                        y_true_raw, y_pred_raw,\n",
    "                        model_name=f\"N-BEATS trial {trial.number} (both, seq={seq_len}, k={n_clusters}, c={cluster_id})\"\n",
    "                    )\n",
    "                    metrics[\"trial\"] = trial.number\n",
    "                    trial_metrics.append(metrics)\n",
    "                    return metrics[\"SMAPE\"]\n",
    "\n",
    "                # Optuna\n",
    "                study = optuna.create_study(direction=\"minimize\")\n",
    "                study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "                # Best trial\n",
    "                best_trial = study.best_trial.number\n",
    "                best_metrics = [m for m in trial_metrics if m[\"trial\"] == best_trial][0]\n",
    "                best_metrics.update({\n",
    "                    \"Seq_Length\": seq_len,\n",
    "                    \"Best_Params\": study.best_trial.params,\n",
    "                    \"Num_Clusters\": n_clusters,\n",
    "                    \"Cluster_ID\": cluster_id,\n",
    "                    \"Normalization\": norm_method\n",
    "                })\n",
    "                all_results.append(best_metrics)\n",
    "\n",
    "    return pd.DataFrame(all_results)\n",
    "\n",
    "# Run\n",
    "both_features_nbeats_with0 = run_nbeats_with_both_features(\n",
    "    atm,\n",
    "    special_days,\n",
    "    half_days,\n",
    "    cluster_range=(1,4),\n",
    "    seq_lens=[7,14,28,84],\n",
    "    n_trials=20,\n",
    "    norm_method=\"log\"\n",
    ")\n",
    "\n",
    "print(both_features_nbeats_with0)\n",
    "save_df_to_downloads(\"both_features_nbeats_with0\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-25.04",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
